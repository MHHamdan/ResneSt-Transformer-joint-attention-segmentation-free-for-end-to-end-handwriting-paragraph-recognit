{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f8ec852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'src/'\n",
      "/home/mhamdan/seq2seqAttenHTR/Transformer_ocr/src\n"
     ]
    }
   ],
   "source": [
    "%cd src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b539d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e503809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 630M/630M [08:14<00:00, 1.34MiB/s]\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"RN50x16\", device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a4b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def get_feature(x):\n",
    "        x = backbone.conv1(x)\n",
    "        x = backbone.bn1(x)   \n",
    "        x = backbone.relu(x)\n",
    "        x = backbone.maxpool(x)\n",
    "\n",
    "        x = backbone.layer1(x)\n",
    "        x = backbone.layer2(x)\n",
    "        x = backbone.layer3(x)\n",
    "        x = backbone.layer4(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2dbc1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6de38d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ec48ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f8b96eda640>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.visual.transformer.register_forward_hook(get_features(\"feats\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6b29d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5f1982d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image = preprocess(Image.open(\"../CLIP.png\")).unsqueeze(0).to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8dc648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e44e7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.nn.Linear(768, 256).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28868b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([197, 1, 256])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(features['feats'].float()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53c13a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['positional_embedding', 'text_projection', 'logit_scale', 'visual.class_embedding', 'visual.positional_embedding', 'visual.proj', 'visual.conv1.weight', 'visual.ln_pre.weight', 'visual.ln_pre.bias', 'visual.transformer.resblocks.0.attn.in_proj_weight', 'visual.transformer.resblocks.0.attn.in_proj_bias', 'visual.transformer.resblocks.0.attn.out_proj.weight', 'visual.transformer.resblocks.0.attn.out_proj.bias', 'visual.transformer.resblocks.0.ln_1.weight', 'visual.transformer.resblocks.0.ln_1.bias', 'visual.transformer.resblocks.0.mlp.c_fc.weight', 'visual.transformer.resblocks.0.mlp.c_fc.bias', 'visual.transformer.resblocks.0.mlp.c_proj.weight', 'visual.transformer.resblocks.0.mlp.c_proj.bias', 'visual.transformer.resblocks.0.ln_2.weight', 'visual.transformer.resblocks.0.ln_2.bias', 'visual.transformer.resblocks.1.attn.in_proj_weight', 'visual.transformer.resblocks.1.attn.in_proj_bias', 'visual.transformer.resblocks.1.attn.out_proj.weight', 'visual.transformer.resblocks.1.attn.out_proj.bias', 'visual.transformer.resblocks.1.ln_1.weight', 'visual.transformer.resblocks.1.ln_1.bias', 'visual.transformer.resblocks.1.mlp.c_fc.weight', 'visual.transformer.resblocks.1.mlp.c_fc.bias', 'visual.transformer.resblocks.1.mlp.c_proj.weight', 'visual.transformer.resblocks.1.mlp.c_proj.bias', 'visual.transformer.resblocks.1.ln_2.weight', 'visual.transformer.resblocks.1.ln_2.bias', 'visual.transformer.resblocks.2.attn.in_proj_weight', 'visual.transformer.resblocks.2.attn.in_proj_bias', 'visual.transformer.resblocks.2.attn.out_proj.weight', 'visual.transformer.resblocks.2.attn.out_proj.bias', 'visual.transformer.resblocks.2.ln_1.weight', 'visual.transformer.resblocks.2.ln_1.bias', 'visual.transformer.resblocks.2.mlp.c_fc.weight', 'visual.transformer.resblocks.2.mlp.c_fc.bias', 'visual.transformer.resblocks.2.mlp.c_proj.weight', 'visual.transformer.resblocks.2.mlp.c_proj.bias', 'visual.transformer.resblocks.2.ln_2.weight', 'visual.transformer.resblocks.2.ln_2.bias', 'visual.transformer.resblocks.3.attn.in_proj_weight', 'visual.transformer.resblocks.3.attn.in_proj_bias', 'visual.transformer.resblocks.3.attn.out_proj.weight', 'visual.transformer.resblocks.3.attn.out_proj.bias', 'visual.transformer.resblocks.3.ln_1.weight', 'visual.transformer.resblocks.3.ln_1.bias', 'visual.transformer.resblocks.3.mlp.c_fc.weight', 'visual.transformer.resblocks.3.mlp.c_fc.bias', 'visual.transformer.resblocks.3.mlp.c_proj.weight', 'visual.transformer.resblocks.3.mlp.c_proj.bias', 'visual.transformer.resblocks.3.ln_2.weight', 'visual.transformer.resblocks.3.ln_2.bias', 'visual.transformer.resblocks.4.attn.in_proj_weight', 'visual.transformer.resblocks.4.attn.in_proj_bias', 'visual.transformer.resblocks.4.attn.out_proj.weight', 'visual.transformer.resblocks.4.attn.out_proj.bias', 'visual.transformer.resblocks.4.ln_1.weight', 'visual.transformer.resblocks.4.ln_1.bias', 'visual.transformer.resblocks.4.mlp.c_fc.weight', 'visual.transformer.resblocks.4.mlp.c_fc.bias', 'visual.transformer.resblocks.4.mlp.c_proj.weight', 'visual.transformer.resblocks.4.mlp.c_proj.bias', 'visual.transformer.resblocks.4.ln_2.weight', 'visual.transformer.resblocks.4.ln_2.bias', 'visual.transformer.resblocks.5.attn.in_proj_weight', 'visual.transformer.resblocks.5.attn.in_proj_bias', 'visual.transformer.resblocks.5.attn.out_proj.weight', 'visual.transformer.resblocks.5.attn.out_proj.bias', 'visual.transformer.resblocks.5.ln_1.weight', 'visual.transformer.resblocks.5.ln_1.bias', 'visual.transformer.resblocks.5.mlp.c_fc.weight', 'visual.transformer.resblocks.5.mlp.c_fc.bias', 'visual.transformer.resblocks.5.mlp.c_proj.weight', 'visual.transformer.resblocks.5.mlp.c_proj.bias', 'visual.transformer.resblocks.5.ln_2.weight', 'visual.transformer.resblocks.5.ln_2.bias', 'visual.transformer.resblocks.6.attn.in_proj_weight', 'visual.transformer.resblocks.6.attn.in_proj_bias', 'visual.transformer.resblocks.6.attn.out_proj.weight', 'visual.transformer.resblocks.6.attn.out_proj.bias', 'visual.transformer.resblocks.6.ln_1.weight', 'visual.transformer.resblocks.6.ln_1.bias', 'visual.transformer.resblocks.6.mlp.c_fc.weight', 'visual.transformer.resblocks.6.mlp.c_fc.bias', 'visual.transformer.resblocks.6.mlp.c_proj.weight', 'visual.transformer.resblocks.6.mlp.c_proj.bias', 'visual.transformer.resblocks.6.ln_2.weight', 'visual.transformer.resblocks.6.ln_2.bias', 'visual.transformer.resblocks.7.attn.in_proj_weight', 'visual.transformer.resblocks.7.attn.in_proj_bias', 'visual.transformer.resblocks.7.attn.out_proj.weight', 'visual.transformer.resblocks.7.attn.out_proj.bias', 'visual.transformer.resblocks.7.ln_1.weight', 'visual.transformer.resblocks.7.ln_1.bias', 'visual.transformer.resblocks.7.mlp.c_fc.weight', 'visual.transformer.resblocks.7.mlp.c_fc.bias', 'visual.transformer.resblocks.7.mlp.c_proj.weight', 'visual.transformer.resblocks.7.mlp.c_proj.bias', 'visual.transformer.resblocks.7.ln_2.weight', 'visual.transformer.resblocks.7.ln_2.bias', 'visual.transformer.resblocks.8.attn.in_proj_weight', 'visual.transformer.resblocks.8.attn.in_proj_bias', 'visual.transformer.resblocks.8.attn.out_proj.weight', 'visual.transformer.resblocks.8.attn.out_proj.bias', 'visual.transformer.resblocks.8.ln_1.weight', 'visual.transformer.resblocks.8.ln_1.bias', 'visual.transformer.resblocks.8.mlp.c_fc.weight', 'visual.transformer.resblocks.8.mlp.c_fc.bias', 'visual.transformer.resblocks.8.mlp.c_proj.weight', 'visual.transformer.resblocks.8.mlp.c_proj.bias', 'visual.transformer.resblocks.8.ln_2.weight', 'visual.transformer.resblocks.8.ln_2.bias', 'visual.transformer.resblocks.9.attn.in_proj_weight', 'visual.transformer.resblocks.9.attn.in_proj_bias', 'visual.transformer.resblocks.9.attn.out_proj.weight', 'visual.transformer.resblocks.9.attn.out_proj.bias', 'visual.transformer.resblocks.9.ln_1.weight', 'visual.transformer.resblocks.9.ln_1.bias', 'visual.transformer.resblocks.9.mlp.c_fc.weight', 'visual.transformer.resblocks.9.mlp.c_fc.bias', 'visual.transformer.resblocks.9.mlp.c_proj.weight', 'visual.transformer.resblocks.9.mlp.c_proj.bias', 'visual.transformer.resblocks.9.ln_2.weight', 'visual.transformer.resblocks.9.ln_2.bias', 'visual.transformer.resblocks.10.attn.in_proj_weight', 'visual.transformer.resblocks.10.attn.in_proj_bias', 'visual.transformer.resblocks.10.attn.out_proj.weight', 'visual.transformer.resblocks.10.attn.out_proj.bias', 'visual.transformer.resblocks.10.ln_1.weight', 'visual.transformer.resblocks.10.ln_1.bias', 'visual.transformer.resblocks.10.mlp.c_fc.weight', 'visual.transformer.resblocks.10.mlp.c_fc.bias', 'visual.transformer.resblocks.10.mlp.c_proj.weight', 'visual.transformer.resblocks.10.mlp.c_proj.bias', 'visual.transformer.resblocks.10.ln_2.weight', 'visual.transformer.resblocks.10.ln_2.bias', 'visual.transformer.resblocks.11.attn.in_proj_weight', 'visual.transformer.resblocks.11.attn.in_proj_bias', 'visual.transformer.resblocks.11.attn.out_proj.weight', 'visual.transformer.resblocks.11.attn.out_proj.bias', 'visual.transformer.resblocks.11.ln_1.weight', 'visual.transformer.resblocks.11.ln_1.bias', 'visual.transformer.resblocks.11.mlp.c_fc.weight', 'visual.transformer.resblocks.11.mlp.c_fc.bias', 'visual.transformer.resblocks.11.mlp.c_proj.weight', 'visual.transformer.resblocks.11.mlp.c_proj.bias', 'visual.transformer.resblocks.11.ln_2.weight', 'visual.transformer.resblocks.11.ln_2.bias', 'visual.ln_post.weight', 'visual.ln_post.bias', 'transformer.resblocks.0.attn.in_proj_weight', 'transformer.resblocks.0.attn.in_proj_bias', 'transformer.resblocks.0.attn.out_proj.weight', 'transformer.resblocks.0.attn.out_proj.bias', 'transformer.resblocks.0.ln_1.weight', 'transformer.resblocks.0.ln_1.bias', 'transformer.resblocks.0.mlp.c_fc.weight', 'transformer.resblocks.0.mlp.c_fc.bias', 'transformer.resblocks.0.mlp.c_proj.weight', 'transformer.resblocks.0.mlp.c_proj.bias', 'transformer.resblocks.0.ln_2.weight', 'transformer.resblocks.0.ln_2.bias', 'transformer.resblocks.1.attn.in_proj_weight', 'transformer.resblocks.1.attn.in_proj_bias', 'transformer.resblocks.1.attn.out_proj.weight', 'transformer.resblocks.1.attn.out_proj.bias', 'transformer.resblocks.1.ln_1.weight', 'transformer.resblocks.1.ln_1.bias', 'transformer.resblocks.1.mlp.c_fc.weight', 'transformer.resblocks.1.mlp.c_fc.bias', 'transformer.resblocks.1.mlp.c_proj.weight', 'transformer.resblocks.1.mlp.c_proj.bias', 'transformer.resblocks.1.ln_2.weight', 'transformer.resblocks.1.ln_2.bias', 'transformer.resblocks.2.attn.in_proj_weight', 'transformer.resblocks.2.attn.in_proj_bias', 'transformer.resblocks.2.attn.out_proj.weight', 'transformer.resblocks.2.attn.out_proj.bias', 'transformer.resblocks.2.ln_1.weight', 'transformer.resblocks.2.ln_1.bias', 'transformer.resblocks.2.mlp.c_fc.weight', 'transformer.resblocks.2.mlp.c_fc.bias', 'transformer.resblocks.2.mlp.c_proj.weight', 'transformer.resblocks.2.mlp.c_proj.bias', 'transformer.resblocks.2.ln_2.weight', 'transformer.resblocks.2.ln_2.bias', 'transformer.resblocks.3.attn.in_proj_weight', 'transformer.resblocks.3.attn.in_proj_bias', 'transformer.resblocks.3.attn.out_proj.weight', 'transformer.resblocks.3.attn.out_proj.bias', 'transformer.resblocks.3.ln_1.weight', 'transformer.resblocks.3.ln_1.bias', 'transformer.resblocks.3.mlp.c_fc.weight', 'transformer.resblocks.3.mlp.c_fc.bias', 'transformer.resblocks.3.mlp.c_proj.weight', 'transformer.resblocks.3.mlp.c_proj.bias', 'transformer.resblocks.3.ln_2.weight', 'transformer.resblocks.3.ln_2.bias', 'transformer.resblocks.4.attn.in_proj_weight', 'transformer.resblocks.4.attn.in_proj_bias', 'transformer.resblocks.4.attn.out_proj.weight', 'transformer.resblocks.4.attn.out_proj.bias', 'transformer.resblocks.4.ln_1.weight', 'transformer.resblocks.4.ln_1.bias', 'transformer.resblocks.4.mlp.c_fc.weight', 'transformer.resblocks.4.mlp.c_fc.bias', 'transformer.resblocks.4.mlp.c_proj.weight', 'transformer.resblocks.4.mlp.c_proj.bias', 'transformer.resblocks.4.ln_2.weight', 'transformer.resblocks.4.ln_2.bias', 'transformer.resblocks.5.attn.in_proj_weight', 'transformer.resblocks.5.attn.in_proj_bias', 'transformer.resblocks.5.attn.out_proj.weight', 'transformer.resblocks.5.attn.out_proj.bias', 'transformer.resblocks.5.ln_1.weight', 'transformer.resblocks.5.ln_1.bias', 'transformer.resblocks.5.mlp.c_fc.weight', 'transformer.resblocks.5.mlp.c_fc.bias', 'transformer.resblocks.5.mlp.c_proj.weight', 'transformer.resblocks.5.mlp.c_proj.bias', 'transformer.resblocks.5.ln_2.weight', 'transformer.resblocks.5.ln_2.bias', 'transformer.resblocks.6.attn.in_proj_weight', 'transformer.resblocks.6.attn.in_proj_bias', 'transformer.resblocks.6.attn.out_proj.weight', 'transformer.resblocks.6.attn.out_proj.bias', 'transformer.resblocks.6.ln_1.weight', 'transformer.resblocks.6.ln_1.bias', 'transformer.resblocks.6.mlp.c_fc.weight', 'transformer.resblocks.6.mlp.c_fc.bias', 'transformer.resblocks.6.mlp.c_proj.weight', 'transformer.resblocks.6.mlp.c_proj.bias', 'transformer.resblocks.6.ln_2.weight', 'transformer.resblocks.6.ln_2.bias', 'transformer.resblocks.7.attn.in_proj_weight', 'transformer.resblocks.7.attn.in_proj_bias', 'transformer.resblocks.7.attn.out_proj.weight', 'transformer.resblocks.7.attn.out_proj.bias', 'transformer.resblocks.7.ln_1.weight', 'transformer.resblocks.7.ln_1.bias', 'transformer.resblocks.7.mlp.c_fc.weight', 'transformer.resblocks.7.mlp.c_fc.bias', 'transformer.resblocks.7.mlp.c_proj.weight', 'transformer.resblocks.7.mlp.c_proj.bias', 'transformer.resblocks.7.ln_2.weight', 'transformer.resblocks.7.ln_2.bias', 'transformer.resblocks.8.attn.in_proj_weight', 'transformer.resblocks.8.attn.in_proj_bias', 'transformer.resblocks.8.attn.out_proj.weight', 'transformer.resblocks.8.attn.out_proj.bias', 'transformer.resblocks.8.ln_1.weight', 'transformer.resblocks.8.ln_1.bias', 'transformer.resblocks.8.mlp.c_fc.weight', 'transformer.resblocks.8.mlp.c_fc.bias', 'transformer.resblocks.8.mlp.c_proj.weight', 'transformer.resblocks.8.mlp.c_proj.bias', 'transformer.resblocks.8.ln_2.weight', 'transformer.resblocks.8.ln_2.bias', 'transformer.resblocks.9.attn.in_proj_weight', 'transformer.resblocks.9.attn.in_proj_bias', 'transformer.resblocks.9.attn.out_proj.weight', 'transformer.resblocks.9.attn.out_proj.bias', 'transformer.resblocks.9.ln_1.weight', 'transformer.resblocks.9.ln_1.bias', 'transformer.resblocks.9.mlp.c_fc.weight', 'transformer.resblocks.9.mlp.c_fc.bias', 'transformer.resblocks.9.mlp.c_proj.weight', 'transformer.resblocks.9.mlp.c_proj.bias', 'transformer.resblocks.9.ln_2.weight', 'transformer.resblocks.9.ln_2.bias', 'transformer.resblocks.10.attn.in_proj_weight', 'transformer.resblocks.10.attn.in_proj_bias', 'transformer.resblocks.10.attn.out_proj.weight', 'transformer.resblocks.10.attn.out_proj.bias', 'transformer.resblocks.10.ln_1.weight', 'transformer.resblocks.10.ln_1.bias', 'transformer.resblocks.10.mlp.c_fc.weight', 'transformer.resblocks.10.mlp.c_fc.bias', 'transformer.resblocks.10.mlp.c_proj.weight', 'transformer.resblocks.10.mlp.c_proj.bias', 'transformer.resblocks.10.ln_2.weight', 'transformer.resblocks.10.ln_2.bias', 'transformer.resblocks.11.attn.in_proj_weight', 'transformer.resblocks.11.attn.in_proj_bias', 'transformer.resblocks.11.attn.out_proj.weight', 'transformer.resblocks.11.attn.out_proj.bias', 'transformer.resblocks.11.ln_1.weight', 'transformer.resblocks.11.ln_1.bias', 'transformer.resblocks.11.mlp.c_fc.weight', 'transformer.resblocks.11.mlp.c_fc.bias', 'transformer.resblocks.11.mlp.c_proj.weight', 'transformer.resblocks.11.mlp.c_proj.bias', 'transformer.resblocks.11.ln_2.weight', 'transformer.resblocks.11.ln_2.bias', 'token_embedding.weight', 'ln_final.weight', 'ln_final.bias'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16bab6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visual.proj = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c94d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df60b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea11aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import groupby\n",
    "import h5py\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50, resnet101\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from data import preproc as pp\n",
    "from data import evaluation\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "import pytorch_lightning as pl \n",
    "import clip as openai\n",
    "from PIL import Image\n",
    "from clip.clip import _transform as preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "539eebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=128):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class OCR(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_len, hidden_dim, nheads,\n",
    "                 num_encoder_layers, num_decoder_layers):\n",
    "        super().__init__()\n",
    "    \n",
    "        m, _ = openai.load(\"ViT-B/16\", device=\"cpu\")\n",
    "        self.clip = m\n",
    "        for param in self.clip.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "        # prediction heads with length of vocab\n",
    "        # DETR used basic 3 layer MLP for output\n",
    "        self.vocab = nn.Linear(hidden_dim,vocab_len)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.decoder = nn.Embedding(vocab_len, hidden_dim)\n",
    "        self.query_pos = PositionalEncoding(hidden_dim, .2)\n",
    "\n",
    "        # spatial positional encodings, sine positional encoding can be used.\n",
    "        # Detr baseline uses sine positional encoding.\n",
    "#         self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "#         self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.trg_mask = None\n",
    "  \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), 1)\n",
    "        mask = mask.masked_fill(mask==1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def get_feature(self,x):\n",
    "        with torch.no_grad():\n",
    "            x = self.clip.encode_image(x)\n",
    "        x = x.float()\n",
    "        return x\n",
    "\n",
    "\n",
    "    def make_len_mask(self, inp):\n",
    "        return (inp == 0).transpose(0, 1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, trg):\n",
    "        # propagate inputs through ResNet-101 up to avg-pool layer\n",
    "        x = self.get_feature(inputs)\n",
    "\n",
    "        # convert from 2048 to 256 feature planes for the transformer\n",
    "        h = x.unsqueeze(2).repeat(1,1,self.hidden_dim)\n",
    "\n",
    "        # construct positional encodings\n",
    "#         bs,_,H, W = h.shape\n",
    "#         pos = torch.cat([\n",
    "#             self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "#             self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "#         ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "        # generating subsequent mask for target\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(trg.shape[1]).to(trg.device)\n",
    "\n",
    "        # Padding mask\n",
    "        trg_pad_mask = self.make_len_mask(trg)\n",
    "\n",
    "        # Getting postional encoding for target\n",
    "        trg = self.decoder(trg)\n",
    "        trg = self.query_pos(trg)\n",
    "        \n",
    "        output = self.transformer(h.permute(1, 0, 2), trg.permute(1,0,2), tgt_mask=self.trg_mask, \n",
    "                                  tgt_key_padding_mask=trg_pad_mask.permute(1,0))\n",
    "\n",
    "        return self.vocab(output.transpose(0,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16c0e29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(vocab_len, hidden_dim=256, nheads=4,\n",
    "                 num_encoder_layers=4, num_decoder_layers=4):\n",
    "    \n",
    "    return OCR(vocab_len, hidden_dim, nheads,\n",
    "                 num_encoder_layers, num_decoder_l\n",
    "               \n",
    "               \n",
    "               \n",
    "\"\"\"\n",
    "Uses generator functions to supply train/test with data.\n",
    "Image renderings and text are created on the fly each time.\n",
    "\"\"\"\n",
    "\n",
    "class DataGenerator(Dataset):\n",
    "    \"\"\"Generator class with data streaming\"\"\"\n",
    "\n",
    "    def __init__(self, source, split, transform, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.split = split\n",
    "        self.dataset = dict()\n",
    "\n",
    "        with h5py.File(source, \"r\") as f:\n",
    "            self.dataset[self.split] = dict()\n",
    "\n",
    "            self.dataset[self.split]['dt'] = np.array(f[self.split]['dt'])\n",
    "            self.dataset[self.split]['gt'] = np.array(f[self.split]['gt'])\n",
    "          \n",
    "            randomize = np.arange(len(self.dataset[self.split]['gt']))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(randomize)\n",
    "\n",
    "            self.dataset[self.split]['dt'] = self.dataset[self.split]['dt'][randomize]\n",
    "            self.dataset[self.split]['gt'] = self.dataset[self.split]['gt'][randomize]\n",
    "\n",
    "            # decode sentences from byte\n",
    "            self.dataset[self.split]['gt'] = [x.decode() for x in self.dataset[self.split]['gt']]\n",
    "            \n",
    "        self.size = len(self.dataset[self.split]['gt'])\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = self.dataset[self.split]['dt'][i]\n",
    "        \n",
    "        #making image compatible with resnet\n",
    "        img = np.repeat(img[..., np.newaxis],3, -1)    \n",
    "        img = Image.fromarray(img.astype('uint8'))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        y_train = self.tokenizer.encode(self.dataset[self.split]['gt'][i]) \n",
    "        \n",
    "        #padding till max length\n",
    "        y_train = np.pad(y_train, (0, self.tokenizer.maxlen - len(y_train)))\n",
    "\n",
    "        gt = torch.Tensor(y_train)\n",
    "\n",
    "        return img, gt          \n",
    "\n",
    "    def __len__(self):\n",
    "      return self.size\n",
    "\n",
    "class Tokenizer():\n",
    "    \"\"\"Manager tokens functions and charset/dictionary properties\"\"\"\n",
    "\n",
    "    def __init__(self, chars, max_text_length=128):\n",
    "        self.PAD_TK, self.UNK_TK,self.SOS,self.EOS = \"¶\", \"¤\", \"SOS\", \"EOS\"\n",
    "        self.chars = [self.PAD_TK] + [self.UNK_TK ]+ [self.SOS] + [self.EOS] +list(chars)\n",
    "        self.PAD = self.chars.index(self.PAD_TK)\n",
    "        self.UNK = self.chars.index(self.UNK_TK)\n",
    "\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.maxlen = max_text_length\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to vector\"\"\"\n",
    "        text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"ASCII\").lower()\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        groups = [\"\".join(group) for _, group in groupby(text)]\n",
    "        text = \"\".join([self.UNK_TK.join(list(x)) if len(x) > 1 else x for x in groups])\n",
    "        encoded = []\n",
    "\n",
    "        text = ['SOS'] + list(text) + ['EOS']\n",
    "        for item in text:\n",
    "            index = self.chars.index(item)\n",
    "            index = self.UNK if index == -1 else index\n",
    "            encoded.append(index)\n",
    "\n",
    "        return np.asarray(encoded)\n",
    "\n",
    "    def decode(self, text):\n",
    "        \"\"\"Decode vector to text\"\"\"\n",
    "        \n",
    "        decoded = \"\".join([self.chars[int(x)] for x in text if x > -1])\n",
    "        decoded = self.remove_tokens(decoded)\n",
    "        decoded = pp.text_standardize(decoded)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def remove_tokens(self, text):\n",
    "        \"\"\"Remove tokens (PAD) from text\"\"\"\n",
    "\n",
    "        return text.replace(self.PAD_TK, \"\").replace(self.UNK_TK, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16ab200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: ../data/iam.hdf5\n",
      "output ../output/iam\n",
      "target ../output/iam/checkpoint_weights_iam.hdf5\n",
      "charset: 0123456789abcdefghijklmnopqrstuvwxyz!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "\n",
    "# define paths\n",
    "#change paths accordingly\n",
    "source = 'iam'\n",
    "source_path = '../data/{}.hdf5'.format(source)\n",
    "output_path = os.path.join(\"..\", \"output\", source)\n",
    "target_path = os.path.join(output_path, \"checkpoint_weights_iam.hdf5\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# define input size, number max of chars per line and list of valid chars\n",
    "input_size = (1024, 128, 1)\n",
    "max_text_length = 128\n",
    "# charset_base = string.printable[:95]\n",
    "charset_base = string.printable[:36].lower() + string.printable[36+26:95].lower() \n",
    "\n",
    "print(\"source:\", source_path)\n",
    "print(\"output\", output_path)\n",
    "print(\"target\", target_path)\n",
    "print(\"charset:\", charset_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f22454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(charset_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8947d5e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = make_model( vocab_len=tokenizer.vocab_size,hidden_dim=256, nheads=4,\n",
    "                 num_encoder_layers=4, num_decoder_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e71a977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ec2d038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.load(\"../output/iam/checkpoint_weights_iam_iam_small_clip.hdf5\")\n",
    "\n",
    "f = {}\n",
    "for i in d:\n",
    "    f[i.replace(\"module.\",\"\")] = d[i]\n",
    "\n",
    "model.load_state_dict(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99d7a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=nheads)\n",
    "self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "\n",
    "output = self.transformer_decoder(\n",
    "    tgt = tgt, \n",
    "    memory = h, \n",
    "    tgt_mask = tgt_mask, # to avoid looking at the future tokens (the ones on the right)\n",
    "    tgt_key_padding_mask = tgt_key_padding_mask, # to avoid working on padding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a3324f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57692d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9769591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory(model,imgs):\n",
    "    x = model.get_feature(imgs)\n",
    "    h = x.unsqueeze(2).repeat(1,1,256)\n",
    "    return model.transformer.encoder(h.permute(1, 0, 2))\n",
    "    \n",
    "\n",
    "def test(model, test_loader, max_text_length):\n",
    "    model.eval()\n",
    "    predicts = []\n",
    "    gt = []\n",
    "    imgs = []\n",
    "    c=0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            src, trg = batch\n",
    "            imgs.append(src.flatten(0,1))\n",
    "            src, trg = src.cuda(), trg.cuda()            \n",
    "            memory = get_memory(model,src.float())\n",
    "            print(memory.shape)\n",
    "            out_indexes = [tokenizer.chars.index('SOS'), ]\n",
    "            for i in range(max_text_length):\n",
    "                mask = model.generate_square_subsequent_mask(i+1).to(device)\n",
    "                trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(device)\n",
    "                output = model.vocab(model.transformer.decoder(model.query_pos(model.decoder(trg_tensor)), memory,tgt_mask=mask))\n",
    "                out_token = output.argmax(2)[-1].item()\n",
    "                out_indexes.append(out_token)\n",
    "                if out_token == tokenizer.chars.index('EOS'):\n",
    "                    break\n",
    "            predicts.append(tokenizer.decode(out_indexes))\n",
    "            gt.append(tokenizer.decode(trg.flatten(0,1)))\n",
    "            if c==2:\n",
    "                break\n",
    "            c+=1\n",
    "    return predicts, gt, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dec9616",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(DataGenerator(source_path,'test',preprocess(224), tokenizer), batch_size=1, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48bff1b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1, 256])\n",
      "torch.Size([512, 1, 256])\n",
      "torch.Size([512, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "predicts, gt, imgs = test(model, test_loader, max_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "393dbefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = list(map(lambda x : x.replace('SOS','').replace('EOS',''),predicts))\n",
    "gt = list(map(lambda x : x.replace('SOS','').replace('EOS',''),gt))\n",
    "\n",
    "evaluate = evaluation.ocr_metrics(predicts=predicts,\n",
    "                                  ground_truth=gt,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e19741e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69273173, 0.88131313, 1.        ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dc4ac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quite unable to explain why he should feel - the strong the strong the state and the strong of\n",
      "meet the deanes , and as soon as guy had - the strong the stat the strong to the strong of the common\n",
      "the horses and drank enough to cure our - the strong the state and the strong to the state and\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_40045/2393610380.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(20):\n",
    "    print(gt[i], \"-\",predicts[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5313b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "488bd1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+------------+\n",
      "|                           Modules                           | Parameters |\n",
      "+-------------------------------------------------------------+------------+\n",
      "|                  clip.positional_embedding                  |   39424    |\n",
      "|                     clip.text_projection                    |   262144   |\n",
      "|                       clip.logit_scale                      |     1      |\n",
      "|                 clip.visual.class_embedding                 |    768     |\n",
      "|               clip.visual.positional_embedding              |   151296   |\n",
      "|                       clip.visual.proj                      |   393216   |\n",
      "|                   clip.visual.conv1.weight                  |   589824   |\n",
      "|                  clip.visual.ln_pre.weight                  |    768     |\n",
      "|                   clip.visual.ln_pre.bias                   |    768     |\n",
      "|   clip.visual.transformer.resblocks.0.attn.in_proj_weight   |  1769472   |\n",
      "|    clip.visual.transformer.resblocks.0.attn.in_proj_bias    |    2304    |\n",
      "|   clip.visual.transformer.resblocks.0.attn.out_proj.weight  |   589824   |\n",
      "|    clip.visual.transformer.resblocks.0.attn.out_proj.bias   |    768     |\n",
      "|       clip.visual.transformer.resblocks.0.ln_1.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.0.ln_1.bias        |    768     |\n",
      "|     clip.visual.transformer.resblocks.0.mlp.c_fc.weight     |  2359296   |\n",
      "|      clip.visual.transformer.resblocks.0.mlp.c_fc.bias      |    3072    |\n",
      "|    clip.visual.transformer.resblocks.0.mlp.c_proj.weight    |  2359296   |\n",
      "|     clip.visual.transformer.resblocks.0.mlp.c_proj.bias     |    768     |\n",
      "|       clip.visual.transformer.resblocks.0.ln_2.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.0.ln_2.bias        |    768     |\n",
      "|   clip.visual.transformer.resblocks.1.attn.in_proj_weight   |  1769472   |\n",
      "|    clip.visual.transformer.resblocks.1.attn.in_proj_bias    |    2304    |\n",
      "|   clip.visual.transformer.resblocks.1.attn.out_proj.weight  |   589824   |\n",
      "|    clip.visual.transformer.resblocks.1.attn.out_proj.bias   |    768     |\n",
      "|       clip.visual.transformer.resblocks.1.ln_1.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.1.ln_1.bias        |    768     |\n",
      "|     clip.visual.transformer.resblocks.1.mlp.c_fc.weight     |  2359296   |\n",
      "|      clip.visual.transformer.resblocks.1.mlp.c_fc.bias      |    3072    |\n",
      "|    clip.visual.transformer.resblocks.1.mlp.c_proj.weight    |  2359296   |\n",
      "|     clip.visual.transformer.resblocks.1.mlp.c_proj.bias     |    768     |\n",
      "|       clip.visual.transformer.resblocks.1.ln_2.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.1.ln_2.bias        |    768     |\n",
      "|   clip.visual.transformer.resblocks.2.attn.in_proj_weight   |  1769472   |\n",
      "|    clip.visual.transformer.resblocks.2.attn.in_proj_bias    |    2304    |\n",
      "|   clip.visual.transformer.resblocks.2.attn.out_proj.weight  |   589824   |\n",
      "|    clip.visual.transformer.resblocks.2.attn.out_proj.bias   |    768     |\n",
      "|       clip.visual.transformer.resblocks.2.ln_1.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.2.ln_1.bias        |    768     |\n",
      "|     clip.visual.transformer.resblocks.2.mlp.c_fc.weight     |  2359296   |\n",
      "|      clip.visual.transformer.resblocks.2.mlp.c_fc.bias      |    3072    |\n",
      "|    clip.visual.transformer.resblocks.2.mlp.c_proj.weight    |  2359296   |\n",
      "|     clip.visual.transformer.resblocks.2.mlp.c_proj.bias     |    768     |\n",
      "|       clip.visual.transformer.resblocks.2.ln_2.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.2.ln_2.bias        |    768     |\n",
      "|   clip.visual.transformer.resblocks.3.attn.in_proj_weight   |  1769472   |\n",
      "|    clip.visual.transformer.resblocks.3.attn.in_proj_bias    |    2304    |\n",
      "|   clip.visual.transformer.resblocks.3.attn.out_proj.weight  |   589824   |\n",
      "|    clip.visual.transformer.resblocks.3.attn.out_proj.bias   |    768     |\n",
      "|       clip.visual.transformer.resblocks.3.ln_1.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.3.ln_1.bias        |    768     |\n",
      "|     clip.visual.transformer.resblocks.3.mlp.c_fc.weight     |  2359296   |\n",
      "|      clip.visual.transformer.resblocks.3.mlp.c_fc.bias      |    3072    |\n",
      "|    clip.visual.transformer.resblocks.3.mlp.c_proj.weight    |  2359296   |\n",
      "|     clip.visual.transformer.resblocks.3.mlp.c_proj.bias     |    768     |\n",
      "|       clip.visual.transformer.resblocks.3.ln_2.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.3.ln_2.bias        |    768     |\n",
      "|   clip.visual.transformer.resblocks.4.attn.in_proj_weight   |  1769472   |\n",
      "|    clip.visual.transformer.resblocks.4.attn.in_proj_bias    |    2304    |\n",
      "|   clip.visual.transformer.resblocks.4.attn.out_proj.weight  |   589824   |\n",
      "|    clip.visual.transformer.resblocks.4.attn.out_proj.bias   |    768     |\n",
      "|       clip.visual.transformer.resblocks.4.ln_1.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.4.ln_1.bias        |    768     |\n",
      "|     clip.visual.transformer.resblocks.4.mlp.c_fc.weight     |  2359296   |\n",
      "|      clip.visual.transformer.resblocks.4.mlp.c_fc.bias      |    3072    |\n",
      "|    clip.visual.transformer.resblocks.4.mlp.c_proj.weight    |  2359296   |\n",
      "|     clip.visual.transformer.resblocks.4.mlp.c_proj.bias     |    768     |\n",
      "|       clip.visual.transformer.resblocks.4.ln_2.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.4.ln_2.bias        |    768     |\n",
      "|   clip.visual.transformer.resblocks.5.attn.in_proj_weight   |  1769472   |\n",
      "|    clip.visual.transformer.resblocks.5.attn.in_proj_bias    |    2304    |\n",
      "|   clip.visual.transformer.resblocks.5.attn.out_proj.weight  |   589824   |\n",
      "|    clip.visual.transformer.resblocks.5.attn.out_proj.bias   |    768     |\n",
      "|       clip.visual.transformer.resblocks.5.ln_1.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.5.ln_1.bias        |    768     |\n",
      "|     clip.visual.transformer.resblocks.5.mlp.c_fc.weight     |  2359296   |\n",
      "|      clip.visual.transformer.resblocks.5.mlp.c_fc.bias      |    3072    |\n",
      "|    clip.visual.transformer.resblocks.5.mlp.c_proj.weight    |  2359296   |\n",
      "|     clip.visual.transformer.resblocks.5.mlp.c_proj.bias     |    768     |\n",
      "|       clip.visual.transformer.resblocks.5.ln_2.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.5.ln_2.bias        |    768     |\n",
      "|   clip.visual.transformer.resblocks.6.attn.in_proj_weight   |  1769472   |\n",
      "|    clip.visual.transformer.resblocks.6.attn.in_proj_bias    |    2304    |\n",
      "|   clip.visual.transformer.resblocks.6.attn.out_proj.weight  |   589824   |\n",
      "|    clip.visual.transformer.resblocks.6.attn.out_proj.bias   |    768     |\n",
      "|       clip.visual.transformer.resblocks.6.ln_1.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.6.ln_1.bias        |    768     |\n",
      "|     clip.visual.transformer.resblocks.6.mlp.c_fc.weight     |  2359296   |\n",
      "|      clip.visual.transformer.resblocks.6.mlp.c_fc.bias      |    3072    |\n",
      "|    clip.visual.transformer.resblocks.6.mlp.c_proj.weight    |  2359296   |\n",
      "|     clip.visual.transformer.resblocks.6.mlp.c_proj.bias     |    768     |\n",
      "|       clip.visual.transformer.resblocks.6.ln_2.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.6.ln_2.bias        |    768     |\n",
      "|   clip.visual.transformer.resblocks.7.attn.in_proj_weight   |  1769472   |\n",
      "|    clip.visual.transformer.resblocks.7.attn.in_proj_bias    |    2304    |\n",
      "|   clip.visual.transformer.resblocks.7.attn.out_proj.weight  |   589824   |\n",
      "|    clip.visual.transformer.resblocks.7.attn.out_proj.bias   |    768     |\n",
      "|       clip.visual.transformer.resblocks.7.ln_1.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.7.ln_1.bias        |    768     |\n",
      "|     clip.visual.transformer.resblocks.7.mlp.c_fc.weight     |  2359296   |\n",
      "|      clip.visual.transformer.resblocks.7.mlp.c_fc.bias      |    3072    |\n",
      "|    clip.visual.transformer.resblocks.7.mlp.c_proj.weight    |  2359296   |\n",
      "|     clip.visual.transformer.resblocks.7.mlp.c_proj.bias     |    768     |\n",
      "|       clip.visual.transformer.resblocks.7.ln_2.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.7.ln_2.bias        |    768     |\n",
      "|   clip.visual.transformer.resblocks.8.attn.in_proj_weight   |  1769472   |\n",
      "|    clip.visual.transformer.resblocks.8.attn.in_proj_bias    |    2304    |\n",
      "|   clip.visual.transformer.resblocks.8.attn.out_proj.weight  |   589824   |\n",
      "|    clip.visual.transformer.resblocks.8.attn.out_proj.bias   |    768     |\n",
      "|       clip.visual.transformer.resblocks.8.ln_1.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.8.ln_1.bias        |    768     |\n",
      "|     clip.visual.transformer.resblocks.8.mlp.c_fc.weight     |  2359296   |\n",
      "|      clip.visual.transformer.resblocks.8.mlp.c_fc.bias      |    3072    |\n",
      "|    clip.visual.transformer.resblocks.8.mlp.c_proj.weight    |  2359296   |\n",
      "|     clip.visual.transformer.resblocks.8.mlp.c_proj.bias     |    768     |\n",
      "|       clip.visual.transformer.resblocks.8.ln_2.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.8.ln_2.bias        |    768     |\n",
      "|   clip.visual.transformer.resblocks.9.attn.in_proj_weight   |  1769472   |\n",
      "|    clip.visual.transformer.resblocks.9.attn.in_proj_bias    |    2304    |\n",
      "|   clip.visual.transformer.resblocks.9.attn.out_proj.weight  |   589824   |\n",
      "|    clip.visual.transformer.resblocks.9.attn.out_proj.bias   |    768     |\n",
      "|       clip.visual.transformer.resblocks.9.ln_1.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.9.ln_1.bias        |    768     |\n",
      "|     clip.visual.transformer.resblocks.9.mlp.c_fc.weight     |  2359296   |\n",
      "|      clip.visual.transformer.resblocks.9.mlp.c_fc.bias      |    3072    |\n",
      "|    clip.visual.transformer.resblocks.9.mlp.c_proj.weight    |  2359296   |\n",
      "|     clip.visual.transformer.resblocks.9.mlp.c_proj.bias     |    768     |\n",
      "|       clip.visual.transformer.resblocks.9.ln_2.weight       |    768     |\n",
      "|        clip.visual.transformer.resblocks.9.ln_2.bias        |    768     |\n",
      "|   clip.visual.transformer.resblocks.10.attn.in_proj_weight  |  1769472   |\n",
      "|    clip.visual.transformer.resblocks.10.attn.in_proj_bias   |    2304    |\n",
      "|  clip.visual.transformer.resblocks.10.attn.out_proj.weight  |   589824   |\n",
      "|   clip.visual.transformer.resblocks.10.attn.out_proj.bias   |    768     |\n",
      "|       clip.visual.transformer.resblocks.10.ln_1.weight      |    768     |\n",
      "|        clip.visual.transformer.resblocks.10.ln_1.bias       |    768     |\n",
      "|     clip.visual.transformer.resblocks.10.mlp.c_fc.weight    |  2359296   |\n",
      "|      clip.visual.transformer.resblocks.10.mlp.c_fc.bias     |    3072    |\n",
      "|    clip.visual.transformer.resblocks.10.mlp.c_proj.weight   |  2359296   |\n",
      "|     clip.visual.transformer.resblocks.10.mlp.c_proj.bias    |    768     |\n",
      "|       clip.visual.transformer.resblocks.10.ln_2.weight      |    768     |\n",
      "|        clip.visual.transformer.resblocks.10.ln_2.bias       |    768     |\n",
      "|   clip.visual.transformer.resblocks.11.attn.in_proj_weight  |  1769472   |\n",
      "|    clip.visual.transformer.resblocks.11.attn.in_proj_bias   |    2304    |\n",
      "|  clip.visual.transformer.resblocks.11.attn.out_proj.weight  |   589824   |\n",
      "|   clip.visual.transformer.resblocks.11.attn.out_proj.bias   |    768     |\n",
      "|       clip.visual.transformer.resblocks.11.ln_1.weight      |    768     |\n",
      "|        clip.visual.transformer.resblocks.11.ln_1.bias       |    768     |\n",
      "|     clip.visual.transformer.resblocks.11.mlp.c_fc.weight    |  2359296   |\n",
      "|      clip.visual.transformer.resblocks.11.mlp.c_fc.bias     |    3072    |\n",
      "|    clip.visual.transformer.resblocks.11.mlp.c_proj.weight   |  2359296   |\n",
      "|     clip.visual.transformer.resblocks.11.mlp.c_proj.bias    |    768     |\n",
      "|       clip.visual.transformer.resblocks.11.ln_2.weight      |    768     |\n",
      "|        clip.visual.transformer.resblocks.11.ln_2.bias       |    768     |\n",
      "|                  clip.visual.ln_post.weight                 |    768     |\n",
      "|                   clip.visual.ln_post.bias                  |    768     |\n",
      "|       clip.transformer.resblocks.0.attn.in_proj_weight      |   786432   |\n",
      "|        clip.transformer.resblocks.0.attn.in_proj_bias       |    1536    |\n",
      "|      clip.transformer.resblocks.0.attn.out_proj.weight      |   262144   |\n",
      "|       clip.transformer.resblocks.0.attn.out_proj.bias       |    512     |\n",
      "|           clip.transformer.resblocks.0.ln_1.weight          |    512     |\n",
      "|            clip.transformer.resblocks.0.ln_1.bias           |    512     |\n",
      "|         clip.transformer.resblocks.0.mlp.c_fc.weight        |  1048576   |\n",
      "|          clip.transformer.resblocks.0.mlp.c_fc.bias         |    2048    |\n",
      "|        clip.transformer.resblocks.0.mlp.c_proj.weight       |  1048576   |\n",
      "|         clip.transformer.resblocks.0.mlp.c_proj.bias        |    512     |\n",
      "|           clip.transformer.resblocks.0.ln_2.weight          |    512     |\n",
      "|            clip.transformer.resblocks.0.ln_2.bias           |    512     |\n",
      "|       clip.transformer.resblocks.1.attn.in_proj_weight      |   786432   |\n",
      "|        clip.transformer.resblocks.1.attn.in_proj_bias       |    1536    |\n",
      "|      clip.transformer.resblocks.1.attn.out_proj.weight      |   262144   |\n",
      "|       clip.transformer.resblocks.1.attn.out_proj.bias       |    512     |\n",
      "|           clip.transformer.resblocks.1.ln_1.weight          |    512     |\n",
      "|            clip.transformer.resblocks.1.ln_1.bias           |    512     |\n",
      "|         clip.transformer.resblocks.1.mlp.c_fc.weight        |  1048576   |\n",
      "|          clip.transformer.resblocks.1.mlp.c_fc.bias         |    2048    |\n",
      "|        clip.transformer.resblocks.1.mlp.c_proj.weight       |  1048576   |\n",
      "|         clip.transformer.resblocks.1.mlp.c_proj.bias        |    512     |\n",
      "|           clip.transformer.resblocks.1.ln_2.weight          |    512     |\n",
      "|            clip.transformer.resblocks.1.ln_2.bias           |    512     |\n",
      "|       clip.transformer.resblocks.2.attn.in_proj_weight      |   786432   |\n",
      "|        clip.transformer.resblocks.2.attn.in_proj_bias       |    1536    |\n",
      "|      clip.transformer.resblocks.2.attn.out_proj.weight      |   262144   |\n",
      "|       clip.transformer.resblocks.2.attn.out_proj.bias       |    512     |\n",
      "|           clip.transformer.resblocks.2.ln_1.weight          |    512     |\n",
      "|            clip.transformer.resblocks.2.ln_1.bias           |    512     |\n",
      "|         clip.transformer.resblocks.2.mlp.c_fc.weight        |  1048576   |\n",
      "|          clip.transformer.resblocks.2.mlp.c_fc.bias         |    2048    |\n",
      "|        clip.transformer.resblocks.2.mlp.c_proj.weight       |  1048576   |\n",
      "|         clip.transformer.resblocks.2.mlp.c_proj.bias        |    512     |\n",
      "|           clip.transformer.resblocks.2.ln_2.weight          |    512     |\n",
      "|            clip.transformer.resblocks.2.ln_2.bias           |    512     |\n",
      "|       clip.transformer.resblocks.3.attn.in_proj_weight      |   786432   |\n",
      "|        clip.transformer.resblocks.3.attn.in_proj_bias       |    1536    |\n",
      "|      clip.transformer.resblocks.3.attn.out_proj.weight      |   262144   |\n",
      "|       clip.transformer.resblocks.3.attn.out_proj.bias       |    512     |\n",
      "|           clip.transformer.resblocks.3.ln_1.weight          |    512     |\n",
      "|            clip.transformer.resblocks.3.ln_1.bias           |    512     |\n",
      "|         clip.transformer.resblocks.3.mlp.c_fc.weight        |  1048576   |\n",
      "|          clip.transformer.resblocks.3.mlp.c_fc.bias         |    2048    |\n",
      "|        clip.transformer.resblocks.3.mlp.c_proj.weight       |  1048576   |\n",
      "|         clip.transformer.resblocks.3.mlp.c_proj.bias        |    512     |\n",
      "|           clip.transformer.resblocks.3.ln_2.weight          |    512     |\n",
      "|            clip.transformer.resblocks.3.ln_2.bias           |    512     |\n",
      "|       clip.transformer.resblocks.4.attn.in_proj_weight      |   786432   |\n",
      "|        clip.transformer.resblocks.4.attn.in_proj_bias       |    1536    |\n",
      "|      clip.transformer.resblocks.4.attn.out_proj.weight      |   262144   |\n",
      "|       clip.transformer.resblocks.4.attn.out_proj.bias       |    512     |\n",
      "|           clip.transformer.resblocks.4.ln_1.weight          |    512     |\n",
      "|            clip.transformer.resblocks.4.ln_1.bias           |    512     |\n",
      "|         clip.transformer.resblocks.4.mlp.c_fc.weight        |  1048576   |\n",
      "|          clip.transformer.resblocks.4.mlp.c_fc.bias         |    2048    |\n",
      "|        clip.transformer.resblocks.4.mlp.c_proj.weight       |  1048576   |\n",
      "|         clip.transformer.resblocks.4.mlp.c_proj.bias        |    512     |\n",
      "|           clip.transformer.resblocks.4.ln_2.weight          |    512     |\n",
      "|            clip.transformer.resblocks.4.ln_2.bias           |    512     |\n",
      "|       clip.transformer.resblocks.5.attn.in_proj_weight      |   786432   |\n",
      "|        clip.transformer.resblocks.5.attn.in_proj_bias       |    1536    |\n",
      "|      clip.transformer.resblocks.5.attn.out_proj.weight      |   262144   |\n",
      "|       clip.transformer.resblocks.5.attn.out_proj.bias       |    512     |\n",
      "|           clip.transformer.resblocks.5.ln_1.weight          |    512     |\n",
      "|            clip.transformer.resblocks.5.ln_1.bias           |    512     |\n",
      "|         clip.transformer.resblocks.5.mlp.c_fc.weight        |  1048576   |\n",
      "|          clip.transformer.resblocks.5.mlp.c_fc.bias         |    2048    |\n",
      "|        clip.transformer.resblocks.5.mlp.c_proj.weight       |  1048576   |\n",
      "|         clip.transformer.resblocks.5.mlp.c_proj.bias        |    512     |\n",
      "|           clip.transformer.resblocks.5.ln_2.weight          |    512     |\n",
      "|            clip.transformer.resblocks.5.ln_2.bias           |    512     |\n",
      "|       clip.transformer.resblocks.6.attn.in_proj_weight      |   786432   |\n",
      "|        clip.transformer.resblocks.6.attn.in_proj_bias       |    1536    |\n",
      "|      clip.transformer.resblocks.6.attn.out_proj.weight      |   262144   |\n",
      "|       clip.transformer.resblocks.6.attn.out_proj.bias       |    512     |\n",
      "|           clip.transformer.resblocks.6.ln_1.weight          |    512     |\n",
      "|            clip.transformer.resblocks.6.ln_1.bias           |    512     |\n",
      "|         clip.transformer.resblocks.6.mlp.c_fc.weight        |  1048576   |\n",
      "|          clip.transformer.resblocks.6.mlp.c_fc.bias         |    2048    |\n",
      "|        clip.transformer.resblocks.6.mlp.c_proj.weight       |  1048576   |\n",
      "|         clip.transformer.resblocks.6.mlp.c_proj.bias        |    512     |\n",
      "|           clip.transformer.resblocks.6.ln_2.weight          |    512     |\n",
      "|            clip.transformer.resblocks.6.ln_2.bias           |    512     |\n",
      "|       clip.transformer.resblocks.7.attn.in_proj_weight      |   786432   |\n",
      "|        clip.transformer.resblocks.7.attn.in_proj_bias       |    1536    |\n",
      "|      clip.transformer.resblocks.7.attn.out_proj.weight      |   262144   |\n",
      "|       clip.transformer.resblocks.7.attn.out_proj.bias       |    512     |\n",
      "|           clip.transformer.resblocks.7.ln_1.weight          |    512     |\n",
      "|            clip.transformer.resblocks.7.ln_1.bias           |    512     |\n",
      "|         clip.transformer.resblocks.7.mlp.c_fc.weight        |  1048576   |\n",
      "|          clip.transformer.resblocks.7.mlp.c_fc.bias         |    2048    |\n",
      "|        clip.transformer.resblocks.7.mlp.c_proj.weight       |  1048576   |\n",
      "|         clip.transformer.resblocks.7.mlp.c_proj.bias        |    512     |\n",
      "|           clip.transformer.resblocks.7.ln_2.weight          |    512     |\n",
      "|            clip.transformer.resblocks.7.ln_2.bias           |    512     |\n",
      "|       clip.transformer.resblocks.8.attn.in_proj_weight      |   786432   |\n",
      "|        clip.transformer.resblocks.8.attn.in_proj_bias       |    1536    |\n",
      "|      clip.transformer.resblocks.8.attn.out_proj.weight      |   262144   |\n",
      "|       clip.transformer.resblocks.8.attn.out_proj.bias       |    512     |\n",
      "|           clip.transformer.resblocks.8.ln_1.weight          |    512     |\n",
      "|            clip.transformer.resblocks.8.ln_1.bias           |    512     |\n",
      "|         clip.transformer.resblocks.8.mlp.c_fc.weight        |  1048576   |\n",
      "|          clip.transformer.resblocks.8.mlp.c_fc.bias         |    2048    |\n",
      "|        clip.transformer.resblocks.8.mlp.c_proj.weight       |  1048576   |\n",
      "|         clip.transformer.resblocks.8.mlp.c_proj.bias        |    512     |\n",
      "|           clip.transformer.resblocks.8.ln_2.weight          |    512     |\n",
      "|            clip.transformer.resblocks.8.ln_2.bias           |    512     |\n",
      "|       clip.transformer.resblocks.9.attn.in_proj_weight      |   786432   |\n",
      "|        clip.transformer.resblocks.9.attn.in_proj_bias       |    1536    |\n",
      "|      clip.transformer.resblocks.9.attn.out_proj.weight      |   262144   |\n",
      "|       clip.transformer.resblocks.9.attn.out_proj.bias       |    512     |\n",
      "|           clip.transformer.resblocks.9.ln_1.weight          |    512     |\n",
      "|            clip.transformer.resblocks.9.ln_1.bias           |    512     |\n",
      "|         clip.transformer.resblocks.9.mlp.c_fc.weight        |  1048576   |\n",
      "|          clip.transformer.resblocks.9.mlp.c_fc.bias         |    2048    |\n",
      "|        clip.transformer.resblocks.9.mlp.c_proj.weight       |  1048576   |\n",
      "|         clip.transformer.resblocks.9.mlp.c_proj.bias        |    512     |\n",
      "|           clip.transformer.resblocks.9.ln_2.weight          |    512     |\n",
      "|            clip.transformer.resblocks.9.ln_2.bias           |    512     |\n",
      "|      clip.transformer.resblocks.10.attn.in_proj_weight      |   786432   |\n",
      "|       clip.transformer.resblocks.10.attn.in_proj_bias       |    1536    |\n",
      "|      clip.transformer.resblocks.10.attn.out_proj.weight     |   262144   |\n",
      "|       clip.transformer.resblocks.10.attn.out_proj.bias      |    512     |\n",
      "|          clip.transformer.resblocks.10.ln_1.weight          |    512     |\n",
      "|           clip.transformer.resblocks.10.ln_1.bias           |    512     |\n",
      "|        clip.transformer.resblocks.10.mlp.c_fc.weight        |  1048576   |\n",
      "|         clip.transformer.resblocks.10.mlp.c_fc.bias         |    2048    |\n",
      "|       clip.transformer.resblocks.10.mlp.c_proj.weight       |  1048576   |\n",
      "|        clip.transformer.resblocks.10.mlp.c_proj.bias        |    512     |\n",
      "|          clip.transformer.resblocks.10.ln_2.weight          |    512     |\n",
      "|           clip.transformer.resblocks.10.ln_2.bias           |    512     |\n",
      "|      clip.transformer.resblocks.11.attn.in_proj_weight      |   786432   |\n",
      "|       clip.transformer.resblocks.11.attn.in_proj_bias       |    1536    |\n",
      "|      clip.transformer.resblocks.11.attn.out_proj.weight     |   262144   |\n",
      "|       clip.transformer.resblocks.11.attn.out_proj.bias      |    512     |\n",
      "|          clip.transformer.resblocks.11.ln_1.weight          |    512     |\n",
      "|           clip.transformer.resblocks.11.ln_1.bias           |    512     |\n",
      "|        clip.transformer.resblocks.11.mlp.c_fc.weight        |  1048576   |\n",
      "|         clip.transformer.resblocks.11.mlp.c_fc.bias         |    2048    |\n",
      "|       clip.transformer.resblocks.11.mlp.c_proj.weight       |  1048576   |\n",
      "|        clip.transformer.resblocks.11.mlp.c_proj.bias        |    512     |\n",
      "|          clip.transformer.resblocks.11.ln_2.weight          |    512     |\n",
      "|           clip.transformer.resblocks.11.ln_2.bias           |    512     |\n",
      "|                 clip.token_embedding.weight                 |  25296896  |\n",
      "|                     clip.ln_final.weight                    |    512     |\n",
      "|                      clip.ln_final.bias                     |    512     |\n",
      "|    transformer.encoder.layers.0.self_attn.in_proj_weight    |   196608   |\n",
      "|     transformer.encoder.layers.0.self_attn.in_proj_bias     |    768     |\n",
      "|    transformer.encoder.layers.0.self_attn.out_proj.weight   |   65536    |\n",
      "|     transformer.encoder.layers.0.self_attn.out_proj.bias    |    256     |\n",
      "|         transformer.encoder.layers.0.linear1.weight         |   524288   |\n",
      "|          transformer.encoder.layers.0.linear1.bias          |    2048    |\n",
      "|         transformer.encoder.layers.0.linear2.weight         |   524288   |\n",
      "|          transformer.encoder.layers.0.linear2.bias          |    256     |\n",
      "|          transformer.encoder.layers.0.norm1.weight          |    256     |\n",
      "|           transformer.encoder.layers.0.norm1.bias           |    256     |\n",
      "|          transformer.encoder.layers.0.norm2.weight          |    256     |\n",
      "|           transformer.encoder.layers.0.norm2.bias           |    256     |\n",
      "|    transformer.encoder.layers.1.self_attn.in_proj_weight    |   196608   |\n",
      "|     transformer.encoder.layers.1.self_attn.in_proj_bias     |    768     |\n",
      "|    transformer.encoder.layers.1.self_attn.out_proj.weight   |   65536    |\n",
      "|     transformer.encoder.layers.1.self_attn.out_proj.bias    |    256     |\n",
      "|         transformer.encoder.layers.1.linear1.weight         |   524288   |\n",
      "|          transformer.encoder.layers.1.linear1.bias          |    2048    |\n",
      "|         transformer.encoder.layers.1.linear2.weight         |   524288   |\n",
      "|          transformer.encoder.layers.1.linear2.bias          |    256     |\n",
      "|          transformer.encoder.layers.1.norm1.weight          |    256     |\n",
      "|           transformer.encoder.layers.1.norm1.bias           |    256     |\n",
      "|          transformer.encoder.layers.1.norm2.weight          |    256     |\n",
      "|           transformer.encoder.layers.1.norm2.bias           |    256     |\n",
      "|    transformer.encoder.layers.2.self_attn.in_proj_weight    |   196608   |\n",
      "|     transformer.encoder.layers.2.self_attn.in_proj_bias     |    768     |\n",
      "|    transformer.encoder.layers.2.self_attn.out_proj.weight   |   65536    |\n",
      "|     transformer.encoder.layers.2.self_attn.out_proj.bias    |    256     |\n",
      "|         transformer.encoder.layers.2.linear1.weight         |   524288   |\n",
      "|          transformer.encoder.layers.2.linear1.bias          |    2048    |\n",
      "|         transformer.encoder.layers.2.linear2.weight         |   524288   |\n",
      "|          transformer.encoder.layers.2.linear2.bias          |    256     |\n",
      "|          transformer.encoder.layers.2.norm1.weight          |    256     |\n",
      "|           transformer.encoder.layers.2.norm1.bias           |    256     |\n",
      "|          transformer.encoder.layers.2.norm2.weight          |    256     |\n",
      "|           transformer.encoder.layers.2.norm2.bias           |    256     |\n",
      "|    transformer.encoder.layers.3.self_attn.in_proj_weight    |   196608   |\n",
      "|     transformer.encoder.layers.3.self_attn.in_proj_bias     |    768     |\n",
      "|    transformer.encoder.layers.3.self_attn.out_proj.weight   |   65536    |\n",
      "|     transformer.encoder.layers.3.self_attn.out_proj.bias    |    256     |\n",
      "|         transformer.encoder.layers.3.linear1.weight         |   524288   |\n",
      "|          transformer.encoder.layers.3.linear1.bias          |    2048    |\n",
      "|         transformer.encoder.layers.3.linear2.weight         |   524288   |\n",
      "|          transformer.encoder.layers.3.linear2.bias          |    256     |\n",
      "|          transformer.encoder.layers.3.norm1.weight          |    256     |\n",
      "|           transformer.encoder.layers.3.norm1.bias           |    256     |\n",
      "|          transformer.encoder.layers.3.norm2.weight          |    256     |\n",
      "|           transformer.encoder.layers.3.norm2.bias           |    256     |\n",
      "|               transformer.encoder.norm.weight               |    256     |\n",
      "|                transformer.encoder.norm.bias                |    256     |\n",
      "|    transformer.decoder.layers.0.self_attn.in_proj_weight    |   196608   |\n",
      "|     transformer.decoder.layers.0.self_attn.in_proj_bias     |    768     |\n",
      "|    transformer.decoder.layers.0.self_attn.out_proj.weight   |   65536    |\n",
      "|     transformer.decoder.layers.0.self_attn.out_proj.bias    |    256     |\n",
      "|  transformer.decoder.layers.0.multihead_attn.in_proj_weight |   196608   |\n",
      "|   transformer.decoder.layers.0.multihead_attn.in_proj_bias  |    768     |\n",
      "| transformer.decoder.layers.0.multihead_attn.out_proj.weight |   65536    |\n",
      "|  transformer.decoder.layers.0.multihead_attn.out_proj.bias  |    256     |\n",
      "|         transformer.decoder.layers.0.linear1.weight         |   524288   |\n",
      "|          transformer.decoder.layers.0.linear1.bias          |    2048    |\n",
      "|         transformer.decoder.layers.0.linear2.weight         |   524288   |\n",
      "|          transformer.decoder.layers.0.linear2.bias          |    256     |\n",
      "|          transformer.decoder.layers.0.norm1.weight          |    256     |\n",
      "|           transformer.decoder.layers.0.norm1.bias           |    256     |\n",
      "|          transformer.decoder.layers.0.norm2.weight          |    256     |\n",
      "|           transformer.decoder.layers.0.norm2.bias           |    256     |\n",
      "|          transformer.decoder.layers.0.norm3.weight          |    256     |\n",
      "|           transformer.decoder.layers.0.norm3.bias           |    256     |\n",
      "|    transformer.decoder.layers.1.self_attn.in_proj_weight    |   196608   |\n",
      "|     transformer.decoder.layers.1.self_attn.in_proj_bias     |    768     |\n",
      "|    transformer.decoder.layers.1.self_attn.out_proj.weight   |   65536    |\n",
      "|     transformer.decoder.layers.1.self_attn.out_proj.bias    |    256     |\n",
      "|  transformer.decoder.layers.1.multihead_attn.in_proj_weight |   196608   |\n",
      "|   transformer.decoder.layers.1.multihead_attn.in_proj_bias  |    768     |\n",
      "| transformer.decoder.layers.1.multihead_attn.out_proj.weight |   65536    |\n",
      "|  transformer.decoder.layers.1.multihead_attn.out_proj.bias  |    256     |\n",
      "|         transformer.decoder.layers.1.linear1.weight         |   524288   |\n",
      "|          transformer.decoder.layers.1.linear1.bias          |    2048    |\n",
      "|         transformer.decoder.layers.1.linear2.weight         |   524288   |\n",
      "|          transformer.decoder.layers.1.linear2.bias          |    256     |\n",
      "|          transformer.decoder.layers.1.norm1.weight          |    256     |\n",
      "|           transformer.decoder.layers.1.norm1.bias           |    256     |\n",
      "|          transformer.decoder.layers.1.norm2.weight          |    256     |\n",
      "|           transformer.decoder.layers.1.norm2.bias           |    256     |\n",
      "|          transformer.decoder.layers.1.norm3.weight          |    256     |\n",
      "|           transformer.decoder.layers.1.norm3.bias           |    256     |\n",
      "|    transformer.decoder.layers.2.self_attn.in_proj_weight    |   196608   |\n",
      "|     transformer.decoder.layers.2.self_attn.in_proj_bias     |    768     |\n",
      "|    transformer.decoder.layers.2.self_attn.out_proj.weight   |   65536    |\n",
      "|     transformer.decoder.layers.2.self_attn.out_proj.bias    |    256     |\n",
      "|  transformer.decoder.layers.2.multihead_attn.in_proj_weight |   196608   |\n",
      "|   transformer.decoder.layers.2.multihead_attn.in_proj_bias  |    768     |\n",
      "| transformer.decoder.layers.2.multihead_attn.out_proj.weight |   65536    |\n",
      "|  transformer.decoder.layers.2.multihead_attn.out_proj.bias  |    256     |\n",
      "|         transformer.decoder.layers.2.linear1.weight         |   524288   |\n",
      "|          transformer.decoder.layers.2.linear1.bias          |    2048    |\n",
      "|         transformer.decoder.layers.2.linear2.weight         |   524288   |\n",
      "|          transformer.decoder.layers.2.linear2.bias          |    256     |\n",
      "|          transformer.decoder.layers.2.norm1.weight          |    256     |\n",
      "|           transformer.decoder.layers.2.norm1.bias           |    256     |\n",
      "|          transformer.decoder.layers.2.norm2.weight          |    256     |\n",
      "|           transformer.decoder.layers.2.norm2.bias           |    256     |\n",
      "|          transformer.decoder.layers.2.norm3.weight          |    256     |\n",
      "|           transformer.decoder.layers.2.norm3.bias           |    256     |\n",
      "|    transformer.decoder.layers.3.self_attn.in_proj_weight    |   196608   |\n",
      "|     transformer.decoder.layers.3.self_attn.in_proj_bias     |    768     |\n",
      "|    transformer.decoder.layers.3.self_attn.out_proj.weight   |   65536    |\n",
      "|     transformer.decoder.layers.3.self_attn.out_proj.bias    |    256     |\n",
      "|  transformer.decoder.layers.3.multihead_attn.in_proj_weight |   196608   |\n",
      "|   transformer.decoder.layers.3.multihead_attn.in_proj_bias  |    768     |\n",
      "| transformer.decoder.layers.3.multihead_attn.out_proj.weight |   65536    |\n",
      "|  transformer.decoder.layers.3.multihead_attn.out_proj.bias  |    256     |\n",
      "|         transformer.decoder.layers.3.linear1.weight         |   524288   |\n",
      "|          transformer.decoder.layers.3.linear1.bias          |    2048    |\n",
      "|         transformer.decoder.layers.3.linear2.weight         |   524288   |\n",
      "|          transformer.decoder.layers.3.linear2.bias          |    256     |\n",
      "|          transformer.decoder.layers.3.norm1.weight          |    256     |\n",
      "|           transformer.decoder.layers.3.norm1.bias           |    256     |\n",
      "|          transformer.decoder.layers.3.norm2.weight          |    256     |\n",
      "|           transformer.decoder.layers.3.norm2.bias           |    256     |\n",
      "|          transformer.decoder.layers.3.norm3.weight          |    256     |\n",
      "|           transformer.decoder.layers.3.norm3.bias           |    256     |\n",
      "|               transformer.decoder.norm.weight               |    256     |\n",
      "|                transformer.decoder.norm.bias                |    256     |\n",
      "|                         vocab.weight                        |   18688    |\n",
      "|                          vocab.bias                         |     73     |\n",
      "|                        decoder.weight                       |   18688    |\n",
      "+-------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 161234506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "161234506"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ff20c7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38311/2355735933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba86446f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
