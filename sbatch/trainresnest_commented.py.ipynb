{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c7de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  # Importing the Path class from the pathlib module\n",
    "import numpy as np  # Importing NumPy, a library for numerical computing in Python\n",
    "import math  # Importing the math module for basic mathematical functions\n",
    "from itertools import groupby  # Importing groupby function from the itertools module to group elements\n",
    "import h5py  # Importing the h5py package for reading and writing HDF5 files\n",
    "import unicodedata  # Importing the unicodedata module to work with Unicode characters\n",
    "import cv2  # Importing the OpenCV library for computer vision tasks\n",
    "import torch  # Importing the PyTorch library for tensor computations and deep learning\n",
    "from torch import nn  # Importing the neural network module from PyTorch\n",
    "from torch.autograd import Variable  # Importing the Variable class for automatic differentiation\n",
    "from torch.utils.data import Dataset  # Importing the Dataset class for creating custom datasets\n",
    "import time  # Importing the time module to measure time\n",
    "import albumentations  # Importing the albumentations library for image augmentation\n",
    "import albumentations.pytorch  # Importing the PyTorch integration for albumentations\n",
    "import timm  # Importing the timm library for using pre-trained models and architectures\n",
    "import argparse  # Importing the argparse module for handling command-line arguments\n",
    "import string  # Importing the string module to work with strings and text\n",
    "import json  # Importing the json module for working with JSON data\n",
    "#os module which provides a way of using operating system dependent functionality like reading or writing to the file system.        \n",
    "import os\n",
    "#the datetime module which provides classes for working with dates and times.\n",
    "import datetime\n",
    "#the string module which provides a collection of string constants and helper functions.\n",
    "import string\n",
    "\n",
    "\n",
    "# Setting the benchmark flag to True for cuDNN to optimize performance\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# Setting the seed for PyTorch's random number generator\n",
    "torch.manual_seed(13)\n",
    "# Setting the seed for NumPy's random number generator\n",
    "np.random.seed(13)\n",
    "\n",
    "def parse_args():\n",
    "    # Define a function called `parse_args` to parse command-line arguments\n",
    "\n",
    "    # Create an instance of the ArgumentParser class with a description for the 'ocr' program\n",
    "    parse = argparse.ArgumentParser(description='ocr')\n",
    "\n",
    "    # Add command-line arguments to the ArgumentParser object\n",
    "    parse.add_argument('--target_path', type=str, default=\"saved_models/\", help='target folder')\n",
    "    parse.add_argument('--name_file', type=str, default=\"resnest_backbone\", help='name of the state dict')\n",
    "    parse.add_argument('--file_path', type=str, default=\"\", help='path of the hdf5 file')\n",
    "    parse.add_argument('--epochs', type=int, default=200, help='Number of total epochs')\n",
    "    parse.add_argument('--batch_size', type=int, default=36, help='Size of one batch')\n",
    "    parse.add_argument('--optimizer', type=int, default=0, help='load the optimizer as well')\n",
    "    parse.add_argument('--lr', type=float, default=0.00006, help='Initial learning rate')\n",
    "    parse.add_argument('--charset_base', type=str, default=string.printable[:95], help='path to vocab')\n",
    "    parse.add_argument('--device', type=int, default=0, help='cuda device')\n",
    "    parse.add_argument('--finetune', type=str, default='', help='pretrain model path')\n",
    "\n",
    "    # Call the `parse_args()` method to parse the command-line arguments and store the results in the `args` variable\n",
    "    args = parse.parse_args()\n",
    "\n",
    "    # Return the `args` variable containing the parsed command-line arguments\n",
    "    return args\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Define a class called 'PositionalEncoding' that inherits from PyTorch's 'nn.Module'\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=46):\n",
    "        # Initialize the class with a constructor that takes d_model, dropout, and max_len as arguments\n",
    "        super(PositionalEncoding, self).__init__()  # Call the superclass constructor\n",
    "        self.dropout = nn.Dropout(p=dropout)  # Initialize a dropout layer with the given dropout probability\n",
    "\n",
    "        # Create a positional encoding matrix with dimensions (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # Create a position tensor of size (max_len, 1) with values from 0 to max_len-1\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # Compute the divisor term for the sine and cosine functions\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        # Calculate the sine values for even indices and store them in the positional encoding matrix\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Calculate the cosine values for odd indices and store them in the positional encoding matrix\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Reshape the positional encoding matrix and transpose its first two dimensions\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        # Register 'pe' as a buffer so that it is not considered as a learnable parameter\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward method for the PositionalEncoding class\n",
    "        x = x + self.pe[: x.size(0), :]  # Add the positional encoding to the input tensor 'x'\n",
    "        return self.dropout(x)  # Apply dropout and return the resulting tensor\n",
    "    \n",
    "\"\"\"\n",
    "Defines a class OCR that inherits from nn.Module.\n",
    "Initializes the constructor with several arguments: vocab_len (the length of the vocabulary), \n",
    "max_len (the maximum length of the input sequence), hidden_dim (the number of hidden dimensions in \n",
    "the transformer model), nheads (the number of heads in the transformer model), num_encoder_layers (the number\n",
    "of encoder layers in the transformer model), and num_decoder_layers (the number of decoder layers in the\n",
    "transformer model).\n",
    "Calls the constructor of the base class nn.Module.\n",
    "\"\"\"    \n",
    "    \n",
    "# We define an OCR OPtical Character Recognition model using transformer-based architecture for text recognition.\n",
    "#The OCR class inherits from 'nn.Module', which is abase class for all neural network modules in PyTorch. \n",
    "class OCR(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_len,\n",
    "        max_len,\n",
    "        hidden_dim,\n",
    "        nheads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        #A backbone model that serves as the feature extractor. \n",
    "        #Here, a resnest101e model from the timm package is used, which is pre-trained on ImageNet.\n",
    "        self.backbone = timm.create_model(\"resnest101e\", pretrained=True)\n",
    "        \n",
    "        #Deletes the fully-connected layer (fc) of the backbone model.\n",
    "        del self.backbone.fc\n",
    "        \n",
    "        #A 2D convolutional layer (conv) \n",
    "        #that takes the feature maps from the backbone and converts them from 2048 to hidden_dim feature planes.\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "        \n",
    "        #A transformer (transformer) that takes the feature maps and output positional encodings (object queries) \n",
    "        #to predict the text. It has num_encoder_layers layers for the encoder and num_decoder_layers for the decoder,\n",
    "        #with hidden_dim number of hidden dimensions and nheads number of heads.\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "        \n",
    "        # A linear layer (vocab) that produces the prediction heads with a length of vocab_len.\n",
    "        self.vocab = nn.Linear(hidden_dim, vocab_len)\n",
    "        \n",
    "        # An embedding layer (decoder) that outputs the positional encodings (object queries) with a length of vocab_len.\n",
    "        self.decoder = nn.Embedding(vocab_len, hidden_dim)\n",
    "        \n",
    "        #Two positional encoding parameters (row_embed and col_embed) that help add positional information to the \n",
    "        #feature maps. Initializes a query_pos object of type PositionalEncoding that adds positional encodings \n",
    "        #to the decoder queries.\n",
    "        self.query_pos = PositionalEncoding(hidden_dim, 0.2, max_len)\n",
    "        \n",
    "        #Initializes two positional encoding parameters (row_embed and col_embed) that help add positional \n",
    "        #information to the feature maps.\n",
    "        self.row_embed = nn.Parameter(torch.rand(15, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(15, hidden_dim // 2))\n",
    "        #A transformer target mask (trg_mask) that is initially set to None.\n",
    "        self.trg_mask = None\n",
    "\n",
    "    #TDefines a method get_feature that applies the backbone model to extract features from the input image.\n",
    "    def get_feature(self, x):\n",
    "        #Applies a series of convolutional, batch normalization, activation, and pooling layers \n",
    "        #from the `backbone model to the input 'x'\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.act1(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "        return x\n",
    "    \n",
    "   #Defines a method generate_square_subsequent_mask that creates a mask to prevent each position \n",
    "#from attending to subsequent positions in the decoder.\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        #Initializes a mask using the torch.ones method with shape (sz, sz) and applies the \n",
    "        #torch.triu function to set all elements above the diagonal to 0.\n",
    "        mask = torch.triu(torch.ones(sz, sz), 1)\n",
    "        #Masks all remaining elements in the mask with a value of -inf using the torch.masked_fill method.\n",
    "        mask = mask.masked_fill(mask == 1, float(\"-inf\"))\n",
    "        return mask\n",
    "    \n",
    "#Defines a method make_len_mask that creates a mask for the target text sequence to exclude padding tokens \n",
    "#from the attention mechanism.\n",
    "    def make_len_mask(self, inp):\n",
    "        #Checks whether each element of the input inp is equal to 0.\n",
    "        #Transposes the resulting boolean mask such that the shape is (sequence length, batch size).\n",
    "        return (inp == 0).transpose(0, 1)\n",
    "    \n",
    "#Defines the forward method that takes the input image and target text as inputs.\n",
    "    def forward(self, inputs, trg):\n",
    "        # Applies the get_feature method to the input image, inputs to obtain the feature maps.\n",
    "        x = self.get_feature(inputs)\n",
    "        \n",
    "        #Applies the conv layer to the feature maps to convert them from 1024 to hidden_dim feature planes.\n",
    "        h = self.conv(x)\n",
    "        \n",
    "        # Initializes the positional encodings for the feature maps using the row_embed and col_embed parameters \n",
    "        #and concatenates them with the feature maps. Adds a small constant (0.1) to the positional encodings \n",
    "        #to help stabilize training.\n",
    "        bs, _, H, W = h.shape\n",
    "        pos = (\n",
    "            torch.cat(\n",
    "                [\n",
    "                    self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "                    self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "                ],\n",
    "                dim=-1,\n",
    "            )\n",
    "            .flatten(0, 1)\n",
    "            .unsqueeze(1)\n",
    "        )\n",
    "        h = pos + 0.1 * h.flatten(2).permute(2, 0, 1)\n",
    "        \n",
    "        #If the trg_mask attribute is None or the length of the target text sequence has changed, creates a \n",
    "        #new target mask using the generate_square_subsequent_mask method and sets trg_mask to the new mask.\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(trg.shape[1]).to(\n",
    "                trg.device\n",
    "            )\n",
    "            \n",
    "        # Creates a padding mask for the target text sequence using the make_len_mask method.\n",
    "        trg_pad_mask = self.make_len_mask(trg)\n",
    "        \n",
    "        #Applies decoder layer to target text to output positional encodings (object queries) for the decoder.\n",
    "        trg = self.decoder(trg)\n",
    "        \n",
    "        #Applies the query_pos object to the target text positional encodings.\n",
    "        trg = self.query_pos(trg.permute(1, 0, 2))\n",
    "        \n",
    "        #Applies the transformer to the feature maps and the positional encodings for the decoder with \n",
    "        #the target mask and padding mask.\n",
    "        output = self.transformer(h, trg, tgt_mask=self.trg_mask,\n",
    "                                  tgt_key_padding_mask=trg_pad_mask.permute(1,0))\n",
    "        \n",
    "        #Passes the resulting output through the vocab linear layer to produce the final prediction.\n",
    "        #Returns the output with the batch dimension first using the transpose method.\n",
    "        return self.vocab(output.transpose(0, 1))\n",
    "\n",
    "    \n",
    "    \n",
    "# defines a function make_model that takes in several hyperparameters and returns an instance of the HPR class \n",
    "#with those hyperparameters. The OCR class is defined in the code above and contains the actual model architecture.     \n",
    "def make_model(\n",
    "    #vocab_len: an integer specifying the number of possible characters in the output sequence (i.e., the size\n",
    "    #of the vocabulary)\n",
    "    vocab_len,\n",
    "    \n",
    "    #maxlen: an integer specifying the maximum length of the output sequence\n",
    "    maxlen,\n",
    "    \n",
    "    #hidden_dim: an integer specifying the number of hidden units in the transformer layers\n",
    "    hidden_dim=256,\n",
    "    \n",
    "    #nheads: an integer specifying the number of attention heads in the transformer layers\n",
    "    nheads=6,\n",
    "    \n",
    "    #num_encoder_layers: an integer specifying the number of transformer encoder layers\n",
    "    num_encoder_layers=2,\n",
    "    \n",
    "    #num_decoder_layers: an integer specifying the number of transformer decoder layers\n",
    "    num_decoder_layers=6,\n",
    "):\n",
    "    #The function simply calls the OCR constructor with these hyperparameters and returns the resulting model.\n",
    "    return OCR(\n",
    "        vocab_len, maxlen, hidden_dim, nheads, num_encoder_layers, num_decoder_layers\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "  \n",
    "# Define a Tokenizer class for managing tokens and character set properties\n",
    "class Tokenizer:\n",
    "    \"\"\"Manager tokens functions and charset/dictionary properties\"\"\"\n",
    "    def __init__(self, chars, max_text_length=630):\n",
    "        # Define special tokens\n",
    "        self.PAD_TK, self.UNK_TK, self.SOS, self.EOS = \"¶\", \"¤\", \"SOS\", \"EOS\"\n",
    "        # Add special tokens and characters to character set\n",
    "        self.chars = (\n",
    "            [self.PAD_TK] + [self.UNK_TK] + [self.SOS] + [self.EOS] + list(chars)\n",
    "        )\n",
    "        # Set PAD and UNK tokens to corresponding indices in character set\n",
    "        self.PAD = self.chars.index(self.PAD_TK)\n",
    "        self.UNK = self.chars.index(self.UNK_TK)\n",
    "\n",
    "        # Set vocabulary size to size of character set\n",
    "        self.vocab_size = len(self.chars)\n",
    "        # Set maximum text length\n",
    "        self.maxlen = max_text_length\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to vector\"\"\"\n",
    "        text = str(text)\n",
    "        encoded = []\n",
    "        # Add start-of-sequence and end-of-sequence tokens to text\n",
    "        text = [\"SOS\"] + list(text.strip()) + [\"EOS\"]\n",
    "        # Convert each character in text to corresponding index in character set\n",
    "        for item in text:\n",
    "            index = self.chars.index(item)\n",
    "            # If character is not in character set, use UNK token index\n",
    "            index = self.UNK if index == -1 else index\n",
    "            encoded.append(index)\n",
    "        # Return encoded text as NumPy array\n",
    "        return np.asarray(encoded)\n",
    "\n",
    "    def decode(self, text):\n",
    "        \"\"\"Decode vector to text\"\"\"\n",
    "        # Convert each index in encoded text to corresponding character in character set\n",
    "        decoded = \"\".join([self.chars[int(x)] for x in text if x > -1])\n",
    "        # Remove padding and unknown tokens from decoded text\n",
    "        decoded = self.remove_tokens(decoded)\n",
    "        # Return decoded text as string\n",
    "        return decoded\n",
    "\n",
    "    def remove_tokens(self, text):\n",
    "        \"\"\"Remove tokens (PAD) from text\"\"\"\n",
    "        # Remove padding and unknown tokens from text\n",
    "        return text.replace(self.PAD_TK, \"\").replace(self.UNK_TK, \"\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "#Parses the command-line arguments and returns them as an object.\n",
    "args = parse_args()\n",
    "print(args)\n",
    "\n",
    "#Assigns the batch size to the variable batch_size.\n",
    "batch_size = args.batch_size\n",
    "\n",
    "#Assigns the number of epochs to the variable epochs.\n",
    "epochs = args.epochs\n",
    "\n",
    "#Opens a file containing character set data in read-only mode, reads the data from it and loads it as a JSON object.\n",
    "# with open(args.charset_base, 'r') as f:\n",
    "#     data = json.load(f)    \n",
    "\n",
    "#Sets the maximum text length to 635, with the longest image text and 10 special character    \n",
    "max_text_length = 635\n",
    "\n",
    "#Sets the character set to a predefined string containing alphanumeric characters and some special characters.\n",
    "charset_base = '0123456789abcdefghijklmnopqrstuvwxyz!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ '\n",
    "\n",
    "#Sets the device to be used for computation, based on the GPU availability.\n",
    "device = torch.device(\"cuda:{}\".format(args.device))\n",
    "\n",
    "#Creates an instance of the Tokenizer class with the specified character set and maximum text length.\n",
    "tokenizer = Tokenizer(charset_base, max_text_length)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Defines two image transformation pipelines using the Albumentations library \n",
    "#for data augmentation and normalization for training and validation sets respectively.   \n",
    "\n",
    "#transform_train: the transformation pipeline for training data, composed of:\n",
    "transform_train = albumentations.Compose(\n",
    "    [\n",
    "        #OneOf: selects one of the following augmentations with equal probability:\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                #MotionBlur: applies motion blur with a limit of 7 pixels\n",
    "                albumentations.MotionBlur(p=1, blur_limit=7),\n",
    "                \n",
    "                #OpticalDistortion: applies random optical distortion with a limit of 0.05\n",
    "                albumentations.OpticalDistortion(p=1, distort_limit=0.05),\n",
    "                \n",
    "                #GaussNoise: applies Gaussian noise with a variance limit between 10.0 and 100.0\n",
    "                albumentations.GaussNoise(p=1, var_limit=(10.0, 100.0)),\n",
    "                \n",
    "                #Equalize: applies histogram equalization for adjusting image intensities to enhance contrast\n",
    "                albumentations.Equalize(p=1),\n",
    "                \n",
    "                #Solarize: inverts all pixels above a given threshold (50 in this case)\n",
    "                albumentations.Solarize(p=1, threshold=50),\n",
    "                \n",
    "                #RandomBrightnessContrast: applies random brightness and contrast adjustments with a limit of 0.2\n",
    "                albumentations.RandomBrightnessContrast(p=1, brightness_limit=0.2),\n",
    "                \n",
    "                #Downscale: scales the image down with a factor between 0.8 and 0.9\n",
    "                albumentations.Downscale(p=1, scale_min=0.8, scale_max=0.9),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        #Normalize: normalizes the image pixel values to have a mean of 0.5 and standard deviation of 0.5\n",
    "        albumentations.Normalize(),\n",
    "        \n",
    "        #pytorch.ToTensorV2(): converts the image to a PyTorch tensor\n",
    "        albumentations.pytorch.ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#transform_valid: the transformation pipeline for validation data, composed of:\n",
    "transform_valid = albumentations.Compose(\n",
    "    [\n",
    "        #Normalize: normalizes the image pixel values to have a mean of 0.5 and standard deviation of 0.5\n",
    "        albumentations.Normalize(),\n",
    "        \n",
    "        #pytorch.ToTensorV2(): converts the image to a PyTorch tensor\n",
    "        albumentations.pytorch.ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#define the number of layers in the encoder and decoder parts of the model, respectively.\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 6\n",
    "\n",
    "#an instance of the transformer model make_model() with parameters for the vocabulary length, maximum text length, \n",
    "#hidden dimension size, number of attention heads, number of encoder layers, and number of decoder layers.\n",
    "ddp_model = make_model(\n",
    "    vocab_len=tokenizer.vocab_size,\n",
    "    maxlen=tokenizer.maxlen,\n",
    "    hidden_dim=384,\n",
    "    nheads=6,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    ")\n",
    "\n",
    "\n",
    "# takes a batch of image and label data and pads the labels to the maximum length of the batch,\n",
    "#stacks the labels and images, and returns the batch.\n",
    "def collate_fn(batch):\n",
    "    imgs, labels = zip(*batch)\n",
    "    labels = [label for label in labels]\n",
    "    imgs = [img for img in imgs]\n",
    "    max_len = max(len(label) for label in labels)\n",
    "    labels = [torch.nn.functional.pad(label, (0, max_len - len(label)), 'constant', 0) for label in labels]\n",
    "    labels = torch.stack(labels)\n",
    "    imgs = torch.stack(imgs)\n",
    "    return imgs, labels\n",
    "\n",
    "#a path to the directory containing the image and label data.\n",
    "file_path = args.file_path\n",
    "\n",
    "#a PyTorch data loader for validation data, using a DataGenerator to generate the data from the file path, \n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    DataGenerator(\"{}\".format(file_path), \"valid\", transform_valid, tokenizer),\n",
    "    #the transform_valid transformation, and the tokenizer,\n",
    "    batch_size=batch_size * 4,\n",
    "    # and using the collate_fn function to collate the data into batches.\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "# PyTorch data loader for training data, using a DataGenerator to generate the data from the file path, \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    \n",
    "    #the transform_train transformation, and the tokenizer, \n",
    "    DataGenerator(\"{}\".format(file_path), \"train\", transform_train, tokenizer),\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1,\n",
    "    \n",
    "    #using the collate_fn function to collate the data into batches.\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "#the ddp_model transferred to the device specified in args.device.\n",
    "model = ddp_model.to(device)\n",
    "\n",
    "\n",
    "#The nn.Module is a base class for all neural network modules in PyTorch.\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    \n",
    "    # takes three parameters: size, padding_idx and smoothing\n",
    "    def __init__(self, size, padding_idx=0, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        \n",
    "        #the criterion is initialized as a nn.KLDivLoss object. \n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        \n",
    "        #The padding_idx is the index of the padding token used in the tokenization process.\n",
    "        self.padding_idx = padding_idx\n",
    "        \n",
    "        #The confidence is the value of confidence that is added to the true distribution\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        \n",
    "        #smoothing is the value that is subtracted from the padded and unknown tokens\n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "        #size represents the number of tokens in the vocabulary \n",
    "        self.size = size\n",
    "        \n",
    "        #true_dist is initialized as None.\n",
    "        self.true_dist = None\n",
    "        \n",
    "\n",
    "    #takes two arguments x and target, which are the predicted and the target probabilities of the model respectively. \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        \n",
    "        #The true_dist is computed using the smoothing and confidence values and is \n",
    "        #used to compute the KLDivLoss between x and true_dist.\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        \n",
    "        #Padding tokens are masked by setting their value to zero in true_dist\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        \n",
    "        #returns a criterion object which is a Kullback-Leibler Divergence loss function used to measure the distance \n",
    "        #between two probability distributions. the gradient is set to zero using requires_grad=False.\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
    "    \n",
    "\n",
    "# Set the value for smoothing\n",
    "smoothing = 0.1\n",
    "\n",
    "# Initialize a new instance of the LabelSmoothing class with size equal to the vocabulary size, \n",
    "# padding_idx equal to 0, and smoothing equal to the value set above\n",
    "criterion = LabelSmoothing(\n",
    "    size=tokenizer.vocab_size, padding_idx=0, smoothing=smoothing\n",
    ")\n",
    "\n",
    "# Move the criterion to the device specified in the args\n",
    "criterion.to(device)\n",
    "\n",
    "# Set the learning rate to the value specified in the args\n",
    "lr = args.lr\n",
    "\n",
    "# Set the factor for the scheduler to be used later in training\n",
    "scheduler_factor = 0.8\n",
    "\n",
    "\n",
    "# Load pre-trained model from checkpoint if finetune flag is set\n",
    "if args.finetune:\n",
    "    checkpoint = torch.load(\"saved_models/{}\".format(args.finetune), map_location=\"cpu\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Define optimizer with AdamW and set learning rate and weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0004)\n",
    "\n",
    "# Load optimizer state from checkpoint if optimizer flag is set\n",
    "if args.optimizer:\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Define learning rate scheduler with StepLR and set step size and decay factor\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=scheduler_factor)\n",
    "\n",
    "\n",
    "def train(model, criterion, optimizer, dataloader, scaler):\n",
    "    # set the model to training mode\n",
    "    model.train()\n",
    "    # initialize the total loss to zero\n",
    "    total_loss = 0\n",
    "    # enumerate over the dataloader which contains batches of data\n",
    "    for batch, (imgs, labels_y,) in enumerate(dataloader):\n",
    "        # move the images and labels to the device (GPU)\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels_y = labels_y.to(device).long()\n",
    "        # zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # enable mixed-precision training context\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            # compute the model output given the input images and labels\n",
    "            output = model(imgs, labels_y[:, :-1])\n",
    "            # compute the loss using the criterion\n",
    "            loss = criterion(\n",
    "                output.log_softmax(-1).contiguous().view(-1, tokenizer.vocab_size),\n",
    "                labels_y[:, 1:].contiguous().view(-1).long(),\n",
    "            )\n",
    "        # backpropagate the loss\n",
    "        scaler.scale(loss).backward()\n",
    "        # update the optimizer\n",
    "        scaler.step(optimizer)\n",
    "        # update the scaler\n",
    "        scaler.update()\n",
    "        # add the loss to the total loss\n",
    "        total_loss += loss.item()\n",
    "    # return the average loss across all batches\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "# Define the 'evaluate' function which takes in three arguments:\n",
    "# 1. model - the neural network model to be evaluated\n",
    "# 2. criterion - the loss function used to compute the error between the model predictions and the ground truth\n",
    "# 3. dataloader - an iterable object that provides batches of data and their corresponding labels\n",
    "def evaluate(\n",
    "    model,\n",
    "    criterion,\n",
    "    dataloader,):\n",
    "    # Set the model to evaluation mode, which disables dropout and batch normalization layers\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize a variable to keep track of the total loss across all batches in the epoch\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Initialize a variable to keep track of the character error rate (CER) across all batches in the epoch (note: this variable is not used in the current implementation)\n",
    "    cer = 0\n",
    "\n",
    "    # Begin a context manager to perform evaluation without tracking gradients for memory efficiency\n",
    "    with torch.no_grad():\n",
    "        # Loop through each batch of data and its corresponding labels in the dataloader\n",
    "        for batch, (\n",
    "            imgs,\n",
    "            labels_y,\n",
    "        ) in enumerate(dataloader):            \n",
    "            # Move the images and labels to the current device (CPU or GPU)\n",
    "            imgs = imgs.to(device)\n",
    "            labels_y = labels_y.to(device)\n",
    "\n",
    "            # Forward pass the images and labels through the model, excluding the last element of each label\n",
    "            output = model(imgs.float(), labels_y.long()[:, :-1])\n",
    "\n",
    "            # Compute the loss using the criterion by comparing the model output and the ground truth labels\n",
    "            # 1. Apply a log_softmax function to the output along the last dimension\n",
    "            # 2. Reshape the output tensor to be 2D with dimensions (-1, tokenizer.vocab_size)\n",
    "            # 3. Remove the first element from each label sequence and reshape the tensor to be 1D with dimensions (-1)\n",
    "            loss = criterion(\n",
    "                output.log_softmax(-1).contiguous().view(-1, tokenizer.vocab_size),\n",
    "                labels_y[:, 1:].contiguous().view(-1).long(),\n",
    "            )\n",
    "\n",
    "            # Add the current batch's loss to the epoch_loss variable\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    # Calculate the average loss across all batches by dividing the total loss by the number of batches\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to calculate the elapsed time in minutes and seconds\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "# Get the target path and name file from the command-line arguments\n",
    "target_path = args.target_path\n",
    "name_file = args.name_file + args.file_path.replace(\".hdf5\",\"\")\n",
    "\n",
    "# Set initial values for the best CER and best validation loss\n",
    "best_CER = np.inf\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "# Create a GradScaler object for mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Print a message to indicate the start of training and the name of the current file\n",
    "print(\"Started training\")\n",
    "print(name_file)\n",
    "\n",
    "# Initialize a counter to keep track of the number of epochs since the last improvement in validation loss\n",
    "c = 0\n",
    "\n",
    "# Loop over the specified number of epochs\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Record the start time of the epoch\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train the model for one epoch and calculate the training loss\n",
    "    train_loss = train(model, criterion, optimizer, train_loader, scaler)\n",
    "\n",
    "    # Evaluate the model on the validation set and calculate the validation loss\n",
    "    valid_loss = evaluate(model, criterion, val_loader)\n",
    "\n",
    "    # Calculate the elapsed time for the epoch and print the current epoch number, time, and losses\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, time.time())\n",
    "    print(f\"Epoch: {epoch+1:02}\", \"learning rate{}\".format(lr_scheduler.get_last_lr()))\n",
    "    print(f\"Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"Train Loss: {train_loss:.3f}\")\n",
    "    print(f\"Val   Loss: {valid_loss:.3f}\")\n",
    "\n",
    "    # Increment the counter and set a flag for whether to save the model\n",
    "    c += 1\n",
    "    save = 1\n",
    "\n",
    "    # If the current validation loss is better than the previous best, save the model and update the best loss\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        print(\"saving it\", best_valid_loss)\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": valid_loss,\n",
    "                \"best_loss\": best_valid_loss,\n",
    "            },\n",
    "            target_path + name_file + \"best_loss.pt\",\n",
    "        )\n",
    "\n",
    "        # Reset the counter and turn off the flag to save the last checkpoint\n",
    "        save=0\n",
    "        c = 0\n",
    "        \n",
    "    # If the flag to save the model is still on and we are not in fine-tuning mode, save the last checkpoint\n",
    "    if save and not args.finetune:        \n",
    "        print(\"saving for last loss\")\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": valid_loss,\n",
    "                \"best_loss\": best_valid_loss,\n",
    "            },\n",
    "            target_path + name_file + \"last.pt\",\n",
    "        )\n",
    "\n",
    "    # If the counter has reached its maximum value, decrease the learning rate and reset the counter\n",
    "    if c > 4:\n",
    "        lr_scheduler.step()\n",
    "        c = 0\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
