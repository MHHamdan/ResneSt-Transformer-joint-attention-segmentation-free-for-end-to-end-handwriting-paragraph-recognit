{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25c0825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mhamdan/seq2seqAttenHTR/Transformer_ocr/src\n"
     ]
    }
   ],
   "source": [
    "%cd src/\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import groupby\n",
    "import h5py\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50, resnet101\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from data import preproc as pp\n",
    "from data import evaluation\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "import pytorch_lightning as pl \n",
    "import clip as openai\n",
    "from PIL import Image\n",
    "from clip.clip import _transform as preprocess\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=128):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class OCR(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_len, hidden_dim, nheads,\n",
    "                 num_encoder_layers, num_decoder_layers):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.backbone, _ = openai.load(\"RN50x16\", device=\"cpu\")\n",
    "        del self.backbone.transformer, self.backbone.visual.attnpool\n",
    "        _ = self.backbone.to(device)\n",
    "        for name,p in self.backbone.named_parameters():\n",
    "            if \"visual\" not in name or \"attnpool\" in name:\n",
    "                p.requires_grad =  False\n",
    "            elif \"visual\" in name and \"bn\" in name:\n",
    "                p.requires_grad =  False\n",
    "\n",
    "        # create a default PyTorch transformer\n",
    "        # create conversion layer\n",
    "        self.conv = nn.Conv2d(3072, hidden_dim, 1)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=nheads)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        \n",
    "        # prediction heads with length of vocab\n",
    "        # DETR used basic 3 layer MLP for output\n",
    "        self.vocab = nn.Linear(hidden_dim,vocab_len)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.decoder = nn.Embedding(vocab_len, hidden_dim)\n",
    "        self.query_pos = PositionalEncoding(hidden_dim, .2)\n",
    "\n",
    "        # spatial positional encodings, sine positional encoding can be used.\n",
    "        # Detr baseline uses sine positional encoding.\n",
    "        self.trg_mask = None\n",
    "  \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), 1)\n",
    "        mask = mask.masked_fill(mask==1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def get_feature(self, x):\n",
    "        x = self.backbone.visual.conv1(x)\n",
    "        x = self.backbone.visual.bn1(x)   \n",
    "        x = self.backbone.visual.relu(x)\n",
    "        x = self.backbone.visual.conv2(x)\n",
    "        x = self.backbone.visual.bn2(x)   \n",
    "        x = self.backbone.visual.relu(x)\n",
    "        x = self.backbone.visual.conv3(x)\n",
    "        x = self.backbone.visual.bn3(x)   \n",
    "        x = self.backbone.visual.relu(x)\n",
    "\n",
    "        x = self.backbone.visual.avgpool(x)\n",
    "        x = self.backbone.visual.layer1(x)\n",
    "        x = self.backbone.visual.layer2(x)\n",
    "        x = self.backbone.visual.layer3(x)\n",
    "        x = self.backbone.visual.layer4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def make_len_mask(self, inp):\n",
    "        return (inp == 0).transpose(0, 1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, trg):\n",
    "        # propagate inputs through ResNet-101 up to avg-pool layer\n",
    "        x = self.get_feature(inputs)\n",
    "\n",
    "        # convert from 2048 to 256 feature planes for the transformer\n",
    "        h = self.conv(x)\n",
    "        h = self.gelu(h)\n",
    "        h = self.norm(h.flatten(2).permute(0,2,1))\n",
    "\n",
    "\n",
    "        # generating subsequent mask for target\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(trg.shape[1]).to(trg.device)\n",
    "\n",
    "        # Padding mask\n",
    "        trg_pad_mask = self.make_len_mask(trg)\n",
    "\n",
    "        # Getting postional encoding for target\n",
    "        trg = self.decoder(trg)\n",
    "        trg = self.query_pos(trg)\n",
    "        output = self.transformer_decoder(trg.permute(1,0,2), h.permute(1,0,2), tgt_mask=self.trg_mask, \n",
    "                                  tgt_key_padding_mask=trg_pad_mask.permute(1,0))\n",
    "\n",
    "        return self.vocab(output.transpose(0,1))\n",
    "\n",
    "\n",
    "def make_model(vocab_len, hidden_dim=256, nheads=4,\n",
    "                 num_encoder_layers=4, num_decoder_layers=4):\n",
    "    \n",
    "    return OCR(vocab_len, hidden_dim, nheads,\n",
    "                 num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "\"\"\"\n",
    "Uses generator functions to supply train/test with data.\n",
    "Image renderings and text are created on the fly each time.\n",
    "\"\"\"\n",
    "\n",
    "class DataGenerator(Dataset):\n",
    "    \"\"\"Generator class with data streaming\"\"\"\n",
    "\n",
    "    def __init__(self, source, split, transform, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.split = split\n",
    "        self.dataset = dict()\n",
    "\n",
    "        with h5py.File(source, \"r\") as f:\n",
    "            self.dataset[self.split] = dict()\n",
    "\n",
    "            self.dataset[self.split]['dt'] = np.array(f[self.split]['dt'])\n",
    "            self.dataset[self.split]['gt'] = np.array(f[self.split]['gt'])\n",
    "          \n",
    "            randomize = np.arange(len(self.dataset[self.split]['gt']))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(randomize)\n",
    "\n",
    "            self.dataset[self.split]['dt'] = self.dataset[self.split]['dt'][randomize]\n",
    "            self.dataset[self.split]['gt'] = self.dataset[self.split]['gt'][randomize]\n",
    "\n",
    "            # decode sentences from byte\n",
    "            self.dataset[self.split]['gt'] = [x.decode() for x in self.dataset[self.split]['gt']]\n",
    "            \n",
    "        self.size = len(self.dataset[self.split]['gt'])\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = self.dataset[self.split]['dt'][i]\n",
    "        \n",
    "        #making image compatible with resnet\n",
    "        img = np.repeat(img[..., np.newaxis],3, -1).astype(\"float32\")   \n",
    "#         img = pp.normalization(img).astype(\"float32\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img)\n",
    "            img = aug['image']\n",
    "            \n",
    "#             img = self.transform(img)\n",
    "            \n",
    "        y_train = self.tokenizer.encode(self.dataset[self.split]['gt'][i]) \n",
    "        \n",
    "        #padding till max length\n",
    "        y_train = np.pad(y_train, (0, self.tokenizer.maxlen - len(y_train)))\n",
    "\n",
    "        gt = torch.Tensor(y_train)\n",
    "\n",
    "        return img, gt          \n",
    "\n",
    "    def __len__(self):\n",
    "      return self.size\n",
    "\n",
    "class Tokenizer():\n",
    "    \"\"\"Manager tokens functions and charset/dictionary properties\"\"\"\n",
    "\n",
    "    def __init__(self, chars, max_text_length=128):\n",
    "        self.PAD_TK, self.UNK_TK,self.SOS,self.EOS = \"¶\", \"¤\", \"SOS\", \"EOS\"\n",
    "        self.chars = [self.PAD_TK] + [self.UNK_TK ]+ [self.SOS] + [self.EOS] +list(chars)\n",
    "        self.PAD = self.chars.index(self.PAD_TK)\n",
    "        self.UNK = self.chars.index(self.UNK_TK)\n",
    "\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.maxlen = max_text_length\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to vector\"\"\"\n",
    "        text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"ASCII\").lower()\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        groups = [\"\".join(group) for _, group in groupby(text)]\n",
    "        text = \"\".join([self.UNK_TK.join(list(x)) if len(x) > 1 else x for x in groups])\n",
    "        encoded = []\n",
    "\n",
    "        text = ['SOS'] + list(text) + ['EOS']\n",
    "        for item in text:\n",
    "            index = self.chars.index(item)\n",
    "            index = self.UNK if index == -1 else index\n",
    "            encoded.append(index)\n",
    "\n",
    "        return np.asarray(encoded)\n",
    "\n",
    "    def decode(self, text):\n",
    "        \"\"\"Decode vector to text\"\"\"\n",
    "        \n",
    "        decoded = \"\".join([self.chars[int(x)] for x in text if x > -1])\n",
    "        decoded = self.remove_tokens(decoded)\n",
    "        decoded = pp.text_standardize(decoded)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def remove_tokens(self, text):\n",
    "        \"\"\"Remove tokens (PAD) from text\"\"\"\n",
    "\n",
    "        return text.replace(self.PAD_TK, \"\").replace(self.UNK_TK, \"\")\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 200\n",
    "\n",
    "# define paths\n",
    "#change paths accordingly\n",
    "source = 'iam'\n",
    "source_path = '../data/{}.hdf5'.format(source)\n",
    "# define input size, number max of chars per line and list of valid chars\n",
    "input_size = (1024, 128, 1)\n",
    "max_text_length = 128\n",
    "# charset_base = string.printable[:95]\n",
    "charset_base = string.printable[:36].lower() + string.printable[36+26:95].lower() \n",
    "\n",
    "device = torch.device(\"cuda:3\")\n",
    "\n",
    "# transform = T.Compose([\n",
    "#     T.ToTensor()])\n",
    "tokenizer = Tokenizer(charset_base)\n",
    "import albumentations\n",
    "import albumentations.pytorch\n",
    "\n",
    "\n",
    "# transform_train = albumentations.Compose([\n",
    "#     albumentations.OneOf(\n",
    "#         [\n",
    "#             albumentations.MotionBlur(p=1, blur_limit=8),\n",
    "#             albumentations.OpticalDistortion(p=1, distort_limit=0.05),\n",
    "#             albumentations.GaussNoise(p=1, var_limit=(10.0, 100.0)),\n",
    "#             albumentations.RandomBrightnessContrast(p=1, brightness_limit=0.2),\n",
    "#             albumentations.Downscale(p=1, scale_min=0.3, scale_max=0.5),\n",
    "#         ],\n",
    "#         p=.5,\n",
    "#     ),    \n",
    "#     albumentations.Normalize(),\n",
    "#     albumentations.pytorch.ToTensorV2()\n",
    "\n",
    "# ])\n",
    "\n",
    "transform_valid = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Normalize(),\n",
    "        albumentations.pytorch.ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b6eb24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model( vocab_len=tokenizer.vocab_size,hidden_dim=256, nheads=4,\n",
    "                 num_encoder_layers=4, num_decoder_layers=6)\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a86402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3522dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_grad_flow(named_parameters):\n",
    "#     ave_grads = []\n",
    "#     layers = []\n",
    "#     for n, p in named_parameters:\n",
    "#         if(p.requires_grad) and (\"bias\" not in n):\n",
    "#             layers.append(n)\n",
    "#             ave_grads.append(p.grad.abs().mean().cpu())\n",
    "#     plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "#     plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
    "#     plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "#     plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "#     plt.xlabel(\"Layers\")\n",
    "#     plt.ylabel(\"average gradient\")\n",
    "#     plt.title(\"Gradient flow\")\n",
    "#     plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae08f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82ed35fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.load(\"../output/iam/checkpoint_weights_iam_small_clip_resnet50x16_aug_decoder_only.hdf5\", map_location={\"cuda:0\":\"cuda:3\"})\n",
    "\n",
    "f = {}\n",
    "for i in d:\n",
    "    f[i.replace(\"module.\",\"\")] = d[i]\n",
    "\n",
    "model.load_state_dict(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4471b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory(model,imgs):\n",
    "    x = model.get_feature(imgs)\n",
    "    h = model.conv(x)\n",
    "    h = model.gelu(h)\n",
    "    h = model.norm(h.flatten(2).permute(0,2,1))    \n",
    "    return h.permute(1,0,2)\n",
    "    \n",
    "\n",
    "def test(model, test_loader, max_text_length):\n",
    "    model.eval()\n",
    "    predicts = []\n",
    "    gt = []\n",
    "    imgs = []\n",
    "    c=0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            src, trg = batch\n",
    "            imgs.append(src.flatten(0,1))\n",
    "            src, trg = src.to(device), trg.to(device)            \n",
    "            memory = get_memory(model,src.float())\n",
    "            out_indexes = [tokenizer.chars.index('SOS'), ]\n",
    "            for i in range(max_text_length):\n",
    "                mask = model.generate_square_subsequent_mask(i+1).to(device)\n",
    "                \n",
    "                trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(device)\n",
    "                output = model.vocab(model.transformer_decoder(model.query_pos(model.decoder(trg_tensor)), memory,tgt_mask=mask))\n",
    "                out_token = output.argmax(2)[-1].item()\n",
    "#                 print(output.argmax(2))\n",
    "                out_indexes.append(out_token)\n",
    "                if out_token == tokenizer.chars.index('EOS'):\n",
    "                    break\n",
    "            \n",
    "            predicts.append(tokenizer.decode(out_indexes))\n",
    "            gt.append(tokenizer.decode(trg.flatten(0,1)))\n",
    "#             if c==100:\n",
    "#                 break\n",
    "            c+=1\n",
    "    return predicts, gt, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1a0334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(DataGenerator(source_path,'test',transform_valid, tokenizer), batch_size=1, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3584358e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicts, gt, imgs = test(model, test_loader, max_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6fcc777",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = list(map(lambda x : x.replace('SOS','').replace('EOS',''),predicts))\n",
    "gt = list(map(lambda x : x.replace('SOS','').replace('EOS',''),gt))\n",
    "\n",
    "evaluate = evaluation.ocr_metrics(predicts=predicts,\n",
    "                                  ground_truth=gt,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1aa2f919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08694478, 0.23839992, 0.83438596])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed6add04",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1,3,224,224).cuda()\n",
    "b = torch.rand(1,128).cuda().long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bf64679",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.LongTensor(tokenizer.encode(gt[0])).cuda().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dc692cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(imgs[0].unsqueeze(0).cuda(),c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f73fba28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tomeuiti opitle ah tnpeanne , ii ae atould boerEOSdpEOS'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output.argmax(2)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04cd47cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ttm uati ositoe mh tnpeanngtii terstould bielsnp'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output.log_softmax(-1).contiguous().view(-1, tokenizer.vocab_size).argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b4f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
