{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783faa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mhamdan/seq2seqAttenHTR/Transformer_ocr/src\n",
      "source: ../data/washington.hdf5\n",
      "output ../output/washington\n",
      "target ../output/washington/checkpoint_weights_iam_dsa.hdf5\n",
      "charset: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n"
     ]
    }
   ],
   "source": [
    "%cd src/\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import groupby\n",
    "import h5py\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50, resnet101\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from data import preproc as pp\n",
    "from data import evaluation\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import timm\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=128):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class OCR(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_len, hidden_dim, nheads,\n",
    "                 num_encoder_layers, num_decoder_layers):\n",
    "        super().__init__()\n",
    "    \n",
    "#         self.backbone = timm.create_model('ecaresnet101d', pretrained=True,)\n",
    "#         del self.backbone.fc\n",
    "        self.backbone = resnet101(pretrained=True)\n",
    "        del self.backbone.fc\n",
    "        \n",
    "        \n",
    "#         del self.backbone.classifier, self.backbone.conv_head, self.backbone.bn2,self.backbone.act2,self.backbone.global_pool\n",
    "        _ = self.backbone.to(\"cpu\")\n",
    "#         for name,p in self.backbone.named_parameters():\n",
    "#             if \"bn\" not in name or \"attnpool\" in name:\n",
    "#                 p.requires_grad =  False\n",
    "\n",
    "        # create a default PyTorch transformer\n",
    "        # create conversion layer\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "\n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "        # prediction heads with length of vocab\n",
    "        # DETR used basic 3 layer MLP for output\n",
    "        self.vocab = nn.Linear(hidden_dim,vocab_len)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.decoder = nn.Embedding(vocab_len, hidden_dim)\n",
    "        self.query_pos = PositionalEncoding(hidden_dim, .2)\n",
    "\n",
    "        # spatial positional encodings, sine positional encoding can be used.\n",
    "        # Detr baseline uses sine positional encoding.\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.trg_mask = None\n",
    "  \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), 1)\n",
    "        mask = mask.masked_fill(mask==1, float('-inf'))\n",
    "        return mask\n",
    "    \n",
    "#     def get_feature(self,x):\n",
    "#             x = self.backbone.conv_stem(x)\n",
    "#             x = self.backbone.bn1(x)   \n",
    "#             x = self.backbone.act1(x)\n",
    "#             x = self.backbone.blocks(x)\n",
    "#             return x\n",
    "\n",
    "    def get_feature(self,x):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)   \n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "        return x\n",
    "\n",
    "#     def get_feature(self, x):\n",
    "#         x = self.backbone.forward_features(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "    def make_len_mask(self, inp):\n",
    "        return (inp == 0).transpose(0, 1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, trg):\n",
    "        # propagate inputs through ResNet-101 up to avg-pool layer\n",
    "        x = self.get_feature(inputs)\n",
    "\n",
    "        # convert from 2048 to 256 feature planes for the transformer\n",
    "        h = self.conv(x)\n",
    "\n",
    "        # construct positional encodings\n",
    "        bs,_,H, W = h.shape\n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "        # generating subsequent mask for target\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(trg.shape[1]).to(trg.device)\n",
    "\n",
    "        # Padding mask\n",
    "        trg_pad_mask = self.make_len_mask(trg)\n",
    "\n",
    "        # Getting postional encoding for target\n",
    "        trg = self.decoder(trg)\n",
    "        trg = self.query_pos(trg)\n",
    "        \n",
    "        output = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1), trg.permute(1,0,2), tgt_mask=self.trg_mask, \n",
    "                                  tgt_key_padding_mask=trg_pad_mask.permute(1,0))\n",
    "\n",
    "        return self.vocab(output.transpose(0,1))\n",
    "\n",
    "\n",
    "def make_model(vocab_len, hidden_dim=256, nheads=4,\n",
    "                 num_encoder_layers=4, num_decoder_layers=4):\n",
    "    \n",
    "    return OCR(vocab_len, hidden_dim, nheads,\n",
    "                 num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "\"\"\"\n",
    "Uses generator functions to supply train/test with data.\n",
    "Image renderings and text are created on the fly each time.\n",
    "\"\"\"\n",
    "\n",
    "class DataGenerator(Dataset):\n",
    "    \"\"\"Generator class with data streaming\"\"\"\n",
    "\n",
    "    def __init__(self, source, split, transform, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.split = split\n",
    "        self.dataset = dict()\n",
    "\n",
    "        with h5py.File(source, \"r\") as f:\n",
    "            self.dataset[self.split] = dict()\n",
    "\n",
    "            self.dataset[self.split]['dt'] = np.array(f[self.split]['dt'])\n",
    "            self.dataset[self.split]['gt'] = np.array(f[self.split]['gt'])\n",
    "          \n",
    "            randomize = np.arange(len(self.dataset[self.split]['gt']))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(randomize)\n",
    "\n",
    "            self.dataset[self.split]['dt'] = self.dataset[self.split]['dt'][randomize]\n",
    "            self.dataset[self.split]['gt'] = self.dataset[self.split]['gt'][randomize]\n",
    "\n",
    "            # decode sentences from byte\n",
    "            self.dataset[self.split]['gt'] = [x.decode() for x in self.dataset[self.split]['gt']]\n",
    "            \n",
    "        self.size = len(self.dataset[self.split]['gt'])\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = self.dataset[self.split]['dt'][i]\n",
    "        \n",
    "        #making image compatible with resnet\n",
    "        img = np.repeat(img[..., np.newaxis],3, -1).astype(\"float32\")   \n",
    "#         img = pp.normalization(img).astype(\"float32\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img)\n",
    "            img = aug['image']\n",
    "            \n",
    "#             img = self.transform(img)\n",
    "            \n",
    "        y_train = self.tokenizer.encode(self.dataset[self.split]['gt'][i]) \n",
    "        \n",
    "        #padding till max length\n",
    "        y_train = np.pad(y_train, (0, self.tokenizer.maxlen - len(y_train)))\n",
    "\n",
    "        gt = torch.Tensor(y_train)\n",
    "\n",
    "        return img, gt          \n",
    "\n",
    "    def __len__(self):\n",
    "      return self.size\n",
    "\n",
    "class Tokenizer():\n",
    "    \"\"\"Manager tokens functions and charset/dictionary properties\"\"\"\n",
    "\n",
    "    def __init__(self, chars, max_text_length=128):\n",
    "        self.PAD_TK, self.UNK_TK,self.SOS,self.EOS = \"¶\", \"¤\", \"SOS\", \"EOS\"\n",
    "        self.chars = [self.PAD_TK] + [self.UNK_TK ]+ [self.SOS] + [self.EOS] +list(chars)\n",
    "        self.PAD = self.chars.index(self.PAD_TK)\n",
    "        self.UNK = self.chars.index(self.UNK_TK)\n",
    "\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.maxlen = max_text_length\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to vector\"\"\"\n",
    "        text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        groups = [\"\".join(group) for _, group in groupby(text)]\n",
    "        text = \"\".join([self.UNK_TK.join(list(x)) if len(x) > 1 else x for x in groups])\n",
    "        encoded = []\n",
    "\n",
    "        text = ['SOS'] + list(text) + ['EOS']\n",
    "        for item in text:\n",
    "            index = self.chars.index(item)\n",
    "            index = self.UNK if index == -1 else index\n",
    "            encoded.append(index)\n",
    "\n",
    "        return np.asarray(encoded)\n",
    "\n",
    "    def decode(self, text):\n",
    "        \"\"\"Decode vector to text\"\"\"\n",
    "        \n",
    "        decoded = \"\".join([self.chars[int(x)] for x in text if x > -1])\n",
    "        decoded = self.remove_tokens(decoded)\n",
    "        decoded = pp.text_standardize(decoded)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def remove_tokens(self, text):\n",
    "        \"\"\"Remove tokens (PAD) from text\"\"\"\n",
    "\n",
    "        return text.replace(self.PAD_TK, \"\").replace(self.UNK_TK, \"\")\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 200\n",
    "\n",
    "# define paths\n",
    "#change paths accordingly\n",
    "source = 'washington'\n",
    "source_path = '../data/{}.hdf5'.format(source)\n",
    "output_path = os.path.join(\"..\", \"output\", source)\n",
    "target_path = os.path.join(output_path, \"checkpoint_weights_iam_{}.hdf5\".format(\"dsa\"))\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# define input size, number max of chars per line and list of valid chars\n",
    "input_size = (1024, 128, 1)\n",
    "max_text_length = 128\n",
    "charset_base = string.printable[:95]\n",
    "# charset_base = string.printable[:36].lower() + string.printable[36+26:95].lower() \n",
    "\n",
    "print(\"source:\", source_path)\n",
    "print(\"output\", output_path)\n",
    "print(\"target\", target_path)\n",
    "print(\"charset:\", charset_base)\n",
    "\n",
    "import torchvision.transforms as T\n",
    "local_rank = 1\n",
    "device = torch.device(\"cuda:{}\".format(local_rank))\n",
    "\n",
    "# transform = T.Compose([\n",
    "#     T.ToTensor()])\n",
    "tokenizer = Tokenizer(charset_base)\n",
    "import albumentations\n",
    "import albumentations.pytorch\n",
    "\n",
    "\n",
    "\n",
    "transform_valid = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Normalize(),\n",
    "        albumentations.pytorch.ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f5cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(charset_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "154fd234",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model( vocab_len=tokenizer.vocab_size,hidden_dim=256, nheads=4,\n",
    "                 num_encoder_layers=4, num_decoder_layers=4)\n",
    "\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ced1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edc05f4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = torch.load(\"/home/mhamdan/seq2seqAttenHTR/Transformer_ocr/output/honest-yogurt-19/washington_firstbest.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2ecb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = {}\n",
    "for i in d:\n",
    "    f[i.replace(\"module.\",\"\")] = d[i]\n",
    "\n",
    "model.load_state_dict(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "50b50345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory(model,imgs):\n",
    "    x = model.conv(model.get_feature(imgs))\n",
    "    bs,_,H, W = x.shape\n",
    "    pos = torch.cat([\n",
    "            model.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            model.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "    return model.transformer.encoder(pos +  0.1 * x.flatten(2).permute(2, 0, 1))\n",
    "    \n",
    "\n",
    "def test(model, test_loader, max_text_length,beam_size=3):\n",
    "    model.eval()\n",
    "    predicts = []\n",
    "    gt = []\n",
    "    imgs = []\n",
    "    c=0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            break_counter=0            \n",
    "            src, trg = batch\n",
    "            imgs.append(src.flatten(0,1))\n",
    "            src, trg = src.to(device), trg.to(device)            \n",
    "            memory = get_memory(model,src.float())\n",
    "            out_indexes = [[[tokenizer.chars.index('SOS')],0] ]\n",
    "            final = []\n",
    "            for i in range(max_text_length):            \n",
    "                all_paths_local = []\n",
    "                for path,proba in out_indexes[::-1]:\n",
    "                    if path[-1]==tokenizer.chars.index('EOS'):\n",
    "                        final.append([path, proba])\n",
    "                        continue\n",
    "                    mask = model.generate_square_subsequent_mask(i+1).to(device)\n",
    "                    trg_tensor = torch.LongTensor(path).unsqueeze(1).to(device)\n",
    "                    output = model.vocab(model.transformer.decoder(model.query_pos(model.decoder(trg_tensor)), memory,tgt_mask=mask))\n",
    "                    probab, indexes = torch.topk(output.log_softmax(-1),beam_size)\n",
    "                    probab, indexes = probab.flatten(), indexes.flatten()\n",
    "                    \n",
    "                    for k in range(beam_size):\n",
    "                        words,prob = path.copy(), proba\n",
    "                        words.append(int(indexes[k].cpu().numpy()))\n",
    "                        prob += probab[k].cpu().numpy()\n",
    "                        all_paths_local.append([words,prob])\n",
    "\n",
    "                out_indexes = sorted(all_paths_local, reverse=False, key=lambda l: l[1])[-beam_size:]\n",
    "    #             print(\"out\",out_indexes)\n",
    "#                 if break_counter:\n",
    "#                     break\n",
    "                if len(final)==beam_size:\n",
    "                    break\n",
    "            path = sorted(final, reverse=False, key=lambda l: l[1])[-1][0]\n",
    "            predicts.append(tokenizer.decode(path))\n",
    "            gt.append(tokenizer.decode(trg.flatten(0,1)))\n",
    "    \n",
    "#             if c==2:\n",
    "#                 break\n",
    "            c+=1\n",
    "    return predicts, gt, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1865a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(DataGenerator(source_path,'test',transform_valid, tokenizer), batch_size=1, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "069b62fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts, gt, imgs = test(model, test_loader, max_text_length, beam_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7ca0344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = test(model, test_loader, max_text_length, beam_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "99ad8350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     print(tokenizer.decode(final[i][0]), final[i][1])\n",
    "#     print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4a3eea28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['SOSLmetters Orders and Instructions Becember 1755 , EOS',\n",
       "  'SOSarrive at Frederic5burgh , between this andEOS',\n",
       "  'SOShereof , to repaur to Winchester , where you will metEOS'],\n",
       " ['SOSletters orders and instructions december 1755 . EOS',\n",
       "  'SOSarrive at fredericksburgh , between this andEOS',\n",
       "  'SOShereof , to repair to winchester , where you will meetEOS'])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[:3],gt[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ddf1d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['SOSLetters Orders and Instructions Becember 1755 , EOS',\n",
       "  'SOSarrive at Frederichsburgh , between this andEOS',\n",
       "  'SOShereof , to repar to Winchester , where you will metEOS'],\n",
       " ['SOSletters orders and instructions december 1755 . EOS',\n",
       "  'SOSarrive at fredericksburgh , between this andEOS',\n",
       "  'SOShereof , to repair to winchester , where you will meetEOS'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[:3],gt[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "04dec56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = list(map(lambda x : x.replace('SOS','').replace('EOS',''),predicts))\n",
    "gt = list(map(lambda x : x.replace('SOS','').replace('EOS',''),gt))\n",
    "\n",
    "evaluate = evaluation.ocr_metrics(predicts=predicts,\n",
    "                                  ground_truth=gt,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "36e6ac44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26811312, 0.38611692, 0.91772152])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "832efab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0789446 , 0.22282633, 0.81012658])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0873ed41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08521912, 0.24903093, 0.86245614])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5519124c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Letters Orders and Instructions Becember 1755 , ',\n",
       " 'arrive at Frederichsburgh , between this and',\n",
       " 'hereof , to repar to Winchester , where you will met',\n",
       " 'at unfit for Duty , on Review ; are ordered to be',\n",
       " 'mediately advertising , and sending a party or']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "736b7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a5d9ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhimanshu13\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/himanshu13/pretrained/runs/2s33z1dy\" target=\"_blank\">curious-water-1</a></strong> to <a href=\"https://wandb.ai/himanshu13/pretrained\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    run = wandb.init(\n",
    "        config={\"g\":1},\n",
    "        project=\"pretrained\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da382265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'curious-water-1'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68f8e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"../data/rimes.hdf5\"\n",
    "split = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d543487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dict()\n",
    "\n",
    "with h5py.File(source, \"r\") as f:\n",
    "    dataset[split] = dict()\n",
    "\n",
    "    dataset[split]['dt'] = np.array(f[split]['dt'])\n",
    "    dataset[split]['gt'] = np.array(f[split]['gt'])\n",
    "\n",
    "    randomize = np.arange(len(dataset[split]['gt']))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(randomize)\n",
    "\n",
    "    dataset[split]['dt'] = dataset[split]['dt'][randomize]\n",
    "    dataset[split]['gt'] = dataset[split]['gt'][randomize]\n",
    "\n",
    "    # decode sentences from byte\n",
    "    dataset[split]['gt'] = [x.decode() for x in dataset[split]['gt']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74039fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Merci de faire le necessaire'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][\"gt\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4d0b3bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"../raw/iam/lines/b01/b01-000/b01-000-00.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9351f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c012c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((140, 1856, 3), (140, 1856))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape, gray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8c744b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = pp.preprocess(\"../raw/iam/lines/a04/a04-000/a04-000-00.png\", (1024, 128, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dab77cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "imm = pp.remove_cursive_style(gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e5482d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"temp.png\",imm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9c32b52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 128)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610953d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
