{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262fdad2",
   "metadata": {},
   "source": [
    "This code defines a PositionalEncoding class that inherits from the PyTorch's nn.Module class. The purpose of this class is to provide a method for adding positional information to an input tensor, which is useful in models like transformers that rely on self-attention mechanisms. It has a constructor method and a forward method. In the constructor, it creates a positional encoding matrix using sine and cosine functions, and then registers this matrix as a buffer so that it is not considered as a learnable parameter. In the forward method, it adds the positional encoding matrix to the input tensor and applies dropout, returning the resulting tensor.\n",
    "\n",
    "As an academic researcher:\n",
    "The PositionalEncoding class implements a technique used in Transformer-based models to incorporate information about the position of elements in a sequence. Since transformers use self-attention mechanisms that are permutation invariant, they lack inherent information about the position of the elements. Positional encoding is designed to address this issue by injecting positional information into the input embeddings.\n",
    "\n",
    "In this implementation, the positional encoding is achieved by computing sine and cosine functions of the position with different frequencies, as proposed in the \"Attention is All You Need\" paper by Vaswani et al. The resulting positional encoding matrix has dimensions (max_len, d_model), where max_len is the maximum sequence length and d_model is the dimension of the input embeddings. The forward method adds the positional encoding matrix to the input tensor and applies dropout for regularization. This way, the model can utilize the positional information during training and inference to make better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeadcef",
   "metadata": {},
   "source": [
    "## Here's the mathematical equation/formula for the positional encoding in LaTeX format:\n",
    "\n",
    "PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
    "\n",
    "PE(pos, 2i + 1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
    "\n",
    "where:\n",
    "\n",
    "$PE$ is the positional encoding matrix\n",
    "$pos$ is the position in the sequence (ranging from 0 to max_len - 1)\n",
    "$i$ is the dimension index in the input embeddings (ranging from 0 to $d_{model}$/2 - 1)\n",
    "$d_{model}$ is the dimension of the input embeddings\n",
    "These equations compute the sine and cosine values for the even and odd dimensions of the positional encoding matrix, respectively. The sinusoidal functions have different frequencies, controlled by the $10000^{2i / d_{model}}$ term, which allows the model to learn to attend to relative positions in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687eaa7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
