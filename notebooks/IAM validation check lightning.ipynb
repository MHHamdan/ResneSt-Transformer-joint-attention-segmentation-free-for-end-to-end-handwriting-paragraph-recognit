{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mhamdan/seq2seqAttenHTR/Transformer_ocr/src\n"
     ]
    }
   ],
   "source": [
    "%cd src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import groupby\n",
    "import h5py\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50, resnet101\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from data import preproc as pp\n",
    "from data import evaluation\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "import pytorch_lightning as pl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(LightningModule):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=128):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx=0, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
    "\n",
    "    \n",
    "class OCR(LightningModule):\n",
    "\n",
    "    def __init__(self, vocab_len, hidden_dim, nheads,\n",
    "                 num_encoder_layers, num_decoder_layers, tokenizer, len_train_loader, epoch):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.criterion = LabelSmoothing(size=tokenizer.vocab_size, padding_idx=0, smoothing=0.1)\n",
    "        self.automatic_optimization = False\n",
    "        self.len_train_loader = len_train_loader\n",
    "        self.epoch = epoch\n",
    "        # create ResNet-101 backbone\n",
    "        self.backbone = resnet101()\n",
    "        del self.backbone.fc\n",
    "\n",
    "        # create conversion layer\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "\n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "        # prediction heads with length of vocab\n",
    "        # DETR used basic 3 layer MLP for output\n",
    "        self.vocab = nn.Linear(hidden_dim,vocab_len)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.decoder = nn.Embedding(vocab_len, hidden_dim)\n",
    "        self.query_pos = PositionalEncoding(hidden_dim, .2)\n",
    "\n",
    "        # spatial positional encodings, sine positional encoding can be used.\n",
    "        # Detr baseline uses sine positional encoding.\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.trg_mask = None\n",
    "  \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), 1)\n",
    "        mask = mask.masked_fill(mask==1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def get_feature(self,x):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)   \n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def make_len_mask(self, inp):\n",
    "        return (inp == 0).transpose(0, 1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, trg):\n",
    "        # propagate inputs through ResNet-101 up to avg-pool layer\n",
    "        x = self.get_feature(inputs)\n",
    "\n",
    "        # convert from 2048 to 256 feature planes for the transformer\n",
    "        h = self.conv(x)\n",
    "\n",
    "        # construct positional encodings\n",
    "        bs,_,H, W = h.shape\n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "        # generating subsequent mask for target\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(trg.shape[1]).to(trg.device)\n",
    "\n",
    "        # Padding mask\n",
    "        trg_pad_mask = self.make_len_mask(trg)\n",
    "\n",
    "        # Getting postional encoding for target\n",
    "        trg = self.decoder(trg)\n",
    "        trg = self.query_pos(trg)\n",
    "        \n",
    "        output = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1), trg.permute(1,0,2), tgt_mask=self.trg_mask, \n",
    "                                  tgt_key_padding_mask=trg_pad_mask.permute(1,0))\n",
    "\n",
    "        return self.vocab(output.transpose(0,1))\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(),lr = 1e-4)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer = optimizer,\n",
    "            max_lr = 1e-4,\n",
    "            pct_start = 0.05,\n",
    "            anneal_strategy = \"linear\",\n",
    "            epochs = self.epoch,\n",
    "            steps_per_epoch = self.len_train_loader\n",
    "        )        \n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        opt = self.optimizers()\n",
    "        sch = self.lr_schedulers()\n",
    "        opt.zero_grad()\n",
    "        output = self(x.float(), y.long()[:,:-1])\n",
    "        norm = (y != 0).sum()\n",
    "        loss = self.criterion(output.log_softmax(-1).contiguous().view(-1, tokenizer.vocab_size), y[:,1:].contiguous().view(-1).long()) / norm        \n",
    "        self.manual_backward(loss)\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(), 0.2)        \n",
    "        opt.step()        \n",
    "        sch.step()\n",
    "        self.log(\"training_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True,sync_dist=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self(x.float(), y.long()[:,:-1])\n",
    "        norm = (y != 0).sum()\n",
    "        loss = self.criterion(output.log_softmax(-1).contiguous().view(-1, tokenizer.vocab_size), labels_y[:,1:].contiguous().view(-1).long()) / norm        \n",
    "        self.log(\"validation_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True,sync_dist=True)        \n",
    "        return loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Uses generator functions to supply train/test with data.\n",
    "Image renderings and text are created on the fly each time.\n",
    "\"\"\"\n",
    "\n",
    "class DataGenerator(Dataset):\n",
    "    \"\"\"Generator class with data streaming\"\"\"\n",
    "\n",
    "    def __init__(self, source, split, transform, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.split = split\n",
    "        self.dataset = dict()\n",
    "\n",
    "        with h5py.File(source, \"r\") as f:\n",
    "            self.dataset[self.split] = dict()\n",
    "\n",
    "            self.dataset[self.split]['dt'] = np.array(f[self.split]['dt'])\n",
    "            self.dataset[self.split]['gt'] = np.array(f[self.split]['gt'])\n",
    "          \n",
    "            randomize = np.arange(len(self.dataset[self.split]['gt']))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(randomize)\n",
    "\n",
    "            self.dataset[self.split]['dt'] = self.dataset[self.split]['dt'][randomize]\n",
    "            self.dataset[self.split]['gt'] = self.dataset[self.split]['gt'][randomize]\n",
    "\n",
    "            # decode sentences from byte\n",
    "            self.dataset[self.split]['gt'] = [x.decode() for x in self.dataset[self.split]['gt']]\n",
    "            \n",
    "        self.size = len(self.dataset[self.split]['gt'])\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = self.dataset[self.split]['dt'][i]\n",
    "        \n",
    "        #making image compatible with resnet\n",
    "        img = np.repeat(img[..., np.newaxis],3, -1)    \n",
    "        img = pp.normalization(img)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        y_train = self.tokenizer.encode(self.dataset[self.split]['gt'][i]) \n",
    "        \n",
    "        #padding till max length\n",
    "        y_train = np.pad(y_train, (0, self.tokenizer.maxlen - len(y_train)))\n",
    "\n",
    "        gt = torch.Tensor(y_train)\n",
    "\n",
    "        return img, gt          \n",
    "\n",
    "    def __len__(self):\n",
    "      return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: ../data/iam.hdf5\n",
      "output ../output/iam\n",
      "target ../output/iam/checkpoint_weights_iam.hdf5\n",
      "charset: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n"
     ]
    }
   ],
   "source": [
    "class Tokenizer():\n",
    "    \"\"\"Manager tokens functions and charset/dictionary properties\"\"\"\n",
    "\n",
    "    def __init__(self, chars, max_text_length=128):\n",
    "        self.PAD_TK, self.UNK_TK,self.SOS,self.EOS = \"¶\", \"¤\", \"SOS\", \"EOS\"\n",
    "        self.chars = [self.PAD_TK] + [self.UNK_TK ]+ [self.SOS] + [self.EOS] +list(chars)\n",
    "        self.PAD = self.chars.index(self.PAD_TK)\n",
    "        self.UNK = self.chars.index(self.UNK_TK)\n",
    "\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.maxlen = max_text_length\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to vector\"\"\"\n",
    "\n",
    "        text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        groups = [\"\".join(group) for _, group in groupby(text)]\n",
    "        text = \"\".join([self.UNK_TK.join(list(x)) if len(x) > 1 else x for x in groups])\n",
    "        encoded = []\n",
    "\n",
    "        text = ['SOS'] + list(text) + ['EOS']\n",
    "        for item in text:\n",
    "            index = self.chars.index(item)\n",
    "            index = self.UNK if index == -1 else index\n",
    "            encoded.append(index)\n",
    "\n",
    "        return np.asarray(encoded)\n",
    "\n",
    "    def decode(self, text):\n",
    "        \"\"\"Decode vector to text\"\"\"\n",
    "        \n",
    "        decoded = \"\".join([self.chars[int(x)] for x in text if x > -1])\n",
    "        decoded = self.remove_tokens(decoded)\n",
    "        decoded = pp.text_standardize(decoded)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def remove_tokens(self, text):\n",
    "        \"\"\"Remove tokens (PAD) from text\"\"\"\n",
    "\n",
    "        return text.replace(self.PAD_TK, \"\").replace(self.UNK_TK, \"\")\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 200\n",
    "\n",
    "# define paths\n",
    "#change paths accordingly\n",
    "source = 'iam'\n",
    "source_path = '../data/{}.hdf5'.format(source)\n",
    "output_path = os.path.join(\"..\", \"output\", source)\n",
    "target_path = os.path.join(output_path, \"checkpoint_weights_iam.hdf5\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# define input size, number max of chars per line and list of valid chars\n",
    "input_size = (1024, 128, 1)\n",
    "max_text_length = 128\n",
    "charset_base = string.printable[:95]\n",
    "\n",
    "print(\"source:\", source_path)\n",
    "print(\"output\", output_path)\n",
    "print(\"target\", target_path)\n",
    "print(\"charset:\", charset_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "transform = T.Compose([\n",
    "    T.ToTensor()])\n",
    "tokenizer = Tokenizer(charset_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, source_path, batch_size, tokenizer, transform):\n",
    "        super().__init__()\n",
    "        self.source_path = source_path        \n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def prepare_data(self):\n",
    "\n",
    "        self.train = DataGenerator(\n",
    "            self.source_path,\n",
    "            \"train\",\n",
    "            transform,\n",
    "            tokenizer\n",
    "        )\n",
    "\n",
    "        self.valid = DataGenerator(\n",
    "            self.source_path,\n",
    "            \"valid\",\n",
    "            transform,\n",
    "            tokenizer            \n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "\n",
    "        return torch.utils.dataDataLoader(\n",
    "            self.train, batch_size=self.batch_size, shuffle=True, num_workers=6\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "\n",
    "        return torch.utils.dataDataLoader(\n",
    "            self.valid, batch_size=self.batch_size, shuffle=False, num_workers=6\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataModule(source_path, 64, tokenizer, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhamdan/miniconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "model = OCR(tokenizer.vocab_size, 256, 4,\n",
    "                 4, 4, tokenizer, 6161, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=\"../output/\",\n",
    "    filename=\"IAM-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=2,\n",
    "    mode=\"min\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=4, max_epochs=200,accelerator='ddp', callbacks=[checkpoint_callback])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OCR(tokenizer.vocab_size, 256, 4,\n",
    "                 4, 4, tokenizer, 6161, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhamdan/miniconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "m1 = OCR.load_from_checkpoint(\"../output/IAM-epoch=149-validation_loss_epoch=1.61.ckpt\",\n",
    "                              vocab_len=tokenizer.vocab_size,\n",
    "    hidden_dim=256,\n",
    "    nheads=4,\n",
    "    num_encoder_layers=4,\n",
    "    num_decoder_layers=4,\n",
    "    tokenizer=tokenizer,\n",
    "    len_train_loader=6161,\n",
    "    epoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../output/IAM-epoch=149-validation_loss_epoch=1.61.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'callbacks', 'optimizer_states', 'lr_schedulers'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory(model,imgs):\n",
    "    x = model.conv(model.get_feature(imgs))\n",
    "    bs,_,H, W = x.shape\n",
    "    pos = torch.cat([\n",
    "            model.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            model.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "    return model.transformer.encoder(pos +  0.1 * x.flatten(2).permute(2, 0, 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, max_text_length):\n",
    "    model.eval()\n",
    "    predicts = []\n",
    "    gt = []\n",
    "    imgs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            src, trg = batch\n",
    "            imgs.append(src.flatten(0,1))\n",
    "            src, trg = src.cuda(), trg.cuda()            \n",
    "            memory = get_memory(model,src.float())\n",
    "            out_indexes = [tokenizer.chars.index('SOS'), ]\n",
    "            for i in range(max_text_length):\n",
    "                mask = model.generate_square_subsequent_mask(i+1).to(device)\n",
    "                trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(device)\n",
    "                output = model.vocab(model.transformer.decoder(model.query_pos(model.decoder(trg_tensor)), memory,tgt_mask=mask))\n",
    "                out_token = output.argmax(2)[-1].item()\n",
    "                out_indexes.append(out_token)\n",
    "                if out_token == tokenizer.chars.index('EOS'):\n",
    "                    break\n",
    "            predicts.append(tokenizer.decode(out_indexes))\n",
    "            gt.append(tokenizer.decode(trg.flatten(0,1)))\n",
    "    return predicts, gt, imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device ='cuda'\n",
    "_ =  m1.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(DataGenerator(source_path,'test',transform, tokenizer), batch_size=1, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhamdan/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "predicts, gt, imgs = test(m1, test_loader, max_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11587/2868241250.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SOS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'EOS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SOS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'EOS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predicts' is not defined"
     ]
    }
   ],
   "source": [
    "predicts = list(map(lambda x : x.replace('SOS','').replace('EOS',''),predicts))\n",
    "gt = list(map(lambda x : x.replace('SOS','').replace('EOS',''),gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11587/1344142129.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m evaluate = evaluation.ocr_metrics(predicts=predicts,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                   ground_truth=gt,)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluation' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate = evaluation.ocr_metrics(predicts=predicts,\n",
    "                                  ground_truth=gt,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72102259, 0.93024818, 1.        ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
