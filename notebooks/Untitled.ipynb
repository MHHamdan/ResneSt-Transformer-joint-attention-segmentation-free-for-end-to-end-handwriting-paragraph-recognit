{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb081f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhamdan/miniconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import timm\n",
    "import random\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import string\n",
    "from torch.autograd import Variable\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "def set_random_seeds(random_seed=0):\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_random_seeds(random_seed=13)\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    debug = False\n",
    "    batch_size = 200\n",
    "    num_workers = 6\n",
    "    head_lr = 0.0006\n",
    "    image_encoder_lr = 0.0001\n",
    "    text_encoder_lr = 0.0001\n",
    "    weight_decay = 1e-3\n",
    "    patience = 5\n",
    "    factor = 0.8\n",
    "    epochs = 200\n",
    "    device = torch.device(\"cuda:1\")\n",
    "\n",
    "    image_embedding = 2048\n",
    "    text_embedding = 300\n",
    "    max_length = 30\n",
    "\n",
    "    pretrained = True # for both image encoder and text encoder\n",
    "    trainable = True # for both image encoder and text encoder\n",
    "    temperature = 1.0\n",
    "\n",
    "    # for projection head; used for both image and text encoders\n",
    "    num_projection_layers = 1\n",
    "    projection_dim = 256 \n",
    "    dropout = 0.1\n",
    "    \n",
    "class Tokenizer():\n",
    "    \"\"\"Manager tokens functions and charset/dictionary properties\"\"\"\n",
    "\n",
    "    def __init__(self, chars, max_text_length=CFG.max_length):\n",
    "        self.PAD_TK, self.UNK_TK,self.SOS,self.EOS = \"¶\", \"¤\", \"SOS\", \"EOS\"\n",
    "        self.chars = [self.PAD_TK] + [self.UNK_TK ]+ [self.SOS] + [self.EOS] +list(chars)\n",
    "        self.PAD = self.chars.index(self.PAD_TK)\n",
    "        self.UNK = self.chars.index(self.UNK_TK)\n",
    "\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.maxlen = max_text_length\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to vector\"\"\"\n",
    "        text = text.decode(\"utf-8\") \n",
    "        text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
    "#         text = \" \".join(text.split())\n",
    "\n",
    "#         groups = [\"\".join(group) for _, group in groupby(text)]\n",
    "#         text = \"\".join([self.UNK_TK.join(list(x)) if len(x) > 1 else x for x in groups])\n",
    "        encoded = []\n",
    "\n",
    "        text = ['SOS'] + list(text) + ['EOS']\n",
    "        for item in text:\n",
    "            index = self.chars.index(item)\n",
    "            index = self.UNK if index == -1 else index\n",
    "            encoded.append(index)\n",
    "\n",
    "        return np.asarray(encoded)\n",
    "\n",
    "    def decode(self, text):\n",
    "        \"\"\"Decode vector to text\"\"\"\n",
    "        \n",
    "        decoded = \"\".join([self.chars[int(x)] for x in text if x > -1])\n",
    "        decoded = self.remove_tokens(decoded)\n",
    "        decoded = pp.text_standardize(decoded)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def remove_tokens(self, text):\n",
    "        \"\"\"Remove tokens (PAD) from text\"\"\"\n",
    "\n",
    "        return text.replace(self.PAD_TK, \"\").replace(self.UNK_TK, \"\")\n",
    "\n",
    "class DataGenerator(Dataset):\n",
    "    \"\"\"Generator class with data streaming\"\"\"\n",
    "\n",
    "    def __init__(self, source, split, transform, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.split = split\n",
    "        self.dataset = dict()\n",
    "\n",
    "#         self.dataset = h5py.File(source, \"r\")\n",
    "        with h5py.File(source, \"r\") as f:\n",
    "            self.dataset[self.split] = dict()\n",
    "\n",
    "            self.dataset[self.split]['dt'] = np.array(f[self.split]['dt'])\n",
    "            self.dataset[self.split]['gt'] = np.array(f[self.split]['gt'])\n",
    "#             self.dataset[self.split]['label'] = np.array(f[self.split]['label'])            \n",
    "          \n",
    "#             randomize = np.arange(len(self.dataset[self.split]['gt']))\n",
    "#             np.random.seed(42)\n",
    "#             np.random.shuffle(randomize)\n",
    "\n",
    "#             self.dataset[self.split]['dt'] = self.dataset[self.split]['dt'][randomize]\n",
    "#             self.dataset[self.split]['gt'] = self.dataset[self.split]['gt'][randomize]\n",
    "#         print(self.dataset[self.split]['gt'].shape)\n",
    "    \n",
    "        self.size = len(self.dataset[self.split]['gt'])\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = self.dataset[self.split]['dt'][i]\n",
    "        #making image compatible with resnet\n",
    "#         img = cv2.transpose(img)\n",
    "#         img = np.repeat(img[..., np.newaxis],3, -1).astype(\"float32\")   \n",
    "#         img = pp.normalization(img).astype(\"float32\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img)\n",
    "            img = aug['image']\n",
    "            \n",
    "            \n",
    "#             img = self.transform(img)\n",
    "        y_train = self.tokenizer.encode(self.dataset[self.split]['gt'][i].lower()) \n",
    "#         print(self.dataset[self.split]['gt'][i])\n",
    "#         print(len(self.dataset[self.split]['gt'][i]))\n",
    "#         if len(y_train)==0:\n",
    "#             asdas\n",
    "#         print(y_train)\n",
    "#         print()\n",
    "        #padding till max length\n",
    "        y_train = np.pad(y_train, (0, self.tokenizer.maxlen - len(y_train)))\n",
    "#         if all(y_train==0):\n",
    "#             print(self.dataset[self.split]['gt'][i])\n",
    "#             print(\"afdas\")\n",
    "#             ssa\n",
    "        gt = torch.Tensor(y_train)\n",
    "#         label = self.dataset[self.split]['label'][i]\n",
    "        label = 1        \n",
    "        if label==0:\n",
    "            label = -1\n",
    "            \n",
    "        return img, gt,label         \n",
    "\n",
    "    def __len__(self):\n",
    "      return self.size\n",
    "\n",
    "charset_base = string.printable[:95]\n",
    "tokenizer = Tokenizer(charset_base)\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx=0, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
    "\n",
    "criterion = LabelSmoothing(size=tokenizer.vocab_size, padding_idx=0, smoothing=.1)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=CFG.max_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=CFG.projection_dim,\n",
    "        dropout=CFG.dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "    \n",
    "cos_loss = nn.CosineEmbeddingLoss(reduction=\"mean\", margin=.5)\n",
    "class Clip(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 tokenizer,\n",
    "                 temperature=CFG.temperature,\n",
    "        image_embedding=CFG.image_embedding,\n",
    "        text_embedding=CFG.text_embedding,\n",
    "):\n",
    "        super().__init__()\n",
    "    \n",
    "#         self.backbone = resnet101(pretrained=args.pretrained)\n",
    "        self.backbone = timm.create_model(\n",
    "                    \"resnest26d\", True, num_classes=0, global_pool=\"avg\"\n",
    "                )\n",
    "#         for p in self.backbone.parameters():\n",
    "#             p.requires_grad = True\n",
    "            \n",
    "        self.tokenizer = tokenizer\n",
    "        self.embeding = nn.Embedding(self.tokenizer.vocab_size,CFG.text_embedding)\n",
    "        self.conv1 = nn.Conv1d(CFG.text_embedding,32,8)        \n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "#         self.pos_encoding = PositionalEncoding(CFG.text_embedding, .2)\n",
    "            \n",
    "#         encoder_layer = nn.TransformerEncoderLayer(d_model=CFG.text_embedding, nhead=4, dropout=.2)\n",
    "#         self.text_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "        \n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "        self.temperature = temperature\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=736)\n",
    "        \n",
    "    def make_len_mask(self, inp):\n",
    "        return (inp == 0)\n",
    "            \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), 1)\n",
    "        mask = mask.masked_fill(mask==1, float('-inf'))\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, img, input_ids, check):        \n",
    "        \n",
    "        image_features = self.backbone.forward(img)        \n",
    "#         with open(\"das.sa\",\"a\") as f:\n",
    "#             f.write(str(pad_mask.sum(0)))\n",
    "                \n",
    "        input_ids = self.embeding(input_ids)\n",
    "        text_features = self.gelu(self.conv1(input_ids.permute(0,2,1)))\n",
    "        text_features = text_features.flatten(1)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         input_ids = self.pos_encoding(input_ids.permute(1,0,2))        \n",
    "#         input_ids = input_ids.permute(1,0,2)\n",
    "#         last_hidden_state = self.text_encoder(input_ids)\n",
    "#         print()\n",
    "#         print(last_hidden_state[:, self.target_token_idx, :])        \n",
    "#         text_features = last_hidden_state[:, self.target_token_idx, :]\n",
    "        \n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        \n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features) \n",
    "            \n",
    "# #         print(image_features,text_features)\n",
    "#         loss = cos_loss(image_embeddings, text_embeddings, check.to(CFG.device))\n",
    "#         return loss\n",
    "#         loss = 1-F.cosine_similarity(image_embeddings, text_embeddings)\n",
    "#         Calculating the Loss\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0372dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"iam_aug\"\n",
    "source_path = '../data/{}.hdf5'.format(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5011be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "import albumentations.pytorch\n",
    "\n",
    "transform_train = albumentations.Compose([\n",
    "    albumentations.OneOf(\n",
    "        [\n",
    "            albumentations.MotionBlur(p=1, blur_limit=8),\n",
    "            albumentations.OpticalDistortion(p=1, distort_limit=0.05),\n",
    "            albumentations.GaussNoise(p=1, var_limit=(10.0, 100.0)),\n",
    "            albumentations.RandomBrightnessContrast(p=1, brightness_limit=0.2),\n",
    "            albumentations.Downscale(p=1, scale_min=0.8, scale_max=.9),\n",
    "        ],\n",
    "        p=.5,\n",
    "    ),\n",
    "#         albumentations.Resize(224,224),\n",
    "    albumentations.Normalize(),\n",
    "    albumentations.pytorch.ToTensorV2()\n",
    "\n",
    "])\n",
    "\n",
    "transform_valid = albumentations.Compose(\n",
    "    [\n",
    "#         albumentations.Resize(224,224),            \n",
    "        albumentations.Normalize(),\n",
    "        albumentations.pytorch.ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acc35526",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Clip(tokenizer).to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03e19f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# d = torch.load(\"../output/clip_entropy\",map_location=\"cuda:0\")\n",
    "\n",
    "d = torch.load(\"../output/clip_entropy_26iam_aug_epoch10.pt\",map_location=\"cpu\")\n",
    "_ = model.load_state_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9cf17f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"../data/iam_aug.hdf5\"\n",
    "split = \"test\"\n",
    "dataset = {}\n",
    "with h5py.File(source, \"r\") as f:\n",
    "    dataset[split] = dict()\n",
    "    dataset[split]['dt'] = np.array(f[split]['dt'])\n",
    "    dataset[split]['gt'] = np.array(f[split]['gt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22e7b25f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = dataset[split]['dt'][49]\n",
    "# img = np.repeat(img[..., np.newaxis],3, -1).astype(\"float32\")   \n",
    "aug = transform_valid(image=img)\n",
    "img = aug['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d75d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter(dataset[split]['gt'])\n",
    "\n",
    "mapping = {}\n",
    "for i in c.keys():\n",
    "    mapping[i] = np.where(dataset[split]['gt']==i)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01fb8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11f46619",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = torch.utils.data.DataLoader(DataGenerator(source_path,'test',transform_valid, tokenizer), batch_size=300, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7f43a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6b3d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG.device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df09adbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_image_embeddings = []\n",
    "for img, input_ids,check in valid_loader:\n",
    "    with torch.no_grad():\n",
    "        image_features = model.backbone(img.to(CFG.device))\n",
    "        image_embeddings = model.image_projection(image_features)\n",
    "        image_embeddings = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "        valid_image_embeddings.append(image_embeddings)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64f81127",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_image_embeddings = [i.cpu().numpy() for i in valid_image_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1977cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_image_embeddings = np.concatenate(valid_image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96b105e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "ct=0\n",
    "for i in range(len(valid_image_embeddings)):    \n",
    "    label = dataset[split]['gt'][i]    \n",
    "    if len(mapping[label])==1:\n",
    "        continue\n",
    "    c = valid_image_embeddings[i] @ valid_image_embeddings.T\n",
    "    score = len(set(np.argsort(c)[::-1][:len(mapping[label])]).intersection(mapping[label]))/len(mapping[label])\n",
    "    scores.append(score)\n",
    "#     if score<.6:\n",
    "#         print(label, i)\n",
    "    ct+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a60b90d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8406309541642519"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2cdf69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9806200302718646"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c67e8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9806059417962377"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ece1e439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854132457580715"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdc28934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9849278977727236"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e9db01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_embeddings = []\n",
    "for key in mapping.keys():\n",
    "    encoded_query = tokenizer.encode(key)\n",
    "    encoded_query = np.pad(encoded_query, (0, 30 - len(encoded_query)))\n",
    "    # encoded_query = tokenizer([\"dsad dsa dasdas\"],padding='max_length', truncation=True, max_length=CFG.max_length)\n",
    "    with torch.no_grad():\n",
    "        a = model.embeding(torch.Tensor(encoded_query).unsqueeze(0).long().to(CFG.device))\n",
    "        text_features2 = model.gelu((model.conv1(a.permute(0,2,1))))\n",
    "        text_features2 = text_features2.flatten(1)    \n",
    "        text_embeddings = model.text_projection(text_features2)  \n",
    "        text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "        text_embeddings = text_embeddings[0].cpu().numpy()\n",
    "        t_embeddings.append(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68630ed4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Clip' object has no attribute 'pos_encoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8142/1180292874.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlast_hidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1131\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Clip' object has no attribute 'pos_encoding'"
     ]
    }
   ],
   "source": [
    "t_embeddings = []\n",
    "for key in mapping.keys():\n",
    "    encoded_query = tokenizer.encode(key)\n",
    "    encoded_query = np.pad(encoded_query, (0, 30 - len(encoded_query)))\n",
    "    # encoded_query = tokenizer([\"dsad dsa dasdas\"],padding='max_length', truncation=True, max_length=CFG.max_length)\n",
    "    with torch.no_grad():\n",
    "        input_ids = model.embeding(torch.Tensor(encoded_query).unsqueeze(0).long().to(CFG.device))\n",
    "        input_ids = model.pos_encoding(input_ids.permute(1,0,2))        \n",
    "        input_ids = input_ids.permute(1,0,2)\n",
    "        last_hidden_state = model.text_encoder(input_ids)\n",
    "        text_features = last_hidden_state[torch.arange(last_hidden_state.shape[0]), encoded_query.argmax()]\n",
    "        \n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        \n",
    "        text_embeddings = model.text_projection(text_features)         \n",
    "        text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "        text_embeddings = text_embeddings[0].cpu().numpy()\n",
    "        t_embeddings.append(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61a44f86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'writer' 25\n",
      "b'wrote' 32\n",
      "b'likes' 40\n",
      "b'mother' 58\n",
      "b'chair' 77\n",
      "b'normally' 93\n",
      "b'seen' 98\n",
      "b'courtenay' 103\n",
      "b'regarded' 112\n",
      "b't v' 114\n",
      "b'potter' 127\n",
      "b'wide' 148\n",
      "b'arrived' 187\n",
      "b'fight' 201\n",
      "b'telephone' 202\n",
      "b'textual' 249\n",
      "b'explanations' 250\n",
      "b'fell' 257\n",
      "b'book' 340\n",
      "b'13' 352\n",
      "b'use' 373\n",
      "b'writing' 380\n",
      "b'cut' 387\n",
      "b'reservoirs' 416\n",
      "b'corner' 469\n",
      "b'rock' 474\n",
      "b'security' 488\n",
      "b'impression' 498\n",
      "b'created' 512\n",
      "b'diligently' 517\n",
      "b'1ye' 574\n",
      "b'greatest' 577\n",
      "b'fruit' 584\n",
      "b'cure' 618\n",
      "b'gives' 640\n",
      "b'meek' 646\n",
      "b'thank' 671\n",
      "b'though' 698\n",
      "b'rome' 701\n",
      "b'king' 725\n",
      "b'came' 746\n",
      "b'battle' 765\n",
      "b'necessarily' 775\n",
      "b'uses' 786\n",
      "b'reverence' 793\n",
      "b'fear' 842\n",
      "b'cup' 868\n",
      "b'bed' 899\n",
      "b'night' 907\n",
      "b'danger' 914\n",
      "b'suppose' 923\n",
      "b'tired' 924\n",
      "b'journey' 926\n",
      "b'seated' 949\n",
      "b'illness' 966\n",
      "b'feared' 975\n",
      "b'comes' 980\n",
      "b'lived' 983\n",
      "b'months' 999\n",
      "b'desperately' 1022\n",
      "b'conscious' 1027\n",
      "b'un' 1043\n",
      "b'save' 1047\n",
      "b'jobs' 1053\n",
      "b'next' 1066\n",
      "b'drift' 1069\n",
      "b'reached' 1070\n",
      "b'becomes' 1080\n",
      "b'interested' 1083\n",
      "b'toes' 1111\n",
      "b'words' 1171\n",
      "b'continue' 1240\n",
      "b'swim' 1246\n",
      "b'sea' 1249\n",
      "b'seabed' 1257\n",
      "b'fishing' 1295\n",
      "b'gets' 1301\n",
      "b'winds' 1332\n",
      "b'shown' 1340\n",
      "b'half' 1354\n",
      "b'wave' 1371\n",
      "b'bow' 1372\n",
      "b'presumably' 1389\n",
      "b'seem' 1398\n",
      "b'described' 1450\n",
      "b'near' 1455\n",
      "b'medical' 1526\n",
      "b'produce' 1530\n",
      "b'roared' 1586\n",
      "b'hair' 1613\n",
      "b'carrying' 1618\n",
      "b'telling' 1621\n",
      "b'buck' 1681\n",
      "b'guest' 1705\n",
      "b'load' 1709\n",
      "b'italy' 1720\n",
      "b'simplified' 1729\n",
      "b'grossness' 1737\n",
      "b'jacob' 1755\n",
      "b'poker' 1764\n",
      "b'cooking' 1776\n",
      "b'embers' 1779\n",
      "b'smoking' 1780\n",
      "b'broke' 1799\n",
      "b'vivid' 1877\n",
      "b'features' 1880\n",
      "b'cook' 1905\n",
      "b'past' 1955\n",
      "b'typically' 1967\n",
      "b'composers' 1970\n",
      "b'household' 1993\n",
      "b'consisted' 2009\n",
      "b'foods' 2014\n",
      "b'impaired' 2016\n",
      "b'coarse' 2017\n",
      "b'opens' 2025\n",
      "b'tastes' 2032\n",
      "b'carlyle' 2067\n",
      "b'skill' 2102\n",
      "b'mining' 2108\n",
      "b'charge' 2125\n",
      "b'route' 2129\n",
      "b'included' 2151\n",
      "b'booty' 2231\n",
      "b'tied' 2255\n",
      "b'bought' 2257\n",
      "b'per' 2310\n",
      "b'formerly' 2315\n",
      "b'demanded' 2354\n",
      "b'walls' 2429\n",
      "b'spite' 2479\n",
      "b'bridge' 2494\n",
      "b'pleasant' 2497\n",
      "b'evenings' 2502\n",
      "b'parsons' 2522\n",
      "b'courage' 2525\n",
      "b'barton' 2534\n",
      "b'member' 2551\n",
      "b'interest' 2563\n",
      "b'1914' 2564\n",
      "b'old' 2576\n",
      "b'windows' 2591\n",
      "b'stretched' 2665\n",
      "b'thrust' 2684\n",
      "b'shook' 2688\n",
      "b'bbc' 2709\n",
      "b'orders' 2742\n",
      "b'thetans' 2748\n",
      "b\"dan's\" 2779\n",
      "b'waved' 2900\n",
      "b'lovely' 2901\n",
      "b'step' 2943\n",
      "b\"'ve\" 2971\n",
      "b'caine' 3003\n",
      "b'wild' 3022\n",
      "b'dinas' 3041\n",
      "b'rang' 3053\n",
      "b'damn' 3058\n",
      "b'job' 3112\n",
      "b'wanted' 3143\n",
      "b'sign' 3160\n",
      "b'lights' 3170\n",
      "b'sat' 3185\n",
      "b'roused' 3200\n",
      "b'price' 3217\n",
      "b'nose' 3226\n",
      "b'glasses' 3228\n",
      "b'changed' 3243\n",
      "b'haris' 3255\n",
      "b'wore' 3299\n",
      "b'events' 3306\n",
      "b'area' 3379\n",
      "b'destination' 3399\n",
      "b'slightly' 3400\n",
      "b'jeep' 3457\n",
      "b'taking' 3458\n",
      "b'graybury' 3503\n",
      "b'dare' 3512\n",
      "b'quick' 3549\n",
      "b\"o'\" 3583\n",
      "b\"wouldn't\" 3584\n",
      "b'act' 3595\n",
      "b'bueno' 3647\n",
      "b\"'em\" 3663\n",
      "b'shallow' 3671\n",
      "b'seaweed' 3716\n",
      "b'warning' 3722\n",
      "b'check' 3764\n",
      "b'looks' 3861\n",
      "b'trail' 3881\n",
      "b'sure' 3922\n",
      "b'whiskers' 3926\n",
      "b'kisses' 4014\n",
      "b'beach' 4018\n",
      "b'join' 4029\n",
      "b'floor' 4053\n",
      "b'shrugged' 4124\n",
      "b\"couldn't\" 4139\n",
      "b'distinguished' 4158\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "ct=0\n",
    "for i,label in enumerate(mapping.keys()):    \n",
    "    if len(mapping[label])==1:\n",
    "        continue\n",
    "    c = t_embeddings[i] @ valid_image_embeddings.T\n",
    "    score = len(set(np.argsort(c)[::-1][:len(mapping[label])]).intersection(mapping[label]))/len(mapping[label])\n",
    "    scores.append(score)\n",
    "    if score<.6:\n",
    "        print(label, i)\n",
    "    ct+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d835715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8785374417963444"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c75bd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9371581622247703"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c0e37d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8335253261688096"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e0b347f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9918401742903649"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79df49e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854395718807263"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27127f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+------------+\n",
      "|                Modules                | Parameters |\n",
      "+---------------------------------------+------------+\n",
      "|        backbone.conv1.0.weight        |    864     |\n",
      "|        backbone.conv1.1.weight        |     32     |\n",
      "|         backbone.conv1.1.bias         |     32     |\n",
      "|        backbone.conv1.3.weight        |    9216    |\n",
      "|        backbone.conv1.4.weight        |     32     |\n",
      "|         backbone.conv1.4.bias         |     32     |\n",
      "|        backbone.conv1.6.weight        |   18432    |\n",
      "|          backbone.bn1.weight          |     64     |\n",
      "|           backbone.bn1.bias           |     64     |\n",
      "|     backbone.layer1.0.conv1.weight    |    4096    |\n",
      "|      backbone.layer1.0.bn1.weight     |     64     |\n",
      "|       backbone.layer1.0.bn1.bias      |     64     |\n",
      "|  backbone.layer1.0.conv2.conv.weight  |   36864    |\n",
      "|   backbone.layer1.0.conv2.bn0.weight  |    128     |\n",
      "|    backbone.layer1.0.conv2.bn0.bias   |    128     |\n",
      "|   backbone.layer1.0.conv2.fc1.weight  |    2048    |\n",
      "|    backbone.layer1.0.conv2.fc1.bias   |     32     |\n",
      "|   backbone.layer1.0.conv2.bn1.weight  |     32     |\n",
      "|    backbone.layer1.0.conv2.bn1.bias   |     32     |\n",
      "|   backbone.layer1.0.conv2.fc2.weight  |    4096    |\n",
      "|    backbone.layer1.0.conv2.fc2.bias   |    128     |\n",
      "|     backbone.layer1.0.conv3.weight    |   16384    |\n",
      "|      backbone.layer1.0.bn3.weight     |    256     |\n",
      "|       backbone.layer1.0.bn3.bias      |    256     |\n",
      "| backbone.layer1.0.downsample.1.weight |   16384    |\n",
      "| backbone.layer1.0.downsample.2.weight |    256     |\n",
      "|  backbone.layer1.0.downsample.2.bias  |    256     |\n",
      "|     backbone.layer1.1.conv1.weight    |   16384    |\n",
      "|      backbone.layer1.1.bn1.weight     |     64     |\n",
      "|       backbone.layer1.1.bn1.bias      |     64     |\n",
      "|  backbone.layer1.1.conv2.conv.weight  |   36864    |\n",
      "|   backbone.layer1.1.conv2.bn0.weight  |    128     |\n",
      "|    backbone.layer1.1.conv2.bn0.bias   |    128     |\n",
      "|   backbone.layer1.1.conv2.fc1.weight  |    2048    |\n",
      "|    backbone.layer1.1.conv2.fc1.bias   |     32     |\n",
      "|   backbone.layer1.1.conv2.bn1.weight  |     32     |\n",
      "|    backbone.layer1.1.conv2.bn1.bias   |     32     |\n",
      "|   backbone.layer1.1.conv2.fc2.weight  |    4096    |\n",
      "|    backbone.layer1.1.conv2.fc2.bias   |    128     |\n",
      "|     backbone.layer1.1.conv3.weight    |   16384    |\n",
      "|      backbone.layer1.1.bn3.weight     |    256     |\n",
      "|       backbone.layer1.1.bn3.bias      |    256     |\n",
      "|     backbone.layer2.0.conv1.weight    |   32768    |\n",
      "|      backbone.layer2.0.bn1.weight     |    128     |\n",
      "|       backbone.layer2.0.bn1.bias      |    128     |\n",
      "|  backbone.layer2.0.conv2.conv.weight  |   147456   |\n",
      "|   backbone.layer2.0.conv2.bn0.weight  |    256     |\n",
      "|    backbone.layer2.0.conv2.bn0.bias   |    256     |\n",
      "|   backbone.layer2.0.conv2.fc1.weight  |    8192    |\n",
      "|    backbone.layer2.0.conv2.fc1.bias   |     64     |\n",
      "|   backbone.layer2.0.conv2.bn1.weight  |     64     |\n",
      "|    backbone.layer2.0.conv2.bn1.bias   |     64     |\n",
      "|   backbone.layer2.0.conv2.fc2.weight  |   16384    |\n",
      "|    backbone.layer2.0.conv2.fc2.bias   |    256     |\n",
      "|     backbone.layer2.0.conv3.weight    |   65536    |\n",
      "|      backbone.layer2.0.bn3.weight     |    512     |\n",
      "|       backbone.layer2.0.bn3.bias      |    512     |\n",
      "| backbone.layer2.0.downsample.1.weight |   131072   |\n",
      "| backbone.layer2.0.downsample.2.weight |    512     |\n",
      "|  backbone.layer2.0.downsample.2.bias  |    512     |\n",
      "|     backbone.layer2.1.conv1.weight    |   65536    |\n",
      "|      backbone.layer2.1.bn1.weight     |    128     |\n",
      "|       backbone.layer2.1.bn1.bias      |    128     |\n",
      "|  backbone.layer2.1.conv2.conv.weight  |   147456   |\n",
      "|   backbone.layer2.1.conv2.bn0.weight  |    256     |\n",
      "|    backbone.layer2.1.conv2.bn0.bias   |    256     |\n",
      "|   backbone.layer2.1.conv2.fc1.weight  |    8192    |\n",
      "|    backbone.layer2.1.conv2.fc1.bias   |     64     |\n",
      "|   backbone.layer2.1.conv2.bn1.weight  |     64     |\n",
      "|    backbone.layer2.1.conv2.bn1.bias   |     64     |\n",
      "|   backbone.layer2.1.conv2.fc2.weight  |   16384    |\n",
      "|    backbone.layer2.1.conv2.fc2.bias   |    256     |\n",
      "|     backbone.layer2.1.conv3.weight    |   65536    |\n",
      "|      backbone.layer2.1.bn3.weight     |    512     |\n",
      "|       backbone.layer2.1.bn3.bias      |    512     |\n",
      "|     backbone.layer3.0.conv1.weight    |   131072   |\n",
      "|      backbone.layer3.0.bn1.weight     |    256     |\n",
      "|       backbone.layer3.0.bn1.bias      |    256     |\n",
      "|  backbone.layer3.0.conv2.conv.weight  |   589824   |\n",
      "|   backbone.layer3.0.conv2.bn0.weight  |    512     |\n",
      "|    backbone.layer3.0.conv2.bn0.bias   |    512     |\n",
      "|   backbone.layer3.0.conv2.fc1.weight  |   32768    |\n",
      "|    backbone.layer3.0.conv2.fc1.bias   |    128     |\n",
      "|   backbone.layer3.0.conv2.bn1.weight  |    128     |\n",
      "|    backbone.layer3.0.conv2.bn1.bias   |    128     |\n",
      "|   backbone.layer3.0.conv2.fc2.weight  |   65536    |\n",
      "|    backbone.layer3.0.conv2.fc2.bias   |    512     |\n",
      "|     backbone.layer3.0.conv3.weight    |   262144   |\n",
      "|      backbone.layer3.0.bn3.weight     |    1024    |\n",
      "|       backbone.layer3.0.bn3.bias      |    1024    |\n",
      "| backbone.layer3.0.downsample.1.weight |   524288   |\n",
      "| backbone.layer3.0.downsample.2.weight |    1024    |\n",
      "|  backbone.layer3.0.downsample.2.bias  |    1024    |\n",
      "|     backbone.layer3.1.conv1.weight    |   262144   |\n",
      "|      backbone.layer3.1.bn1.weight     |    256     |\n",
      "|       backbone.layer3.1.bn1.bias      |    256     |\n",
      "|  backbone.layer3.1.conv2.conv.weight  |   589824   |\n",
      "|   backbone.layer3.1.conv2.bn0.weight  |    512     |\n",
      "|    backbone.layer3.1.conv2.bn0.bias   |    512     |\n",
      "|   backbone.layer3.1.conv2.fc1.weight  |   32768    |\n",
      "|    backbone.layer3.1.conv2.fc1.bias   |    128     |\n",
      "|   backbone.layer3.1.conv2.bn1.weight  |    128     |\n",
      "|    backbone.layer3.1.conv2.bn1.bias   |    128     |\n",
      "|   backbone.layer3.1.conv2.fc2.weight  |   65536    |\n",
      "|    backbone.layer3.1.conv2.fc2.bias   |    512     |\n",
      "|     backbone.layer3.1.conv3.weight    |   262144   |\n",
      "|      backbone.layer3.1.bn3.weight     |    1024    |\n",
      "|       backbone.layer3.1.bn3.bias      |    1024    |\n",
      "|     backbone.layer4.0.conv1.weight    |   524288   |\n",
      "|      backbone.layer4.0.bn1.weight     |    512     |\n",
      "|       backbone.layer4.0.bn1.bias      |    512     |\n",
      "|  backbone.layer4.0.conv2.conv.weight  |  2359296   |\n",
      "|   backbone.layer4.0.conv2.bn0.weight  |    1024    |\n",
      "|    backbone.layer4.0.conv2.bn0.bias   |    1024    |\n",
      "|   backbone.layer4.0.conv2.fc1.weight  |   131072   |\n",
      "|    backbone.layer4.0.conv2.fc1.bias   |    256     |\n",
      "|   backbone.layer4.0.conv2.bn1.weight  |    256     |\n",
      "|    backbone.layer4.0.conv2.bn1.bias   |    256     |\n",
      "|   backbone.layer4.0.conv2.fc2.weight  |   262144   |\n",
      "|    backbone.layer4.0.conv2.fc2.bias   |    1024    |\n",
      "|     backbone.layer4.0.conv3.weight    |  1048576   |\n",
      "|      backbone.layer4.0.bn3.weight     |    2048    |\n",
      "|       backbone.layer4.0.bn3.bias      |    2048    |\n",
      "| backbone.layer4.0.downsample.1.weight |  2097152   |\n",
      "| backbone.layer4.0.downsample.2.weight |    2048    |\n",
      "|  backbone.layer4.0.downsample.2.bias  |    2048    |\n",
      "|     backbone.layer4.1.conv1.weight    |  1048576   |\n",
      "|      backbone.layer4.1.bn1.weight     |    512     |\n",
      "|       backbone.layer4.1.bn1.bias      |    512     |\n",
      "|  backbone.layer4.1.conv2.conv.weight  |  2359296   |\n",
      "|   backbone.layer4.1.conv2.bn0.weight  |    1024    |\n",
      "|    backbone.layer4.1.conv2.bn0.bias   |    1024    |\n",
      "|   backbone.layer4.1.conv2.fc1.weight  |   131072   |\n",
      "|    backbone.layer4.1.conv2.fc1.bias   |    256     |\n",
      "|   backbone.layer4.1.conv2.bn1.weight  |    256     |\n",
      "|    backbone.layer4.1.conv2.bn1.bias   |    256     |\n",
      "|   backbone.layer4.1.conv2.fc2.weight  |   262144   |\n",
      "|    backbone.layer4.1.conv2.fc2.bias   |    1024    |\n",
      "|     backbone.layer4.1.conv3.weight    |  1048576   |\n",
      "|      backbone.layer4.1.bn3.weight     |    2048    |\n",
      "|       backbone.layer4.1.bn3.bias      |    2048    |\n",
      "|            embeding.weight            |   29700    |\n",
      "|              conv1.weight             |   76800    |\n",
      "|               conv1.bias              |     32     |\n",
      "|   image_projection.projection.weight  |   524288   |\n",
      "|    image_projection.projection.bias   |    256     |\n",
      "|       image_projection.fc.weight      |   65536    |\n",
      "|        image_projection.fc.bias       |    256     |\n",
      "|   image_projection.layer_norm.weight  |    256     |\n",
      "|    image_projection.layer_norm.bias   |    256     |\n",
      "|   text_projection.projection.weight   |   188416   |\n",
      "|    text_projection.projection.bias    |    256     |\n",
      "|       text_projection.fc.weight       |   65536    |\n",
      "|        text_projection.fc.bias        |    256     |\n",
      "|   text_projection.layer_norm.weight   |    256     |\n",
      "|    text_projection.layer_norm.bias    |    256     |\n",
      "+---------------------------------------+------------+\n",
      "Total Trainable Params: 15972804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15972804"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "185e99f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.972804"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "15972804/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a556126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
