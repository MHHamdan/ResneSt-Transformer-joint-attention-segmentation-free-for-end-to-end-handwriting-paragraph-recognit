{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc23346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mhamdan/seq2seqAttenHTR/Transformer_ocr/src\n",
      "source: ../data/dasd.hdf5\n",
      "charset: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n"
     ]
    }
   ],
   "source": [
    "%cd src\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import groupby\n",
    "import h5py\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet101,resnet50\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from data import preproc as pp\n",
    "from data import evaluation\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import timm\n",
    "import random\n",
    "from torch.nn import Module, ModuleList\n",
    "from torch.nn import Conv2d, InstanceNorm2d, Dropout, Dropout2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn.functional import pad\n",
    "import random\n",
    "\n",
    "import wandb\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "    \n",
    "def set_random_seeds(random_seed=0):\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "class DepthSepConv2D(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, activation=None, padding=True, stride=(1, 1), dilation=(1, 1)):\n",
    "        super(DepthSepConv2D, self).__init__()\n",
    "\n",
    "        self.padding = None\n",
    "\n",
    "        if padding:\n",
    "            if padding is True:\n",
    "                padding = [int((k - 1) / 2) for k in kernel_size]\n",
    "                if kernel_size[0] % 2 == 0 or kernel_size[1] % 2 == 0:\n",
    "                    padding_h = kernel_size[1] - 1\n",
    "                    padding_w = kernel_size[0] - 1\n",
    "                    self.padding = [padding_h//2, padding_h-padding_h//2, padding_w//2, padding_w-padding_w//2]\n",
    "                    padding = (0, 0)\n",
    "\n",
    "        else:\n",
    "            padding = (0, 0)\n",
    "        self.depth_conv = Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, dilation=dilation, stride=stride, padding=padding, groups=in_channels)\n",
    "        self.point_conv = Conv2d(in_channels=in_channels, out_channels=out_channels, dilation=dilation, kernel_size=(1, 1))\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depth_conv(x)\n",
    "        if self.padding:\n",
    "            x = pad(x, self.padding)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        x = self.point_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MixDropout(Module):\n",
    "    def __init__(self, dropout_proba=0.4, dropout2d_proba=0.2):\n",
    "        super(MixDropout, self).__init__()\n",
    "\n",
    "        self.dropout = Dropout(dropout_proba)\n",
    "        self.dropout2d = Dropout2d(dropout2d_proba)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if random.random() < 0.5:\n",
    "            return self.dropout(x)\n",
    "        return self.dropout2d(x)\n",
    "\n",
    "\n",
    "class FCN_Encoder(Module):\n",
    "    def __init__(self, params):\n",
    "        super(FCN_Encoder, self).__init__()\n",
    "\n",
    "        self.dropout = params[\"dropout\"]\n",
    "\n",
    "        self.init_blocks = ModuleList([\n",
    "            ConvBlock(params[\"input_channels\"], 16, stride=(1, 1), dropout=self.dropout),\n",
    "            ConvBlock(16, 32, stride=(2, 2), dropout=self.dropout),\n",
    "            ConvBlock(32, 64, stride=(2, 2), dropout=self.dropout),\n",
    "            ConvBlock(64, 128, stride=(2, 2), dropout=self.dropout),\n",
    "            ConvBlock(128, 128, stride=(2, 1), dropout=self.dropout),\n",
    "            ConvBlock(128, 128, stride=(2, 1), dropout=self.dropout),\n",
    "        ])\n",
    "        self.blocks = ModuleList([\n",
    "            DSCBlock(128, 128, pool=(1, 1), dropout=self.dropout),\n",
    "            DSCBlock(128, 128, pool=(1, 1), dropout=self.dropout),\n",
    "            DSCBlock(128, 128, pool=(1, 1), dropout=self.dropout),\n",
    "            DSCBlock(128, 256, pool=(1, 1), dropout=self.dropout),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for b in self.init_blocks:\n",
    "            x = b(x)\n",
    "        for b in self.blocks:\n",
    "            xt = b(x)\n",
    "            x = x + xt if x.size() == xt.size() else xt\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvBlock(Module):\n",
    "\n",
    "    def __init__(self, in_, out_, stride=(1, 1), k=3, activation=ReLU, dropout=0.4):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.activation = activation()\n",
    "        self.conv1 = Conv2d(in_channels=in_, out_channels=out_, kernel_size=k, padding=k // 2)\n",
    "        self.conv2 = Conv2d(in_channels=out_, out_channels=out_, kernel_size=k, padding=k // 2)\n",
    "        self.conv3 = Conv2d(out_, out_, kernel_size=(3, 3), padding=(1, 1), stride=stride)\n",
    "        self.norm_layer = InstanceNorm2d(out_, eps=0.001, momentum=0.99, track_running_stats=False)\n",
    "        self.dropout = MixDropout(dropout_proba=dropout, dropout2d_proba=dropout / 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pos = random.randint(1, 3)\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        if pos == 1:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        if pos == 2:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.norm_layer(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        if pos == 3:\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DSCBlock(Module):\n",
    "\n",
    "    def __init__(self, in_, out_, pool=(2, 1), activation=ReLU, dropout=0.4):\n",
    "        super(DSCBlock, self).__init__()\n",
    "\n",
    "        self.activation = activation()\n",
    "        self.conv1 = DepthSepConv2D(in_, out_, kernel_size=(3, 3))\n",
    "        self.conv2 = DepthSepConv2D(out_, out_, kernel_size=(3, 3))\n",
    "        self.conv3 = DepthSepConv2D(out_, out_, kernel_size=(3, 3), padding=(1, 1), stride=pool)\n",
    "        self.norm_layer = InstanceNorm2d(out_, eps=0.001, momentum=0.99, track_running_stats=False)\n",
    "        self.dropout = MixDropout(dropout_proba=dropout, dropout2d_proba=dropout/2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pos = random.randint(1, 3)\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        if pos == 1:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        if pos == 2:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.norm_layer(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        if pos == 3:\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=680):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "params = {            \"input_channels\": 3,  # 3 for RGB images, 1 for grayscale images\n",
    "\n",
    "            # dropout probability for standard dropout (half dropout probability is taken for spatial dropout)\n",
    "            \"dropout\": 0.5,  # dropout for encoder module\n",
    "            \"dec_dropout\": 0.5,  # dropout for decoder module\n",
    "            \"att_dropout\": 0,  # dropout for attention module\n",
    "\n",
    "            \"features_size\": 256,  # encoder output features maps\n",
    "            \"att_fc_size\": 256,  # number of channels for attention sum computation\n",
    "\n",
    "         }\n",
    "    \n",
    "class OCR(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_len, hidden_dim, nheads,\n",
    "                 num_encoder_layers, num_decoder_layers):\n",
    "        super().__init__()\n",
    "    \n",
    "#         self.backbone = resnet101(pretrained=args.pretrained)\n",
    "        self.backbone = FCN_Encoder(params)                \n",
    "#         for name,p in self.backbone.named_parameters():\n",
    "#             if \"bn\" not in name or \"attnpool\" in name:\n",
    "#                 p.requires_grad =  False\n",
    "\n",
    "        # create a default PyTorch transformer\n",
    "        # create conversion layer\n",
    "        self.converter = nn.AdaptiveAvgPool2d((8,64))\n",
    "        \n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "        # prediction heads with length of vocab\n",
    "        # DETR used basic 3 layer MLP for output\n",
    "        self.vocab = nn.Linear(hidden_dim,vocab_len)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.decoder = nn.Embedding(vocab_len, hidden_dim)\n",
    "        self.query_pos = PositionalEncoding(hidden_dim, .2)\n",
    "\n",
    "        # spatial positional encodings, sine positional encoding can be used.\n",
    "        # Detr baseline uses sine positional encoding.\n",
    "        self.row_embed = nn.Parameter(torch.rand(8, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(64, hidden_dim // 2))\n",
    "#         self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "#         self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "\n",
    "        self.trg_mask = None\n",
    "  \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), 1)\n",
    "        mask = mask.masked_fill(mask==1, float('-inf'))\n",
    "        return mask\n",
    "    \n",
    "    def get_feature(self,x):\n",
    "        x = self.backbone(x)        \n",
    "        return x\n",
    "\n",
    "\n",
    "    def make_len_mask(self, inp):\n",
    "        return (inp == 0).transpose(0, 1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, trg):\n",
    "        # propagate inputs through ResNet-101 up to avg-pool layer\n",
    "        x = self.get_feature(inputs)\n",
    "\n",
    "        # convert from 2048 to 256 feature planes for the transformer\n",
    "        h = self.converter(x)\n",
    "\n",
    "        # construct positional encodings\n",
    "        bs,_,H, W = h.shape\n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "        # generating subsequent mask for target\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(trg.shape[1]).to(trg.device)\n",
    "\n",
    "        # Padding mask\n",
    "        trg_pad_mask = self.make_len_mask(trg)\n",
    "\n",
    "        # Getting postional encoding for target\n",
    "        trg = self.decoder(trg)\n",
    "        trg = self.query_pos(trg)\n",
    "        \n",
    "        output = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1), trg.permute(1,0,2), tgt_mask=self.trg_mask, \n",
    "                                  tgt_key_padding_mask=trg_pad_mask.permute(1,0))\n",
    "\n",
    "        return self.vocab(output.transpose(0,1))\n",
    "\n",
    "\n",
    "def make_model(vocab_len, hidden_dim=256, nheads=4,\n",
    "                 num_encoder_layers=4, num_decoder_layers=4):\n",
    "    \n",
    "    return OCR(vocab_len, hidden_dim, nheads,\n",
    "                 num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "\"\"\"\n",
    "Uses generator functions to supply train/test with data.\n",
    "Image renderings and text are created on the fly each time.\n",
    "\"\"\"\n",
    "import pickle\n",
    "class DataGenerator(Dataset):\n",
    "    \"\"\"Generator class with data streaming\"\"\"\n",
    "\n",
    "    def __init__(self, source, split, transform, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        \n",
    "#         self.split = split\n",
    "#         self.dataset = dict()\n",
    "        \n",
    "        with open('../data/full_paragraph{}.zip'.format(split), 'rb') as handle:\n",
    "            self.dataset = pickle.load(handle)        \n",
    "\n",
    "#         with h5py.File(source, \"r\") as f:\n",
    "#             self.dataset[self.split] = dict()\n",
    "\n",
    "#             self.dataset[self.split]['dt'] = np.array(f[self.split]['dt'])\n",
    "#             self.dataset[self.split]['gt'] = np.array(f[self.split]['gt'])\n",
    "          \n",
    "#             randomize = np.arange(len(self.dataset[self.split]['gt']))\n",
    "#             np.random.seed(42)\n",
    "#             np.random.shuffle(randomize)\n",
    "\n",
    "#             self.dataset[self.split]['dt'] = self.dataset[self.split]['dt'][randomize]\n",
    "#             self.dataset[self.split]['gt'] = self.dataset[self.split]['gt'][randomize]\n",
    "#             # decode sentences from byte\n",
    "#             self.dataset[self.split]['gt'] = [x.decode() for x in self.dataset[self.split]['gt']]\n",
    "\n",
    "            \n",
    "        self.size = len(self.dataset['gt'])\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = self.dataset['dt'][i]\n",
    "        \n",
    "        #making image compatible with resnet\n",
    "#         img = cv2.transpose(img)\n",
    "        img = np.repeat(img[..., np.newaxis],3, -1).astype(\"float32\")   \n",
    "#         img = pp.normalization(img).astype(\"float32\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img)\n",
    "            img = aug['image']\n",
    "            \n",
    "#             img = self.transform(img)\n",
    "            \n",
    "        y_train = self.tokenizer.encode(self.dataset['gt'][i]) \n",
    "        \n",
    "        #padding till max length\n",
    "        y_train = np.pad(y_train, (0, self.tokenizer.maxlen - len(y_train)))\n",
    "\n",
    "        gt = y_train\n",
    "\n",
    "        return img, gt          \n",
    "\n",
    "    def __len__(self):\n",
    "      return self.size\n",
    "\n",
    "class Tokenizer():\n",
    "    \"\"\"Manager tokens functions and charset/dictionary properties\"\"\"\n",
    "\n",
    "    def __init__(self, chars, max_text_length=128):\n",
    "        self.PAD_TK, self.UNK_TK,self.SOS,self.EOS = \"¶\", \"¤\", \"SOS\", \"EOS\"\n",
    "        self.chars = [self.PAD_TK] + [self.UNK_TK ]+ [self.SOS] + [self.EOS] +list(chars)\n",
    "        self.PAD = self.chars.index(self.PAD_TK)\n",
    "        self.UNK = self.chars.index(self.UNK_TK)\n",
    "\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.maxlen = max_text_length\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to vector\"\"\"\n",
    "        text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        groups = [\"\".join(group) for _, group in groupby(text)]\n",
    "        text = \"\".join([self.UNK_TK.join(list(x)) if len(x) > 1 else x for x in groups])\n",
    "        encoded = []\n",
    "\n",
    "        text = ['SOS'] + list(text) + ['EOS']\n",
    "        for item in text:\n",
    "            index = self.chars.index(item)\n",
    "            index = self.UNK if index == -1 else index\n",
    "            encoded.append(index)\n",
    "\n",
    "        return np.asarray(encoded)\n",
    "\n",
    "    def decode(self, text):\n",
    "        \"\"\"Decode vector to text\"\"\"\n",
    "        \n",
    "        decoded = \"\".join([self.chars[int(x)] for x in text if x > -1])\n",
    "        decoded = self.remove_tokens(decoded)\n",
    "        decoded = pp.text_standardize(decoded)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def remove_tokens(self, text):\n",
    "        \"\"\"Remove tokens (PAD) from text\"\"\"\n",
    "\n",
    "        return text.replace(self.PAD_TK, \"\").replace(self.UNK_TK, \"\")\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import string\n",
    "local_rank = 0\n",
    "batch_size = 1\n",
    "epochs = 300\n",
    "\n",
    "# define paths\n",
    "#change paths accordingly\n",
    "source = \"dasd\"\n",
    "source_path = '../data/{}.hdf5'.format(source)\n",
    "\n",
    "# define input size, number max of chars per line and list of valid chars\n",
    "input_size = (1024, 128, 1)\n",
    "max_text_length = 680\n",
    "charset_base = string.printable[:95]\n",
    "# charset_base = string.printable[:36].lower() + string.printable[36+26:95].lower() \n",
    "\n",
    "print(\"source:\", source_path)\n",
    "print(\"charset:\", charset_base)\n",
    "\n",
    "\n",
    "import torchvision.transforms as T\n",
    "device = torch.device(\"cuda:{}\".format(local_rank))\n",
    "\n",
    "# transform = T.Compose([\n",
    "#     T.ToTensor()])\n",
    "tokenizer = Tokenizer(charset_base,max_text_length)\n",
    "import albumentations\n",
    "import albumentations.pytorch\n",
    "\n",
    "if True:\n",
    "\n",
    "    transform_train = albumentations.Compose([\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.MotionBlur(p=1, blur_limit=8),\n",
    "                albumentations.OpticalDistortion(p=1, distort_limit=0.05),\n",
    "                albumentations.GaussNoise(p=1, var_limit=(10.0, 100.0)),\n",
    "                albumentations.RandomBrightnessContrast(p=1, brightness_limit=0.2),\n",
    "                albumentations.Downscale(p=1, scale_min=0.3, scale_max=0.5),\n",
    "            ],\n",
    "            p=.5,\n",
    "        ),\n",
    "#         albumentations.Resize(224,224),\n",
    "        albumentations.Normalize(),\n",
    "#         albumentations.pytorch.ToTensorV2()\n",
    "\n",
    "    ])\n",
    "\n",
    "    transform_valid = albumentations.Compose(\n",
    "        [\n",
    "#         albumentations.Resize(224,224),            \n",
    "            albumentations.Normalize(),\n",
    "#             albumentations.pytorch.ToTensorV2()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "else:\n",
    "    transform_train = albumentations.Compose(\n",
    "        [\n",
    "#         albumentations.Resize(224,224),            \n",
    "            albumentations.Normalize(),\n",
    "#             albumentations.pytorch.ToTensorV2()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    transform_valid = albumentations.Compose(\n",
    "        [\n",
    "#         albumentations.Resize(224,224),\n",
    "            \n",
    "            albumentations.Normalize(),\n",
    "#             albumentations.pytorch.ToTensorV2()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "def pad_images(data, padding_value=0):\n",
    "    \"\"\"\n",
    "    data: list of numpy array\n",
    "    \"\"\"\n",
    "    x_lengths = [x.shape[0] for x in data]\n",
    "    y_lengths = [x.shape[1] for x in data]\n",
    "    longest_x = max(x_lengths)\n",
    "    longest_y = max(y_lengths)\n",
    "    padded_data = np.ones((len(data), longest_x, longest_y, data[0].shape[2])) * padding_value\n",
    "    for i, xy_len in enumerate(zip(x_lengths, y_lengths)):\n",
    "        x_len, y_len = xy_len\n",
    "        padded_data[i, :x_len, :y_len, ...] = data[i][:x_len, :y_len, ...]\n",
    "    return padded_data\n",
    "\n",
    "\n",
    "def my_collate(batch):\n",
    "    \n",
    "    imgs = [item[0] for item in batch]\n",
    "    imgs = pad_images(imgs)\n",
    "    imgs = torch.tensor(imgs).float().permute(0, 3, 1, 2)   \n",
    "    target = torch.LongTensor([item[1] for item in batch])\n",
    "    return imgs, target\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddeb2dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(charset_base, max_text_length=700)\n",
    "\n",
    "model = make_model( vocab_len=tokenizer.vocab_size,hidden_dim=256, nheads=4,\n",
    "                 num_encoder_layers=4, num_decoder_layers=4)\n",
    "\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a95835c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.load(\"/home/mhamdan/seq2seqAttenHTR/Transformer_ocr/output/crisp-pyramid-152/full_paragraph_firstbest_loss.pt\", map_location=\"cuda:1\")\n",
    "\n",
    "f = {}\n",
    "for i in d:\n",
    "    f[i.replace(\"module.\",\"\")] = d[i]\n",
    "\n",
    "model.load_state_dict(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6662d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory(model,imgs):\n",
    "    x = model.converter(model.get_feature(imgs))\n",
    "    bs,_,H, W = x.shape\n",
    "    pos = torch.cat([\n",
    "            model.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            model.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "    return model.transformer.encoder(pos +  0.1 * x.flatten(2).permute(2, 0, 1))\n",
    "    \n",
    "\n",
    "def test(model, test_loader, max_text_length):\n",
    "    model.eval()\n",
    "    predicts = []\n",
    "    gt = []\n",
    "    imgs = []\n",
    "    c=0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            src, trg = batch\n",
    "            imgs.append(src.flatten(0,1))\n",
    "            src, trg = src.to(device), trg.to(device)  \n",
    "            t1 = time.time()\n",
    "            memory = get_memory(model,src.float())\n",
    "            out_indexes = [tokenizer.chars.index('SOS'), ]\n",
    "            for i in range(max_text_length):\n",
    "                mask = model.generate_square_subsequent_mask(i+1).to(device)\n",
    "                trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(device)\n",
    "                output = model.vocab(model.transformer.decoder(model.query_pos(model.decoder(trg_tensor)), memory,tgt_mask=mask))\n",
    "                out_token = output.argmax(2)[-1].item()\n",
    "                out_indexes.append(out_token)\n",
    "                if out_token == tokenizer.chars.index('EOS'):\n",
    "                    break\n",
    "            print(time.time()-t1)\n",
    "            predicts.append(tokenizer.decode(out_indexes))\n",
    "            gt.append(tokenizer.decode(trg.flatten(0,1)))\n",
    "            if c==0:\n",
    "                break\n",
    "            c+=1\n",
    "    return predicts, gt, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38123f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(DataGenerator(source_path,'test',transform_valid, tokenizer), batch_size=1, shuffle=False, num_workers=6,collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd3b3f40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1021392345428467\n"
     ]
    }
   ],
   "source": [
    "predicts, gt, imgs = test(model,test_loader , max_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de221326",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOSThehee theathe theatheathe the te ttate te tttto t ta the tatan the t t e t te e e te te e e e e e e e e e e e e e e a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6a795e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 605, 1820, 3]), torch.Size([1, 700]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[0].shape,i[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0708255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
