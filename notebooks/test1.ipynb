{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35b49ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mhamdan/seq2seqAttenHTR/Transformer_ocr/src\n"
     ]
    }
   ],
   "source": [
    "%cd src/\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import groupby\n",
    "import h5py\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50, resnet101\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from data import preproc as pp\n",
    "from data import evaluation\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "import pytorch_lightning as pl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6fd3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=128):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e738227",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCR(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_len, hidden_dim, nheads,\n",
    "                 num_encoder_layers, num_decoder_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        # create ResNet-101 backbone\n",
    "        self.backbone = resnet101()\n",
    "        del self.backbone.fc\n",
    "\n",
    "        # create conversion layer\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "\n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "        # prediction heads with length of vocab\n",
    "        # DETR used basic 3 layer MLP for output\n",
    "        self.vocab = nn.Linear(hidden_dim,vocab_len)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.decoder = nn.Embedding(vocab_len, hidden_dim)\n",
    "        self.query_pos = PositionalEncoding(hidden_dim, .2)\n",
    "\n",
    "        # spatial positional encodings, sine positional encoding can be used.\n",
    "        # Detr baseline uses sine positional encoding.\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.trg_mask = None\n",
    "  \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), 1)\n",
    "        mask = mask.masked_fill(mask==1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def get_feature(self,x):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)   \n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def make_len_mask(self, inp):\n",
    "        return (inp == 0).transpose(0, 1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, trg):\n",
    "        # propagate inputs through ResNet-101 up to avg-pool layer\n",
    "        x = self.get_feature(inputs)\n",
    "\n",
    "        # convert from 2048 to 256 feature planes for the transformer\n",
    "        h = self.conv(x)\n",
    "\n",
    "        # construct positional encodings\n",
    "        bs,_,H, W = h.shape\n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "        # generating subsequent mask for target\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(trg.shape[1]).to(trg.device)\n",
    "\n",
    "        # Padding mask\n",
    "        trg_pad_mask = self.make_len_mask(trg)\n",
    "\n",
    "        # Getting postional encoding for target\n",
    "        trg = self.decoder(trg)\n",
    "        trg = self.query_pos(trg)\n",
    "        \n",
    "        output = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1), trg.permute(1,0,2), tgt_mask=self.trg_mask, \n",
    "                                  tgt_key_padding_mask=trg_pad_mask.permute(1,0))\n",
    "\n",
    "        return self.vocab(output.transpose(0,1))\n",
    "\n",
    "def make_model(vocab_len, hidden_dim=256, nheads=4,\n",
    "                 num_encoder_layers=4, num_decoder_layers=4):\n",
    "    \n",
    "    return OCR(vocab_len, hidden_dim, nheads,\n",
    "                 num_encoder_layers, num_decoder_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02db0a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Uses generator functions to supply train/test with data.\n",
    "Image renderings and text are created on the fly each time.\n",
    "\"\"\"\n",
    "\n",
    "class DataGenerator(Dataset):\n",
    "    \"\"\"Generator class with data streaming\"\"\"\n",
    "\n",
    "    def __init__(self, source, split, transform, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.split = split\n",
    "        self.dataset = dict()\n",
    "\n",
    "        with h5py.File(source, \"r\") as f:\n",
    "            self.dataset[self.split] = dict()\n",
    "\n",
    "            self.dataset[self.split]['dt'] = np.array(f[self.split]['dt'])\n",
    "            self.dataset[self.split]['gt'] = np.array(f[self.split]['gt'])\n",
    "          \n",
    "            randomize = np.arange(len(self.dataset[self.split]['gt']))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(randomize)\n",
    "\n",
    "            self.dataset[self.split]['dt'] = self.dataset[self.split]['dt'][randomize]\n",
    "            self.dataset[self.split]['gt'] = self.dataset[self.split]['gt'][randomize]\n",
    "\n",
    "            # decode sentences from byte\n",
    "            self.dataset[self.split]['gt'] = [x.decode() for x in self.dataset[self.split]['gt']]\n",
    "            \n",
    "        self.size = len(self.dataset[self.split]['gt'])\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = self.dataset[self.split]['dt'][i]\n",
    "        \n",
    "        #making image compatible with resnet\n",
    "        img = np.repeat(img[..., np.newaxis],3, -1)    \n",
    "        img = pp.normalization(img).astype(\"float32\")\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)        \n",
    "#         if self.transform is not None:\n",
    "#             img = self.transform(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmented = self.transform(image = img)\n",
    "            img = augmented['image']\n",
    "            \n",
    "        y_train = self.tokenizer.encode(self.dataset[self.split]['gt'][i]) \n",
    "        \n",
    "        #padding till max length\n",
    "        y_train = np.pad(y_train, (0, self.tokenizer.maxlen - len(y_train)))\n",
    "\n",
    "        gt = torch.Tensor(y_train)\n",
    "\n",
    "        return img, gt          \n",
    "\n",
    "    def __len__(self):\n",
    "      return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3fcdfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: ../data/iam.hdf5\n",
      "output ../output/iam\n",
      "target ../output/iam/checkpoint_weights_iam.hdf5\n",
      "charset: 0123456789abcdefghijklmnopqrstuvwxyz!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n"
     ]
    }
   ],
   "source": [
    "class Tokenizer():\n",
    "    \"\"\"Manager tokens functions and charset/dictionary properties\"\"\"\n",
    "\n",
    "    def __init__(self, chars, max_text_length=128):\n",
    "        self.PAD_TK, self.UNK_TK,self.SOS,self.EOS = \"¶\", \"¤\", \"SOS\", \"EOS\"\n",
    "        self.chars = [self.PAD_TK] + [self.UNK_TK ]+ [self.SOS] + [self.EOS] +list(chars)\n",
    "        self.PAD = self.chars.index(self.PAD_TK)\n",
    "        self.UNK = self.chars.index(self.UNK_TK)\n",
    "\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.maxlen = max_text_length\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to vector\"\"\"\n",
    "        text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"ASCII\").lower()\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        groups = [\"\".join(group) for _, group in groupby(text)]\n",
    "        text = \"\".join([self.UNK_TK.join(list(x)) if len(x) > 1 else x for x in groups])\n",
    "        encoded = []\n",
    "\n",
    "        text = ['SOS'] + list(text) + ['EOS']\n",
    "        for item in text:\n",
    "            index = self.chars.index(item)\n",
    "            index = self.UNK if index == -1 else index\n",
    "            encoded.append(index)\n",
    "\n",
    "        return np.asarray(encoded)\n",
    "\n",
    "    def decode(self, text):\n",
    "        \"\"\"Decode vector to text\"\"\"\n",
    "        \n",
    "        decoded = \"\".join([self.chars[int(x)] for x in text if x > -1])\n",
    "        decoded = self.remove_tokens(decoded)\n",
    "        decoded = pp.text_standardize(decoded)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def remove_tokens(self, text):\n",
    "        \"\"\"Remove tokens (PAD) from text\"\"\"\n",
    "\n",
    "        return text.replace(self.PAD_TK, \"\").replace(self.UNK_TK, \"\")\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 200\n",
    "\n",
    "# define paths\n",
    "#change paths accordingly\n",
    "source = 'iam'\n",
    "source_path = '../data/{}.hdf5'.format(source)\n",
    "output_path = os.path.join(\"..\", \"output\", source)\n",
    "target_path = os.path.join(output_path, \"checkpoint_weights_iam.hdf5\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# define input size, number max of chars per line and list of valid chars\n",
    "input_size = (1024, 128, 1)\n",
    "max_text_length = 128\n",
    "# charset_base = string.printable[:95]\n",
    "charset_base = string.printable[:36].lower() + string.printable[36+26:95].lower() \n",
    "\n",
    "print(\"source:\", source_path)\n",
    "print(\"output\", output_path)\n",
    "print(\"target\", target_path)\n",
    "print(\"charset:\", charset_base)\n",
    "\n",
    "import torchvision.transforms as T\n",
    "transform = T.Compose([\n",
    "    T.ToTensor()])\n",
    "tokenizer = Tokenizer(charset_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd6a6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhamdan/miniconda3/lib/python3.8/site-packages/albumentations/augmentations/transforms.py:1770: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Compose([\n",
       "  RandomContrast(always_apply=False, p=0.5, limit=(-0.2, 0.2)),\n",
       "  MotionBlur(always_apply=False, p=0.2, blur_limit=(3, 7)),\n",
       "  OpticalDistortion(always_apply=False, p=0.3, distort_limit=(-0.05, 0.05), shift_limit=(-0.05, 0.05), interpolation=1, border_mode=4, value=None, mask_value=None),\n",
       "  GaussNoise(always_apply=False, p=0.2, var_limit=(10.0, 50.0), per_channel=True, mean=0),\n",
       "  RandomBrightnessContrast(always_apply=False, p=0.2, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True),\n",
       "  ToTensorV2(always_apply=True, p=1.0, transpose_mask=False),\n",
       "], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import albumentations\n",
    "import albumentations.pytorch\n",
    "\n",
    "\n",
    "albumentations.Compose([\n",
    "    albumentations.RandomContrast(),\n",
    "  albumentations.MotionBlur(p=.2),\n",
    "  albumentations.OpticalDistortion(p=.3),\n",
    "  albumentations.GaussNoise(p=.2),\n",
    "    albumentations.RandomBrightnessContrast(p=0.2),       \n",
    "    albumentations.pytorch.transforms.ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ecf06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "testt = albumentations.Compose([\n",
    "    albumentations.pytorch.transforms.ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f31dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bb43597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = DataModule(source_path, 16, tokenizer, transform)\n",
    "\n",
    "model = make_model(tokenizer.vocab_size,hidden_dim=256, nheads=4,\n",
    "                 num_encoder_layers=4, num_decoder_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68ee4ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load(\"../output/iam/checkpoint_weights_iam_small_4_enc_4_dec_aug.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1352de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = {}\n",
    "for i in d:\n",
    "    f[i.replace(\"module.\",\"\")] = d[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d996ab6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "210a3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory(model,imgs):\n",
    "    x = model.conv(model.get_feature(imgs))\n",
    "    bs,_,H, W = x.shape\n",
    "    pos = torch.cat([\n",
    "            model.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            model.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "    return model.transformer.encoder(pos +  0.1 * x.flatten(2).permute(2, 0, 1))\n",
    "    \n",
    "\n",
    "def test(model, test_loader, max_text_length):\n",
    "    model.eval()\n",
    "    predicts = []\n",
    "    gt = []\n",
    "    imgs = []\n",
    "    c=0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            src, trg = batch\n",
    "            imgs.append(src.flatten(0,1))\n",
    "            src, trg = src.cuda(), trg.cuda()            \n",
    "            memory = get_memory(model,src.float())\n",
    "            out_indexes = [tokenizer.chars.index('SOS'), ]\n",
    "            for i in range(max_text_length):\n",
    "                mask = model.generate_square_subsequent_mask(i+1).to(device)\n",
    "                trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(device)\n",
    "                output = model.vocab(model.transformer.decoder(model.query_pos(model.decoder(trg_tensor)), memory,tgt_mask=mask))\n",
    "                out_token = output.argmax(2)[-1].item()\n",
    "                out_indexes.append(out_token)\n",
    "                if out_token == tokenizer.chars.index('EOS'):\n",
    "                    break\n",
    "            predicts.append(tokenizer.decode(out_indexes))\n",
    "            gt.append(tokenizer.decode(trg.flatten(0,1)))\n",
    "            if c==100:\n",
    "                break\n",
    "            c+=1\n",
    "    return predicts, gt, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87f1322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device ='cuda'\n",
    "_ =  model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "980f1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(DataGenerator(source_path,'test',testt, tokenizer), batch_size=1, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab0199f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1425"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48bc9bc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicts, gt, imgs = test(model, test_loader, max_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d86bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = list(map(lambda x : x.replace('SOS','').replace('EOS',''),predicts))\n",
    "gt = list(map(lambda x : x.replace('SOS','').replace('EOS',''),gt))\n",
    "\n",
    "evaluate = evaluation.ocr_metrics(predicts=predicts,\n",
    "                                  ground_truth=gt,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "723e6475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76314335, 0.9618147 , 1.        ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa212929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1498361 , 0.38938587, 0.97052632])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8a1d6119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quite unable to explain why he should feel - gofite unalle to explain whey hald feel\n",
      "meet the deanes , and as soon as guy had - mect the decemes , and as scon as 63uy had\n",
      "the horses and drank enough to cure our - the harses and denk ghough to use uw\n",
      "you not killed ? ' ' because we know all things , ' the - you not lilled . ' becuse we know all things , ' the\n",
      "with the possibility of faulty design . \" he held - with the possibility of faulty design . \" ste held\n",
      "all due deference , miss deane - come off it !  - all due deference , miss deaue - coms oft ' . \n",
      "it would have been acceptable to all concerned - 3t would have been accepstable to all concermed\n",
      "to make you understand just what happened - to wake you udustard just what hapered\n",
      "course of action should be . first , to avoid the - course of action should be . first to avoid the\n",
      " ( stamp department ) while sally sulked at home .  - istams cepartment ) while sally sulted at homo . \n",
      "bill is good man , and bueno buck is raised on lake .  - bill is goodman , and breno buchis rained on lake . \n",
      "then what of the cultural life ? did this per -  - then what of theoulturol lite ( did this per - \n",
      "muttered together . ' do you say that you poor - muttered together . ' do you say that you poor\n",
      "sally and of course mrs septimus , for surely - sally and of caurse ris spstimes , for suvely\n",
      "did you come from ? \" the judge said in - id yan come from ! \" the pindge raid in\n",
      "junior medical officer . \" you won ' t be free about nine , i - juild messicm orrven . \" lon masit st feel hoed whit whire , \n",
      "soul , \" said megan thomas tartly .  - soch : \" said mogan thomas tartly . \n",
      "was clear . the street was quiet and - was clear . tho street was quiet and\n",
      "island . \" what have you got in that basket , price ?  - island . \" what haveyou got inthat barlut , prive ! \n",
      "that they use dan as a specimen demonstra -  - that they use dan as a specinen demoustea - \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(20):\n",
    "    print(gt[i], \"-\",predicts[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cda19146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mhamdan/.cache/torch/hub/facebookresearch_dino_main\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/dino/dino_resnet50_pretrain/dino_resnet50_pretrain.pth\" to /home/mhamdan/.cache/torch/hub/checkpoints/dino_resnet50_pretrain.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2da2d27f9b44d5aa62094da90e2371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/90.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "xcit_small_12_p8 = torch.hub.load('facebookresearch/dino:main',\"dino_resnet50\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "901ebedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1,3,1024,128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46e721df",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xcit_small_12_p8.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea2130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af7402dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12503/1233698692.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_process_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gloo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'file:///tmp/somefile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\u001b[0m in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;31m# Use store based barrier here since barrier() used a bunch of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;31m# default devices and messes up NCCL internal state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0m_store_based_barrier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;31m# Set sequence numbers for gloo and nccl process groups.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_pg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNCCL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\u001b[0m in \u001b[0;36m_store_based_barrier\u001b[0;34m(rank, store, timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mlog_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mworker_count\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0mworker_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import torch.distributed as dist\n",
    "# dist.init_process_group('gloo', init_method='file:///tmp/somefile', rank=0, world_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8f618fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_feature(backbone,x):\n",
    "        x = backbone.conv1(x)\n",
    "        x = backbone.bn1(x)   \n",
    "        x = backbone.relu(x)\n",
    "        x = backbone.maxpool(x)\n",
    "\n",
    "        x = backbone.layer1(x)\n",
    "        x = backbone.layer2(x)\n",
    "        x = backbone.layer3(x)\n",
    "        x = backbone.layer4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25b449ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = get_feature(xcit_small_12_p8, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80f2024d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 32, 4])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e680b20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b =  xcit_small_12_p8.forward(a)\n",
    "d =  xcit_small_12_p8(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08763b55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2048]), torch.Size([1, 2048]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b75f44f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class OCR(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_len, hidden_dim, nheads,\n",
    "                 num_encoder_layers, num_decoder_layers):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.hidden_dim = hidden_dim\n",
    "        # create ResNet-101 backbone\n",
    "        self.backbone = torch.hub.load('facebookresearch/dino:main',\"dino_xcit_small_12_p8\")\n",
    "\n",
    "        # create conversion layer\n",
    "        self.converter = nn.Linear(self.backbone.embed_dim, hidden_dim)\n",
    "\n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "        # prediction heads with length of vocab\n",
    "        # DETR used basic 3 layer MLP for output\n",
    "        self.vocab = nn.Linear(hidden_dim,vocab_len)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.decoder = nn.Embedding(vocab_len, hidden_dim)\n",
    "        self.query_pos = PositionalEncoding(hidden_dim, .2)\n",
    "#         self.cnn_pos = PositionalEncodingCNN(hidden_dim,.2)\n",
    "\n",
    "        # spatial positional encodings, sine positional encoding can be used.\n",
    "        # Detr baseline uses sine positional encoding.\n",
    "#         self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "#         self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.trg_mask = None\n",
    "  \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), 1)\n",
    "        mask = mask.masked_fill(mask==1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def get_feature(self,x):\n",
    "        x = self.backbone.forward_features(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def make_len_mask(self, inp):\n",
    "        return (inp == 0).transpose(0, 1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, trg):\n",
    "        # propagate inputs through ResNet-101 up to avg-pool layer\n",
    "        h = self.get_feature(inputs)\n",
    "        h = h.unsqueeze(2).repeat(1,1,self.hidden_dim)\n",
    "        # convert from 2048 to 256 feature planes for the transformer\n",
    "#         h = self.converter(x)\n",
    "#         # construct positional encodings\n",
    "#         bs,_,H, W = h.shape\n",
    "#         pos = torch.cat([\n",
    "#             self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "#             self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "#         ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "#         # generating subsequent mask for target\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(trg.shape[1]).to(trg.device)\n",
    "\n",
    "        # Padding mask\n",
    "        trg_pad_mask = self.make_len_mask(trg)\n",
    "\n",
    "        # Getting postional encoding for target\n",
    "        trg = self.decoder(trg)\n",
    "        trg = self.query_pos(trg)\n",
    "        \n",
    "        output = self.transformer(h.permute(1, 0, 2), trg.permute(1,0,2), tgt_mask=self.trg_mask, \n",
    "                                  tgt_key_padding_mask=trg_pad_mask.permute(1,0))\n",
    "\n",
    "        return self.vocab(output.transpose(0,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11fb8520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(vocab_len, hidden_dim=256, nheads=4,\n",
    "                 num_encoder_layers=4, num_decoder_layers=4):\n",
    "    \n",
    "    return OCR(vocab_len, hidden_dim, nheads,\n",
    "                 num_encoder_layers, num_decoder_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd468e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mhamdan/.cache/torch/hub/facebookresearch_dino_main\n",
      "Using cache found in /home/mhamdan/.cache/torch/hub/facebookresearch_xcit_master\n"
     ]
    }
   ],
   "source": [
    "m = make_model(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "941696c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1,3,225,225).cuda()\n",
    "b = torch.rand(1,128).long().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "89018eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = m.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e72d01c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384, 256])\n",
      "torch.Size([1, 128, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3495c7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e524e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50b77c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+------------+\n",
      "|                           Modules                           | Parameters |\n",
      "+-------------------------------------------------------------+------------+\n",
      "|                          row_embed                          |    6400    |\n",
      "|                          col_embed                          |    6400    |\n",
      "|                      backbone.cls_token                     |    384     |\n",
      "|             backbone.patch_embed.proj.0.0.weight            |    2592    |\n",
      "|             backbone.patch_embed.proj.0.1.weight            |     96     |\n",
      "|              backbone.patch_embed.proj.0.1.bias             |     96     |\n",
      "|             backbone.patch_embed.proj.2.0.weight            |   165888   |\n",
      "|             backbone.patch_embed.proj.2.1.weight            |    192     |\n",
      "|              backbone.patch_embed.proj.2.1.bias             |    192     |\n",
      "|             backbone.patch_embed.proj.4.0.weight            |   663552   |\n",
      "|             backbone.patch_embed.proj.4.1.weight            |    384     |\n",
      "|              backbone.patch_embed.proj.4.1.bias             |    384     |\n",
      "|                   backbone.blocks.0.gamma1                  |    384     |\n",
      "|                   backbone.blocks.0.gamma2                  |    384     |\n",
      "|                   backbone.blocks.0.gamma3                  |    384     |\n",
      "|                backbone.blocks.0.norm1.weight               |    384     |\n",
      "|                 backbone.blocks.0.norm1.bias                |    384     |\n",
      "|              backbone.blocks.0.attn.temperature             |     8      |\n",
      "|              backbone.blocks.0.attn.qkv.weight              |   442368   |\n",
      "|               backbone.blocks.0.attn.qkv.bias               |    1152    |\n",
      "|              backbone.blocks.0.attn.proj.weight             |   147456   |\n",
      "|               backbone.blocks.0.attn.proj.bias              |    384     |\n",
      "|                backbone.blocks.0.norm2.weight               |    384     |\n",
      "|                 backbone.blocks.0.norm2.bias                |    384     |\n",
      "|               backbone.blocks.0.mlp.fc1.weight              |   589824   |\n",
      "|                backbone.blocks.0.mlp.fc1.bias               |    1536    |\n",
      "|               backbone.blocks.0.mlp.fc2.weight              |   589824   |\n",
      "|                backbone.blocks.0.mlp.fc2.bias               |    384     |\n",
      "|                backbone.blocks.0.norm3.weight               |    384     |\n",
      "|                 backbone.blocks.0.norm3.bias                |    384     |\n",
      "|           backbone.blocks.0.local_mp.conv1.weight           |    3456    |\n",
      "|            backbone.blocks.0.local_mp.conv1.bias            |    384     |\n",
      "|             backbone.blocks.0.local_mp.bn.weight            |    384     |\n",
      "|              backbone.blocks.0.local_mp.bn.bias             |    384     |\n",
      "|           backbone.blocks.0.local_mp.conv2.weight           |    3456    |\n",
      "|            backbone.blocks.0.local_mp.conv2.bias            |    384     |\n",
      "|                   backbone.blocks.1.gamma1                  |    384     |\n",
      "|                   backbone.blocks.1.gamma2                  |    384     |\n",
      "|                   backbone.blocks.1.gamma3                  |    384     |\n",
      "|                backbone.blocks.1.norm1.weight               |    384     |\n",
      "|                 backbone.blocks.1.norm1.bias                |    384     |\n",
      "|              backbone.blocks.1.attn.temperature             |     8      |\n",
      "|              backbone.blocks.1.attn.qkv.weight              |   442368   |\n",
      "|               backbone.blocks.1.attn.qkv.bias               |    1152    |\n",
      "|              backbone.blocks.1.attn.proj.weight             |   147456   |\n",
      "|               backbone.blocks.1.attn.proj.bias              |    384     |\n",
      "|                backbone.blocks.1.norm2.weight               |    384     |\n",
      "|                 backbone.blocks.1.norm2.bias                |    384     |\n",
      "|               backbone.blocks.1.mlp.fc1.weight              |   589824   |\n",
      "|                backbone.blocks.1.mlp.fc1.bias               |    1536    |\n",
      "|               backbone.blocks.1.mlp.fc2.weight              |   589824   |\n",
      "|                backbone.blocks.1.mlp.fc2.bias               |    384     |\n",
      "|                backbone.blocks.1.norm3.weight               |    384     |\n",
      "|                 backbone.blocks.1.norm3.bias                |    384     |\n",
      "|           backbone.blocks.1.local_mp.conv1.weight           |    3456    |\n",
      "|            backbone.blocks.1.local_mp.conv1.bias            |    384     |\n",
      "|             backbone.blocks.1.local_mp.bn.weight            |    384     |\n",
      "|              backbone.blocks.1.local_mp.bn.bias             |    384     |\n",
      "|           backbone.blocks.1.local_mp.conv2.weight           |    3456    |\n",
      "|            backbone.blocks.1.local_mp.conv2.bias            |    384     |\n",
      "|                   backbone.blocks.2.gamma1                  |    384     |\n",
      "|                   backbone.blocks.2.gamma2                  |    384     |\n",
      "|                   backbone.blocks.2.gamma3                  |    384     |\n",
      "|                backbone.blocks.2.norm1.weight               |    384     |\n",
      "|                 backbone.blocks.2.norm1.bias                |    384     |\n",
      "|              backbone.blocks.2.attn.temperature             |     8      |\n",
      "|              backbone.blocks.2.attn.qkv.weight              |   442368   |\n",
      "|               backbone.blocks.2.attn.qkv.bias               |    1152    |\n",
      "|              backbone.blocks.2.attn.proj.weight             |   147456   |\n",
      "|               backbone.blocks.2.attn.proj.bias              |    384     |\n",
      "|                backbone.blocks.2.norm2.weight               |    384     |\n",
      "|                 backbone.blocks.2.norm2.bias                |    384     |\n",
      "|               backbone.blocks.2.mlp.fc1.weight              |   589824   |\n",
      "|                backbone.blocks.2.mlp.fc1.bias               |    1536    |\n",
      "|               backbone.blocks.2.mlp.fc2.weight              |   589824   |\n",
      "|                backbone.blocks.2.mlp.fc2.bias               |    384     |\n",
      "|                backbone.blocks.2.norm3.weight               |    384     |\n",
      "|                 backbone.blocks.2.norm3.bias                |    384     |\n",
      "|           backbone.blocks.2.local_mp.conv1.weight           |    3456    |\n",
      "|            backbone.blocks.2.local_mp.conv1.bias            |    384     |\n",
      "|             backbone.blocks.2.local_mp.bn.weight            |    384     |\n",
      "|              backbone.blocks.2.local_mp.bn.bias             |    384     |\n",
      "|           backbone.blocks.2.local_mp.conv2.weight           |    3456    |\n",
      "|            backbone.blocks.2.local_mp.conv2.bias            |    384     |\n",
      "|                   backbone.blocks.3.gamma1                  |    384     |\n",
      "|                   backbone.blocks.3.gamma2                  |    384     |\n",
      "|                   backbone.blocks.3.gamma3                  |    384     |\n",
      "|                backbone.blocks.3.norm1.weight               |    384     |\n",
      "|                 backbone.blocks.3.norm1.bias                |    384     |\n",
      "|              backbone.blocks.3.attn.temperature             |     8      |\n",
      "|              backbone.blocks.3.attn.qkv.weight              |   442368   |\n",
      "|               backbone.blocks.3.attn.qkv.bias               |    1152    |\n",
      "|              backbone.blocks.3.attn.proj.weight             |   147456   |\n",
      "|               backbone.blocks.3.attn.proj.bias              |    384     |\n",
      "|                backbone.blocks.3.norm2.weight               |    384     |\n",
      "|                 backbone.blocks.3.norm2.bias                |    384     |\n",
      "|               backbone.blocks.3.mlp.fc1.weight              |   589824   |\n",
      "|                backbone.blocks.3.mlp.fc1.bias               |    1536    |\n",
      "|               backbone.blocks.3.mlp.fc2.weight              |   589824   |\n",
      "|                backbone.blocks.3.mlp.fc2.bias               |    384     |\n",
      "|                backbone.blocks.3.norm3.weight               |    384     |\n",
      "|                 backbone.blocks.3.norm3.bias                |    384     |\n",
      "|           backbone.blocks.3.local_mp.conv1.weight           |    3456    |\n",
      "|            backbone.blocks.3.local_mp.conv1.bias            |    384     |\n",
      "|             backbone.blocks.3.local_mp.bn.weight            |    384     |\n",
      "|              backbone.blocks.3.local_mp.bn.bias             |    384     |\n",
      "|           backbone.blocks.3.local_mp.conv2.weight           |    3456    |\n",
      "|            backbone.blocks.3.local_mp.conv2.bias            |    384     |\n",
      "|                   backbone.blocks.4.gamma1                  |    384     |\n",
      "|                   backbone.blocks.4.gamma2                  |    384     |\n",
      "|                   backbone.blocks.4.gamma3                  |    384     |\n",
      "|                backbone.blocks.4.norm1.weight               |    384     |\n",
      "|                 backbone.blocks.4.norm1.bias                |    384     |\n",
      "|              backbone.blocks.4.attn.temperature             |     8      |\n",
      "|              backbone.blocks.4.attn.qkv.weight              |   442368   |\n",
      "|               backbone.blocks.4.attn.qkv.bias               |    1152    |\n",
      "|              backbone.blocks.4.attn.proj.weight             |   147456   |\n",
      "|               backbone.blocks.4.attn.proj.bias              |    384     |\n",
      "|                backbone.blocks.4.norm2.weight               |    384     |\n",
      "|                 backbone.blocks.4.norm2.bias                |    384     |\n",
      "|               backbone.blocks.4.mlp.fc1.weight              |   589824   |\n",
      "|                backbone.blocks.4.mlp.fc1.bias               |    1536    |\n",
      "|               backbone.blocks.4.mlp.fc2.weight              |   589824   |\n",
      "|                backbone.blocks.4.mlp.fc2.bias               |    384     |\n",
      "|                backbone.blocks.4.norm3.weight               |    384     |\n",
      "|                 backbone.blocks.4.norm3.bias                |    384     |\n",
      "|           backbone.blocks.4.local_mp.conv1.weight           |    3456    |\n",
      "|            backbone.blocks.4.local_mp.conv1.bias            |    384     |\n",
      "|             backbone.blocks.4.local_mp.bn.weight            |    384     |\n",
      "|              backbone.blocks.4.local_mp.bn.bias             |    384     |\n",
      "|           backbone.blocks.4.local_mp.conv2.weight           |    3456    |\n",
      "|            backbone.blocks.4.local_mp.conv2.bias            |    384     |\n",
      "|                   backbone.blocks.5.gamma1                  |    384     |\n",
      "|                   backbone.blocks.5.gamma2                  |    384     |\n",
      "|                   backbone.blocks.5.gamma3                  |    384     |\n",
      "|                backbone.blocks.5.norm1.weight               |    384     |\n",
      "|                 backbone.blocks.5.norm1.bias                |    384     |\n",
      "|              backbone.blocks.5.attn.temperature             |     8      |\n",
      "|              backbone.blocks.5.attn.qkv.weight              |   442368   |\n",
      "|               backbone.blocks.5.attn.qkv.bias               |    1152    |\n",
      "|              backbone.blocks.5.attn.proj.weight             |   147456   |\n",
      "|               backbone.blocks.5.attn.proj.bias              |    384     |\n",
      "|                backbone.blocks.5.norm2.weight               |    384     |\n",
      "|                 backbone.blocks.5.norm2.bias                |    384     |\n",
      "|               backbone.blocks.5.mlp.fc1.weight              |   589824   |\n",
      "|                backbone.blocks.5.mlp.fc1.bias               |    1536    |\n",
      "|               backbone.blocks.5.mlp.fc2.weight              |   589824   |\n",
      "|                backbone.blocks.5.mlp.fc2.bias               |    384     |\n",
      "|                backbone.blocks.5.norm3.weight               |    384     |\n",
      "|                 backbone.blocks.5.norm3.bias                |    384     |\n",
      "|           backbone.blocks.5.local_mp.conv1.weight           |    3456    |\n",
      "|            backbone.blocks.5.local_mp.conv1.bias            |    384     |\n",
      "|             backbone.blocks.5.local_mp.bn.weight            |    384     |\n",
      "|              backbone.blocks.5.local_mp.bn.bias             |    384     |\n",
      "|           backbone.blocks.5.local_mp.conv2.weight           |    3456    |\n",
      "|            backbone.blocks.5.local_mp.conv2.bias            |    384     |\n",
      "|                   backbone.blocks.6.gamma1                  |    384     |\n",
      "|                   backbone.blocks.6.gamma2                  |    384     |\n",
      "|                   backbone.blocks.6.gamma3                  |    384     |\n",
      "|                backbone.blocks.6.norm1.weight               |    384     |\n",
      "|                 backbone.blocks.6.norm1.bias                |    384     |\n",
      "|              backbone.blocks.6.attn.temperature             |     8      |\n",
      "|              backbone.blocks.6.attn.qkv.weight              |   442368   |\n",
      "|               backbone.blocks.6.attn.qkv.bias               |    1152    |\n",
      "|              backbone.blocks.6.attn.proj.weight             |   147456   |\n",
      "|               backbone.blocks.6.attn.proj.bias              |    384     |\n",
      "|                backbone.blocks.6.norm2.weight               |    384     |\n",
      "|                 backbone.blocks.6.norm2.bias                |    384     |\n",
      "|               backbone.blocks.6.mlp.fc1.weight              |   589824   |\n",
      "|                backbone.blocks.6.mlp.fc1.bias               |    1536    |\n",
      "|               backbone.blocks.6.mlp.fc2.weight              |   589824   |\n",
      "|                backbone.blocks.6.mlp.fc2.bias               |    384     |\n",
      "|                backbone.blocks.6.norm3.weight               |    384     |\n",
      "|                 backbone.blocks.6.norm3.bias                |    384     |\n",
      "|           backbone.blocks.6.local_mp.conv1.weight           |    3456    |\n",
      "|            backbone.blocks.6.local_mp.conv1.bias            |    384     |\n",
      "|             backbone.blocks.6.local_mp.bn.weight            |    384     |\n",
      "|              backbone.blocks.6.local_mp.bn.bias             |    384     |\n",
      "|           backbone.blocks.6.local_mp.conv2.weight           |    3456    |\n",
      "|            backbone.blocks.6.local_mp.conv2.bias            |    384     |\n",
      "|                   backbone.blocks.7.gamma1                  |    384     |\n",
      "|                   backbone.blocks.7.gamma2                  |    384     |\n",
      "|                   backbone.blocks.7.gamma3                  |    384     |\n",
      "|                backbone.blocks.7.norm1.weight               |    384     |\n",
      "|                 backbone.blocks.7.norm1.bias                |    384     |\n",
      "|              backbone.blocks.7.attn.temperature             |     8      |\n",
      "|              backbone.blocks.7.attn.qkv.weight              |   442368   |\n",
      "|               backbone.blocks.7.attn.qkv.bias               |    1152    |\n",
      "|              backbone.blocks.7.attn.proj.weight             |   147456   |\n",
      "|               backbone.blocks.7.attn.proj.bias              |    384     |\n",
      "|                backbone.blocks.7.norm2.weight               |    384     |\n",
      "|                 backbone.blocks.7.norm2.bias                |    384     |\n",
      "|               backbone.blocks.7.mlp.fc1.weight              |   589824   |\n",
      "|                backbone.blocks.7.mlp.fc1.bias               |    1536    |\n",
      "|               backbone.blocks.7.mlp.fc2.weight              |   589824   |\n",
      "|                backbone.blocks.7.mlp.fc2.bias               |    384     |\n",
      "|                backbone.blocks.7.norm3.weight               |    384     |\n",
      "|                 backbone.blocks.7.norm3.bias                |    384     |\n",
      "|           backbone.blocks.7.local_mp.conv1.weight           |    3456    |\n",
      "|            backbone.blocks.7.local_mp.conv1.bias            |    384     |\n",
      "|             backbone.blocks.7.local_mp.bn.weight            |    384     |\n",
      "|              backbone.blocks.7.local_mp.bn.bias             |    384     |\n",
      "|           backbone.blocks.7.local_mp.conv2.weight           |    3456    |\n",
      "|            backbone.blocks.7.local_mp.conv2.bias            |    384     |\n",
      "|                   backbone.blocks.8.gamma1                  |    384     |\n",
      "|                   backbone.blocks.8.gamma2                  |    384     |\n",
      "|                   backbone.blocks.8.gamma3                  |    384     |\n",
      "|                backbone.blocks.8.norm1.weight               |    384     |\n",
      "|                 backbone.blocks.8.norm1.bias                |    384     |\n",
      "|              backbone.blocks.8.attn.temperature             |     8      |\n",
      "|              backbone.blocks.8.attn.qkv.weight              |   442368   |\n",
      "|               backbone.blocks.8.attn.qkv.bias               |    1152    |\n",
      "|              backbone.blocks.8.attn.proj.weight             |   147456   |\n",
      "|               backbone.blocks.8.attn.proj.bias              |    384     |\n",
      "|                backbone.blocks.8.norm2.weight               |    384     |\n",
      "|                 backbone.blocks.8.norm2.bias                |    384     |\n",
      "|               backbone.blocks.8.mlp.fc1.weight              |   589824   |\n",
      "|                backbone.blocks.8.mlp.fc1.bias               |    1536    |\n",
      "|               backbone.blocks.8.mlp.fc2.weight              |   589824   |\n",
      "|                backbone.blocks.8.mlp.fc2.bias               |    384     |\n",
      "|                backbone.blocks.8.norm3.weight               |    384     |\n",
      "|                 backbone.blocks.8.norm3.bias                |    384     |\n",
      "|           backbone.blocks.8.local_mp.conv1.weight           |    3456    |\n",
      "|            backbone.blocks.8.local_mp.conv1.bias            |    384     |\n",
      "|             backbone.blocks.8.local_mp.bn.weight            |    384     |\n",
      "|              backbone.blocks.8.local_mp.bn.bias             |    384     |\n",
      "|           backbone.blocks.8.local_mp.conv2.weight           |    3456    |\n",
      "|            backbone.blocks.8.local_mp.conv2.bias            |    384     |\n",
      "|                   backbone.blocks.9.gamma1                  |    384     |\n",
      "|                   backbone.blocks.9.gamma2                  |    384     |\n",
      "|                   backbone.blocks.9.gamma3                  |    384     |\n",
      "|                backbone.blocks.9.norm1.weight               |    384     |\n",
      "|                 backbone.blocks.9.norm1.bias                |    384     |\n",
      "|              backbone.blocks.9.attn.temperature             |     8      |\n",
      "|              backbone.blocks.9.attn.qkv.weight              |   442368   |\n",
      "|               backbone.blocks.9.attn.qkv.bias               |    1152    |\n",
      "|              backbone.blocks.9.attn.proj.weight             |   147456   |\n",
      "|               backbone.blocks.9.attn.proj.bias              |    384     |\n",
      "|                backbone.blocks.9.norm2.weight               |    384     |\n",
      "|                 backbone.blocks.9.norm2.bias                |    384     |\n",
      "|               backbone.blocks.9.mlp.fc1.weight              |   589824   |\n",
      "|                backbone.blocks.9.mlp.fc1.bias               |    1536    |\n",
      "|               backbone.blocks.9.mlp.fc2.weight              |   589824   |\n",
      "|                backbone.blocks.9.mlp.fc2.bias               |    384     |\n",
      "|                backbone.blocks.9.norm3.weight               |    384     |\n",
      "|                 backbone.blocks.9.norm3.bias                |    384     |\n",
      "|           backbone.blocks.9.local_mp.conv1.weight           |    3456    |\n",
      "|            backbone.blocks.9.local_mp.conv1.bias            |    384     |\n",
      "|             backbone.blocks.9.local_mp.bn.weight            |    384     |\n",
      "|              backbone.blocks.9.local_mp.bn.bias             |    384     |\n",
      "|           backbone.blocks.9.local_mp.conv2.weight           |    3456    |\n",
      "|            backbone.blocks.9.local_mp.conv2.bias            |    384     |\n",
      "|                  backbone.blocks.10.gamma1                  |    384     |\n",
      "|                  backbone.blocks.10.gamma2                  |    384     |\n",
      "|                  backbone.blocks.10.gamma3                  |    384     |\n",
      "|               backbone.blocks.10.norm1.weight               |    384     |\n",
      "|                backbone.blocks.10.norm1.bias                |    384     |\n",
      "|             backbone.blocks.10.attn.temperature             |     8      |\n",
      "|              backbone.blocks.10.attn.qkv.weight             |   442368   |\n",
      "|               backbone.blocks.10.attn.qkv.bias              |    1152    |\n",
      "|             backbone.blocks.10.attn.proj.weight             |   147456   |\n",
      "|              backbone.blocks.10.attn.proj.bias              |    384     |\n",
      "|               backbone.blocks.10.norm2.weight               |    384     |\n",
      "|                backbone.blocks.10.norm2.bias                |    384     |\n",
      "|              backbone.blocks.10.mlp.fc1.weight              |   589824   |\n",
      "|               backbone.blocks.10.mlp.fc1.bias               |    1536    |\n",
      "|              backbone.blocks.10.mlp.fc2.weight              |   589824   |\n",
      "|               backbone.blocks.10.mlp.fc2.bias               |    384     |\n",
      "|               backbone.blocks.10.norm3.weight               |    384     |\n",
      "|                backbone.blocks.10.norm3.bias                |    384     |\n",
      "|           backbone.blocks.10.local_mp.conv1.weight          |    3456    |\n",
      "|            backbone.blocks.10.local_mp.conv1.bias           |    384     |\n",
      "|            backbone.blocks.10.local_mp.bn.weight            |    384     |\n",
      "|             backbone.blocks.10.local_mp.bn.bias             |    384     |\n",
      "|           backbone.blocks.10.local_mp.conv2.weight          |    3456    |\n",
      "|            backbone.blocks.10.local_mp.conv2.bias           |    384     |\n",
      "|                  backbone.blocks.11.gamma1                  |    384     |\n",
      "|                  backbone.blocks.11.gamma2                  |    384     |\n",
      "|                  backbone.blocks.11.gamma3                  |    384     |\n",
      "|               backbone.blocks.11.norm1.weight               |    384     |\n",
      "|                backbone.blocks.11.norm1.bias                |    384     |\n",
      "|             backbone.blocks.11.attn.temperature             |     8      |\n",
      "|              backbone.blocks.11.attn.qkv.weight             |   442368   |\n",
      "|               backbone.blocks.11.attn.qkv.bias              |    1152    |\n",
      "|             backbone.blocks.11.attn.proj.weight             |   147456   |\n",
      "|              backbone.blocks.11.attn.proj.bias              |    384     |\n",
      "|               backbone.blocks.11.norm2.weight               |    384     |\n",
      "|                backbone.blocks.11.norm2.bias                |    384     |\n",
      "|              backbone.blocks.11.mlp.fc1.weight              |   589824   |\n",
      "|               backbone.blocks.11.mlp.fc1.bias               |    1536    |\n",
      "|              backbone.blocks.11.mlp.fc2.weight              |   589824   |\n",
      "|               backbone.blocks.11.mlp.fc2.bias               |    384     |\n",
      "|               backbone.blocks.11.norm3.weight               |    384     |\n",
      "|                backbone.blocks.11.norm3.bias                |    384     |\n",
      "|           backbone.blocks.11.local_mp.conv1.weight          |    3456    |\n",
      "|            backbone.blocks.11.local_mp.conv1.bias           |    384     |\n",
      "|            backbone.blocks.11.local_mp.bn.weight            |    384     |\n",
      "|             backbone.blocks.11.local_mp.bn.bias             |    384     |\n",
      "|           backbone.blocks.11.local_mp.conv2.weight          |    3456    |\n",
      "|            backbone.blocks.11.local_mp.conv2.bias           |    384     |\n",
      "|              backbone.cls_attn_blocks.0.gamma1              |    384     |\n",
      "|              backbone.cls_attn_blocks.0.gamma2              |    384     |\n",
      "|           backbone.cls_attn_blocks.0.norm1.weight           |    384     |\n",
      "|            backbone.cls_attn_blocks.0.norm1.bias            |    384     |\n",
      "|          backbone.cls_attn_blocks.0.attn.qkv.weight         |   442368   |\n",
      "|           backbone.cls_attn_blocks.0.attn.qkv.bias          |    1152    |\n",
      "|         backbone.cls_attn_blocks.0.attn.proj.weight         |   147456   |\n",
      "|          backbone.cls_attn_blocks.0.attn.proj.bias          |    384     |\n",
      "|           backbone.cls_attn_blocks.0.norm2.weight           |    384     |\n",
      "|            backbone.cls_attn_blocks.0.norm2.bias            |    384     |\n",
      "|          backbone.cls_attn_blocks.0.mlp.fc1.weight          |   589824   |\n",
      "|           backbone.cls_attn_blocks.0.mlp.fc1.bias           |    1536    |\n",
      "|          backbone.cls_attn_blocks.0.mlp.fc2.weight          |   589824   |\n",
      "|           backbone.cls_attn_blocks.0.mlp.fc2.bias           |    384     |\n",
      "|              backbone.cls_attn_blocks.1.gamma1              |    384     |\n",
      "|              backbone.cls_attn_blocks.1.gamma2              |    384     |\n",
      "|           backbone.cls_attn_blocks.1.norm1.weight           |    384     |\n",
      "|            backbone.cls_attn_blocks.1.norm1.bias            |    384     |\n",
      "|          backbone.cls_attn_blocks.1.attn.qkv.weight         |   442368   |\n",
      "|           backbone.cls_attn_blocks.1.attn.qkv.bias          |    1152    |\n",
      "|         backbone.cls_attn_blocks.1.attn.proj.weight         |   147456   |\n",
      "|          backbone.cls_attn_blocks.1.attn.proj.bias          |    384     |\n",
      "|           backbone.cls_attn_blocks.1.norm2.weight           |    384     |\n",
      "|            backbone.cls_attn_blocks.1.norm2.bias            |    384     |\n",
      "|          backbone.cls_attn_blocks.1.mlp.fc1.weight          |   589824   |\n",
      "|           backbone.cls_attn_blocks.1.mlp.fc1.bias           |    1536    |\n",
      "|          backbone.cls_attn_blocks.1.mlp.fc2.weight          |   589824   |\n",
      "|           backbone.cls_attn_blocks.1.mlp.fc2.bias           |    384     |\n",
      "|                     backbone.norm.weight                    |    384     |\n",
      "|                      backbone.norm.bias                     |    384     |\n",
      "|         backbone.pos_embeder.token_projection.weight        |   24576    |\n",
      "|          backbone.pos_embeder.token_projection.bias         |    384     |\n",
      "|                       converter.weight                      |   98304    |\n",
      "|                        converter.bias                       |    256     |\n",
      "|    transformer.encoder.layers.0.self_attn.in_proj_weight    |   196608   |\n",
      "|     transformer.encoder.layers.0.self_attn.in_proj_bias     |    768     |\n",
      "|    transformer.encoder.layers.0.self_attn.out_proj.weight   |   65536    |\n",
      "|     transformer.encoder.layers.0.self_attn.out_proj.bias    |    256     |\n",
      "|         transformer.encoder.layers.0.linear1.weight         |   524288   |\n",
      "|          transformer.encoder.layers.0.linear1.bias          |    2048    |\n",
      "|         transformer.encoder.layers.0.linear2.weight         |   524288   |\n",
      "|          transformer.encoder.layers.0.linear2.bias          |    256     |\n",
      "|          transformer.encoder.layers.0.norm1.weight          |    256     |\n",
      "|           transformer.encoder.layers.0.norm1.bias           |    256     |\n",
      "|          transformer.encoder.layers.0.norm2.weight          |    256     |\n",
      "|           transformer.encoder.layers.0.norm2.bias           |    256     |\n",
      "|    transformer.encoder.layers.1.self_attn.in_proj_weight    |   196608   |\n",
      "|     transformer.encoder.layers.1.self_attn.in_proj_bias     |    768     |\n",
      "|    transformer.encoder.layers.1.self_attn.out_proj.weight   |   65536    |\n",
      "|     transformer.encoder.layers.1.self_attn.out_proj.bias    |    256     |\n",
      "|         transformer.encoder.layers.1.linear1.weight         |   524288   |\n",
      "|          transformer.encoder.layers.1.linear1.bias          |    2048    |\n",
      "|         transformer.encoder.layers.1.linear2.weight         |   524288   |\n",
      "|          transformer.encoder.layers.1.linear2.bias          |    256     |\n",
      "|          transformer.encoder.layers.1.norm1.weight          |    256     |\n",
      "|           transformer.encoder.layers.1.norm1.bias           |    256     |\n",
      "|          transformer.encoder.layers.1.norm2.weight          |    256     |\n",
      "|           transformer.encoder.layers.1.norm2.bias           |    256     |\n",
      "|    transformer.encoder.layers.2.self_attn.in_proj_weight    |   196608   |\n",
      "|     transformer.encoder.layers.2.self_attn.in_proj_bias     |    768     |\n",
      "|    transformer.encoder.layers.2.self_attn.out_proj.weight   |   65536    |\n",
      "|     transformer.encoder.layers.2.self_attn.out_proj.bias    |    256     |\n",
      "|         transformer.encoder.layers.2.linear1.weight         |   524288   |\n",
      "|          transformer.encoder.layers.2.linear1.bias          |    2048    |\n",
      "|         transformer.encoder.layers.2.linear2.weight         |   524288   |\n",
      "|          transformer.encoder.layers.2.linear2.bias          |    256     |\n",
      "|          transformer.encoder.layers.2.norm1.weight          |    256     |\n",
      "|           transformer.encoder.layers.2.norm1.bias           |    256     |\n",
      "|          transformer.encoder.layers.2.norm2.weight          |    256     |\n",
      "|           transformer.encoder.layers.2.norm2.bias           |    256     |\n",
      "|    transformer.encoder.layers.3.self_attn.in_proj_weight    |   196608   |\n",
      "|     transformer.encoder.layers.3.self_attn.in_proj_bias     |    768     |\n",
      "|    transformer.encoder.layers.3.self_attn.out_proj.weight   |   65536    |\n",
      "|     transformer.encoder.layers.3.self_attn.out_proj.bias    |    256     |\n",
      "|         transformer.encoder.layers.3.linear1.weight         |   524288   |\n",
      "|          transformer.encoder.layers.3.linear1.bias          |    2048    |\n",
      "|         transformer.encoder.layers.3.linear2.weight         |   524288   |\n",
      "|          transformer.encoder.layers.3.linear2.bias          |    256     |\n",
      "|          transformer.encoder.layers.3.norm1.weight          |    256     |\n",
      "|           transformer.encoder.layers.3.norm1.bias           |    256     |\n",
      "|          transformer.encoder.layers.3.norm2.weight          |    256     |\n",
      "|           transformer.encoder.layers.3.norm2.bias           |    256     |\n",
      "|               transformer.encoder.norm.weight               |    256     |\n",
      "|                transformer.encoder.norm.bias                |    256     |\n",
      "|    transformer.decoder.layers.0.self_attn.in_proj_weight    |   196608   |\n",
      "|     transformer.decoder.layers.0.self_attn.in_proj_bias     |    768     |\n",
      "|    transformer.decoder.layers.0.self_attn.out_proj.weight   |   65536    |\n",
      "|     transformer.decoder.layers.0.self_attn.out_proj.bias    |    256     |\n",
      "|  transformer.decoder.layers.0.multihead_attn.in_proj_weight |   196608   |\n",
      "|   transformer.decoder.layers.0.multihead_attn.in_proj_bias  |    768     |\n",
      "| transformer.decoder.layers.0.multihead_attn.out_proj.weight |   65536    |\n",
      "|  transformer.decoder.layers.0.multihead_attn.out_proj.bias  |    256     |\n",
      "|         transformer.decoder.layers.0.linear1.weight         |   524288   |\n",
      "|          transformer.decoder.layers.0.linear1.bias          |    2048    |\n",
      "|         transformer.decoder.layers.0.linear2.weight         |   524288   |\n",
      "|          transformer.decoder.layers.0.linear2.bias          |    256     |\n",
      "|          transformer.decoder.layers.0.norm1.weight          |    256     |\n",
      "|           transformer.decoder.layers.0.norm1.bias           |    256     |\n",
      "|          transformer.decoder.layers.0.norm2.weight          |    256     |\n",
      "|           transformer.decoder.layers.0.norm2.bias           |    256     |\n",
      "|          transformer.decoder.layers.0.norm3.weight          |    256     |\n",
      "|           transformer.decoder.layers.0.norm3.bias           |    256     |\n",
      "|    transformer.decoder.layers.1.self_attn.in_proj_weight    |   196608   |\n",
      "|     transformer.decoder.layers.1.self_attn.in_proj_bias     |    768     |\n",
      "|    transformer.decoder.layers.1.self_attn.out_proj.weight   |   65536    |\n",
      "|     transformer.decoder.layers.1.self_attn.out_proj.bias    |    256     |\n",
      "|  transformer.decoder.layers.1.multihead_attn.in_proj_weight |   196608   |\n",
      "|   transformer.decoder.layers.1.multihead_attn.in_proj_bias  |    768     |\n",
      "| transformer.decoder.layers.1.multihead_attn.out_proj.weight |   65536    |\n",
      "|  transformer.decoder.layers.1.multihead_attn.out_proj.bias  |    256     |\n",
      "|         transformer.decoder.layers.1.linear1.weight         |   524288   |\n",
      "|          transformer.decoder.layers.1.linear1.bias          |    2048    |\n",
      "|         transformer.decoder.layers.1.linear2.weight         |   524288   |\n",
      "|          transformer.decoder.layers.1.linear2.bias          |    256     |\n",
      "|          transformer.decoder.layers.1.norm1.weight          |    256     |\n",
      "|           transformer.decoder.layers.1.norm1.bias           |    256     |\n",
      "|          transformer.decoder.layers.1.norm2.weight          |    256     |\n",
      "|           transformer.decoder.layers.1.norm2.bias           |    256     |\n",
      "|          transformer.decoder.layers.1.norm3.weight          |    256     |\n",
      "|           transformer.decoder.layers.1.norm3.bias           |    256     |\n",
      "|    transformer.decoder.layers.2.self_attn.in_proj_weight    |   196608   |\n",
      "|     transformer.decoder.layers.2.self_attn.in_proj_bias     |    768     |\n",
      "|    transformer.decoder.layers.2.self_attn.out_proj.weight   |   65536    |\n",
      "|     transformer.decoder.layers.2.self_attn.out_proj.bias    |    256     |\n",
      "|  transformer.decoder.layers.2.multihead_attn.in_proj_weight |   196608   |\n",
      "|   transformer.decoder.layers.2.multihead_attn.in_proj_bias  |    768     |\n",
      "| transformer.decoder.layers.2.multihead_attn.out_proj.weight |   65536    |\n",
      "|  transformer.decoder.layers.2.multihead_attn.out_proj.bias  |    256     |\n",
      "|         transformer.decoder.layers.2.linear1.weight         |   524288   |\n",
      "|          transformer.decoder.layers.2.linear1.bias          |    2048    |\n",
      "|         transformer.decoder.layers.2.linear2.weight         |   524288   |\n",
      "|          transformer.decoder.layers.2.linear2.bias          |    256     |\n",
      "|          transformer.decoder.layers.2.norm1.weight          |    256     |\n",
      "|           transformer.decoder.layers.2.norm1.bias           |    256     |\n",
      "|          transformer.decoder.layers.2.norm2.weight          |    256     |\n",
      "|           transformer.decoder.layers.2.norm2.bias           |    256     |\n",
      "|          transformer.decoder.layers.2.norm3.weight          |    256     |\n",
      "|           transformer.decoder.layers.2.norm3.bias           |    256     |\n",
      "|    transformer.decoder.layers.3.self_attn.in_proj_weight    |   196608   |\n",
      "|     transformer.decoder.layers.3.self_attn.in_proj_bias     |    768     |\n",
      "|    transformer.decoder.layers.3.self_attn.out_proj.weight   |   65536    |\n",
      "|     transformer.decoder.layers.3.self_attn.out_proj.bias    |    256     |\n",
      "|  transformer.decoder.layers.3.multihead_attn.in_proj_weight |   196608   |\n",
      "|   transformer.decoder.layers.3.multihead_attn.in_proj_bias  |    768     |\n",
      "| transformer.decoder.layers.3.multihead_attn.out_proj.weight |   65536    |\n",
      "|  transformer.decoder.layers.3.multihead_attn.out_proj.bias  |    256     |\n",
      "|         transformer.decoder.layers.3.linear1.weight         |   524288   |\n",
      "|          transformer.decoder.layers.3.linear1.bias          |    2048    |\n",
      "|         transformer.decoder.layers.3.linear2.weight         |   524288   |\n",
      "|          transformer.decoder.layers.3.linear2.bias          |    256     |\n",
      "|          transformer.decoder.layers.3.norm1.weight          |    256     |\n",
      "|           transformer.decoder.layers.3.norm1.bias           |    256     |\n",
      "|          transformer.decoder.layers.3.norm2.weight          |    256     |\n",
      "|           transformer.decoder.layers.3.norm2.bias           |    256     |\n",
      "|          transformer.decoder.layers.3.norm3.weight          |    256     |\n",
      "|           transformer.decoder.layers.3.norm3.bias           |    256     |\n",
      "|               transformer.decoder.norm.weight               |    256     |\n",
      "|                transformer.decoder.norm.bias                |    256     |\n",
      "|                         vocab.weight                        |    5120    |\n",
      "|                          vocab.bias                         |     20     |\n",
      "|                        decoder.weight                       |    5120    |\n",
      "+-------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 37525972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37525972"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
