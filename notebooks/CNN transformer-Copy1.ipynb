{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d8c639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mhamdan/seq2seqAttenHTR/Transformer_ocr/src\n"
     ]
    }
   ],
   "source": [
    "%cd src/\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import groupby\n",
    "import h5py\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import cv2\n",
    "from torchvision.models import resnet50, resnet101\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from data import preproc as pp\n",
    "from data import evaluation\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from operator import __add__\n",
    "import random\n",
    "\n",
    "\n",
    "def set_random_seeds(random_seed=0):\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_random_seeds(random_seed=13)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Unmodified from https://github.com/fastai/fastai/blob/5c51f9eabf76853a89a9bc5741804d2ed4407e49/fastai/layers.py\n",
    "def conv1d(ni:int, no:int, ks:int=1, stride:int=1, padding:int=0, bias:bool=False):\n",
    "    \"Create and initialize a `nn.Conv1d` layer with spectral normalization.\"\n",
    "    conv = nn.Conv1d(ni, no, ks, stride=stride, padding=padding, bias=bias)\n",
    "    nn.init.kaiming_normal_(conv.weight)\n",
    "    if bias: conv.bias.data.zero_()\n",
    "    return nn.utils.spectral_norm(conv)\n",
    "\n",
    "\n",
    "\n",
    "# Adapted from SelfAttention layer at https://github.com/fastai/fastai/blob/5c51f9eabf76853a89a9bc5741804d2ed4407e49/fastai/layers.py\n",
    "# Inspired by https://arxiv.org/pdf/1805.08318.pdf\n",
    "class SimpleSelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_in:int, ks=1, sym=False):#, n_out:int):\n",
    "        super().__init__()\n",
    "           \n",
    "        self.conv = conv1d(n_in, n_in, ks, padding=ks//2, bias=False)      \n",
    "       \n",
    "        self.gamma = nn.Parameter(torch.Tensor([0.]))\n",
    "        \n",
    "        self.sym = sym\n",
    "        self.n_in = n_in\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        \n",
    "        if self.sym:\n",
    "            # symmetry hack by https://github.com/mgrankin\n",
    "            c = self.conv.weight.view(self.n_in,self.n_in)\n",
    "            c = (c + c.t())/2\n",
    "            self.conv.weight = c.view(self.n_in,self.n_in,1)\n",
    "                \n",
    "        size = x.size()  \n",
    "        x = x.view(*size[:2],-1)   # (C,N)\n",
    "        \n",
    "        # changed the order of mutiplication to avoid O(N^2) complexity\n",
    "        # (x*xT)*(W*x) instead of (x*(xT*(W*x)))\n",
    "        \n",
    "        convx = self.conv(x)   # (C,C) * (C,N) = (C,N)   => O(NC^2)\n",
    "        xxT = torch.bmm(x,x.permute(0,2,1).contiguous())   # (C,N) * (N,C) = (C,C)   => O(NC^2)\n",
    "        \n",
    "        o = torch.bmm(xxT, convx)   # (C,C) * (C,N) = (C,N)   => O(NC^2)\n",
    "          \n",
    "        o = self.gamma * o + x\n",
    "        \n",
    "          \n",
    "        return o.view(*size).contiguous()        \n",
    "        \n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=128):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class OCR(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_len, hidden_dim, nheads,\n",
    "                 num_encoder_layers, num_decoder_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=\"same\"\n",
    "        )\n",
    "#         self.sa1 = SimpleSelfAttention(64)        \n",
    "        self.batch1 = nn.BatchNorm2d(16)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        ##CNN Layer 2\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=\"same\"\n",
    "        )\n",
    "#         self.sa2 = SimpleSelfAttention(128)        \n",
    "        self.batch2 = nn.BatchNorm2d(32)\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        ##CNN Layer 3\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=48, kernel_size=3, stride=1, padding=\"same\"\n",
    "        )\n",
    "#         self.sa3 = SimpleSelfAttention(256)        \n",
    "        self.batch3 = nn.BatchNorm2d(48)\n",
    "        self.act3 = nn.LeakyReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        ##CNN Layer 4\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=48, out_channels=64, kernel_size=3, stride=1, padding=\"same\"\n",
    "        )\n",
    "#         self.sa4 = SimpleSelfAttention(512)        \n",
    "        self.batch4 = nn.BatchNorm2d(64)\n",
    "        self.act4 = nn.LeakyReLU()\n",
    "        ##CNN Layer 5\n",
    "        self.drop3 = nn.Dropout(0.2)\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=64, out_channels=80, kernel_size=3, stride=1, padding=\"same\"\n",
    "        )\n",
    "#         self.sa5 = SimpleSelfAttention(512)        \n",
    "        self.batch5 = nn.BatchNorm2d(80)\n",
    "        self.act5 = nn.LeakyReLU()\n",
    "        \n",
    "        self.conv = nn.Conv2d(80, hidden_dim, 1)\n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "        # prediction heads with length of vocab\n",
    "        # DETR used basic 3 layer MLP for output\n",
    "        self.vocab = nn.Linear(hidden_dim,vocab_len)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.decoder = nn.Embedding(vocab_len, hidden_dim)\n",
    "        self.query_pos = PositionalEncoding(hidden_dim, .2)\n",
    "\n",
    "        # spatial positional encodings, sine positional encoding can be used.\n",
    "        # Detr baseline uses sine positional encoding.\n",
    "        self.row_embed = nn.Parameter(torch.rand(128, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(16, hidden_dim // 2))\n",
    "        self.trg_mask = None\n",
    "  \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), 1)\n",
    "        mask = mask.masked_fill(mask==1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def get_feature(self,x):\n",
    "        x = self.conv1(x)\n",
    "#         x = self.sa1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "        ##CNN Layer 2\n",
    "        x = self.conv2(x)\n",
    "#         x = self.sa2(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "        ##CNN Layer 3\n",
    "        x = self.drop1(x)\n",
    "        x = self.conv3(x)\n",
    "#         x = self.sa3(x)\n",
    "        x = self.batch3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.pool3(x)\n",
    "        ##CNN Layer 4\n",
    "        x = self.drop2(x)\n",
    "        x = self.conv4(x)\n",
    "#         x = self.sa4(x)        \n",
    "        x = self.batch4(x)\n",
    "        x = self.act4(x)\n",
    "        ##CNN Layer 5\n",
    "        x = self.drop3(x)\n",
    "        x = self.conv5(x)\n",
    "#         x = self.sa5(x)        \n",
    "        x = self.batch5(x)\n",
    "        x = self.act5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def make_len_mask(self, inp):\n",
    "        return (inp == 0).transpose(0, 1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, trg):\n",
    "        # propagate inputs through ResNet-101 up to avg-pool layer\n",
    "        x = self.get_feature(inputs)\n",
    "        # convert from 2048 to 256 feature planes for the transformer\n",
    "        h = self.conv(x)\n",
    "        # construct positional encodings\n",
    "        bs,_,H, W = h.shape\n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "        # generating subsequent mask for target\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(trg.shape[1]).to(trg.device)\n",
    "\n",
    "        # Padding mask\n",
    "        trg_pad_mask = self.make_len_mask(trg)\n",
    "\n",
    "        # Getting postional encoding for target\n",
    "        trg = self.decoder(trg)\n",
    "        trg = self.query_pos(trg)\n",
    "        \n",
    "        output = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1), trg.permute(1,0,2), tgt_mask=self.trg_mask, \n",
    "                                  tgt_key_padding_mask=trg_pad_mask.permute(1,0))\n",
    "\n",
    "        return self.vocab(output.transpose(0,1))\n",
    "\n",
    "\n",
    "def make_model(vocab_len, hidden_dim=256, nheads=4,\n",
    "                 num_encoder_layers=4, num_decoder_layers=4):\n",
    "    \n",
    "    return OCR(vocab_len, hidden_dim, nheads,\n",
    "                 num_encoder_layers, num_decoder_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a26465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = make_model(vocab_len=99,hidden_dim=256, nheads=4,\n",
    "#                  num_encoder_layers=4, num_decoder_layers=4)\n",
    "\n",
    "# img = torch.rand(1,1,1024,128)\n",
    "# trg = torch.randint(1,5,(1,128))\n",
    "# x = model(img,trg)\n",
    "\n",
    "\"\"\"\n",
    "Uses generator functions to supply train/test with data.\n",
    "Image renderings and text are created on the fly each time.\n",
    "\"\"\"\n",
    "\n",
    "class DataGenerator(Dataset):\n",
    "    \"\"\"Generator class with data streaming\"\"\"\n",
    "\n",
    "    def __init__(self, source, split, transform, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.split = split\n",
    "        self.dataset = dict()\n",
    "\n",
    "        with h5py.File(source, \"r\") as f:\n",
    "            self.dataset[self.split] = dict()\n",
    "\n",
    "            self.dataset[self.split]['dt'] = np.array(f[self.split]['dt'])\n",
    "            self.dataset[self.split]['gt'] = np.array(f[self.split]['gt'])\n",
    "          \n",
    "            randomize = np.arange(len(self.dataset[self.split]['gt']))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(randomize)\n",
    "\n",
    "            self.dataset[self.split]['dt'] = self.dataset[self.split]['dt'][randomize]\n",
    "            self.dataset[self.split]['gt'] = self.dataset[self.split]['gt'][randomize]\n",
    "\n",
    "            # decode sentences from byte\n",
    "            self.dataset[self.split]['gt'] = [x.decode() for x in self.dataset[self.split]['gt']]\n",
    "            \n",
    "        self.size = len(self.dataset[self.split]['gt'])\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = self.dataset[self.split]['dt'][i]\n",
    "        \n",
    "        #making image compatible with resnet\n",
    "#         img = cv2.transpose(img)\n",
    "        img = np.repeat(img[..., np.newaxis],3, -1).astype(\"float32\")   \n",
    "#         img = pp.normalization(img).astype(\"float32\")\n",
    "#         img = img.astype(\"float32\")\n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img)\n",
    "            img = aug['image']\n",
    "            \n",
    "#             img = self.transform(img)\n",
    "            \n",
    "        y_train = self.tokenizer.encode(self.dataset[self.split]['gt'][i]) \n",
    "        \n",
    "        #padding till max length\n",
    "        y_train = np.pad(y_train, (0, self.tokenizer.maxlen - len(y_train)))\n",
    "\n",
    "        gt = torch.Tensor(y_train)\n",
    "\n",
    "        return img, gt          \n",
    "\n",
    "    def __len__(self):\n",
    "      return self.size\n",
    "\n",
    "class Tokenizer():\n",
    "    \"\"\"Manager tokens functions and charset/dictionary properties\"\"\"\n",
    "\n",
    "    def __init__(self, chars, max_text_length=128):\n",
    "        self.PAD_TK, self.UNK_TK,self.SOS,self.EOS = \"¶\", \"¤\", \"SOS\", \"EOS\"\n",
    "        self.chars = [self.PAD_TK] + [self.UNK_TK ]+ [self.SOS] + [self.EOS] +list(chars)\n",
    "        self.PAD = self.chars.index(self.PAD_TK)\n",
    "        self.UNK = self.chars.index(self.UNK_TK)\n",
    "\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.maxlen = max_text_length\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to vector\"\"\"\n",
    "        text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        groups = [\"\".join(group) for _, group in groupby(text)]\n",
    "        text = \"\".join([self.UNK_TK.join(list(x)) if len(x) > 1 else x for x in groups])\n",
    "        encoded = []\n",
    "\n",
    "        text = ['SOS'] + list(text) + ['EOS']\n",
    "        for item in text:\n",
    "            index = self.chars.index(item)\n",
    "            index = self.UNK if index == -1 else index\n",
    "            encoded.append(index)\n",
    "\n",
    "        return np.asarray(encoded)\n",
    "\n",
    "    def decode(self, text):\n",
    "        \"\"\"Decode vector to text\"\"\"\n",
    "        \n",
    "        decoded = \"\".join([self.chars[int(x)] for x in text if x > -1])\n",
    "        decoded = self.remove_tokens(decoded)\n",
    "        decoded = pp.text_standardize(decoded)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def remove_tokens(self, text):\n",
    "        \"\"\"Remove tokens (PAD) from text\"\"\"\n",
    "\n",
    "        return text.replace(self.PAD_TK, \"\").replace(self.UNK_TK, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a148b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: ../data/iam_only_cursive.hdf5\n",
      "charset: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 200\n",
    "\n",
    "# define paths\n",
    "#change paths accordingly\n",
    "source = 'iam_only_cursive'\n",
    "source_path = '../data/{}.hdf5'.format(source)\n",
    "\n",
    "# define input size, number max of chars per line and list of valid chars\n",
    "input_size = (1024, 128, 1)\n",
    "max_text_length = 128\n",
    "charset_base = string.printable[:95]\n",
    "# charset_base = string.printable[:36].lower() + string.printable[36+26:95].lower() \n",
    "\n",
    "print(\"source:\", source_path)\n",
    "print(\"charset:\", charset_base)\n",
    "\n",
    "\n",
    "import torchvision.transforms as T\n",
    "local_rank = 2\n",
    "device = torch.device(\"cuda:{}\".format(local_rank))\n",
    "\n",
    "# transform = T.Compose([\n",
    "#     T.ToTensor()])\n",
    "tokenizer = Tokenizer(charset_base)\n",
    "import albumentations\n",
    "import albumentations.pytorch\n",
    "\n",
    "if True:\n",
    "\n",
    "    transform_train = albumentations.Compose([\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.MotionBlur(p=1, blur_limit=8),\n",
    "                albumentations.OpticalDistortion(p=1, distort_limit=0.05),\n",
    "                albumentations.GaussNoise(p=1, var_limit=(10.0, 100.0)),\n",
    "                albumentations.RandomBrightnessContrast(p=1, brightness_limit=0.2),\n",
    "                albumentations.Downscale(p=1, scale_min=0.3, scale_max=0.5),\n",
    "            ],\n",
    "            p=.5,\n",
    "        ),\n",
    "#         albumentations.Resize(224,224),\n",
    "        albumentations.Normalize(),\n",
    "        albumentations.pytorch.ToTensorV2()\n",
    "\n",
    "    ])\n",
    "\n",
    "    transform_valid = albumentations.Compose(\n",
    "        [\n",
    "#         albumentations.Resize(224,224),            \n",
    "            albumentations.Normalize(),\n",
    "            albumentations.pytorch.ToTensorV2()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "else:\n",
    "    transform_train = albumentations.Compose(\n",
    "        [\n",
    "#         albumentations.Resize(224,224),            \n",
    "            albumentations.Normalize(),\n",
    "            albumentations.pytorch.ToTensorV2()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    transform_valid = albumentations.Compose(\n",
    "        [\n",
    "#         albumentations.Resize(224,224),\n",
    "            \n",
    "            albumentations.Normalize(),\n",
    "            albumentations.pytorch.ToTensorV2()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(DataGenerator(source_path,'train',transform_train, tokenizer), batch_size=batch_size,  shuffle=True,num_workers=6)\n",
    "val_loader = torch.utils.data.DataLoader(DataGenerator(source_path,'valid',transform_valid, tokenizer), batch_size=batch_size, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1936b83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhamdan/miniconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "\n",
    "\n",
    "model = make_model( vocab_len=tokenizer.vocab_size,hidden_dim=256, nheads=4,\n",
    "                 num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers)\n",
    "\n",
    "            \n",
    "# init_funcs = {\n",
    "# 1: lambda x: torch.nn.init.normal_(x, mean=0., std=1.), # can be bias\n",
    "# 2: lambda x: torch.nn.init.xavier_normal_(x, gain=1.), # can be weight\n",
    "# 3: lambda x: torch.nn.init.xavier_uniform_(x, gain=1.), # can be conv1D filter\n",
    "# 4: lambda x: torch.nn.init.xavier_uniform_(x, gain=1.), # can be conv2D filter\n",
    "# \"default\": lambda x: torch.nn.init.constant(x, 1.), # everything else\n",
    "# }\n",
    "# for p in model.parameters():\n",
    "#     init_func = init_funcs.get(len(p.shape), init_funcs[\"default\"])\n",
    "#     init_func(p)\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx=0, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
    "\n",
    "smoothing = .4\n",
    "criterion = LabelSmoothing(size=tokenizer.vocab_size, padding_idx=0, smoothing=smoothing)\n",
    "criterion.to(device)\n",
    "lr = .5e-05# learnig rte\n",
    "backbone_lr = .003\n",
    "# if not args.pretrained:\n",
    "#     backbone_lr = backbone_lr*10\n",
    "\n",
    "# param_dicts = [\n",
    "#     {\"params\": [p for n, p in model.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "#     {\n",
    "#         \"params\": [p for n, p in model.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "#         \"lr\": backbone_lr,\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "scheduler_factor = .8\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr,weight_decay=.0004)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=scheduler_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b79020",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2ef9ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = \"../output_cnnt/only_cursive/{}_best.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1abe8c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimiser,dataloader):\n",
    " \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch, (imgs, labels_y,) in enumerate(dataloader):\n",
    "          imgs = imgs.to(device)\n",
    "          labels_y = labels_y.to(device)\n",
    "    \n",
    "          optimiser.zero_grad()\n",
    "          output = model(imgs.float(),labels_y.long()[:,:-1])\n",
    " \n",
    "          loss = criterion(output.log_softmax(-1).contiguous().view(-1, tokenizer.vocab_size), labels_y[:,1:].contiguous().view(-1).long()) \n",
    " \n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), 0.2)\n",
    "          optimizer.step()\n",
    "          total_loss += loss.item()\n",
    " \n",
    "    return total_loss / len(dataloader)\n",
    " \n",
    "def evaluate(model, criterion, dataloader,):\n",
    " \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    cer = 0\n",
    "    with torch.no_grad():\n",
    "      for batch, (imgs, labels_y,) in enumerate(dataloader):\n",
    "            imgs = imgs.to(device)\n",
    "            labels_y = labels_y.to(device)\n",
    " \n",
    "            output = model(imgs.float(),labels_y.long()[:,:-1])\n",
    "            o = output.argmax(-1)\n",
    "            predicts = list(map(lambda x : tokenizer.decode(x).replace('SOS','').replace('EOS',''),o))\n",
    "            gt = list(map(lambda x : tokenizer.decode(x).replace('SOS','').replace('EOS',''),labels_y))\n",
    "            cer += evaluation.ocr_metrics(predicts=predicts,\n",
    "                                   ground_truth=gt)[0]\n",
    "            \n",
    "            loss = criterion(output.log_softmax(-1).contiguous().view(-1, tokenizer.vocab_size), labels_y[:,1:].contiguous().view(-1).long())\n",
    "  \n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    " \n",
    "    return epoch_loss / len(dataloader), cer\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    " \n",
    "best_valid_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c6c00d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 learning rate[5e-06]\n",
      "Time: 2m 51s\n",
      "Train Loss: 553.042\n",
      "Val   Loss: 477.897\n",
      "cer: 81.736\n",
      "Epoch: 02 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 500.974\n",
      "Val   Loss: 464.722\n",
      "cer: 82.193\n",
      "Epoch: 03 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 489.960\n",
      "Val   Loss: 458.206\n",
      "cer: 81.810\n",
      "Epoch: 04 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 484.293\n",
      "Val   Loss: 456.002\n",
      "cer: 81.976\n",
      "Epoch: 05 learning rate[5e-06]\n",
      "Time: 2m 52s\n",
      "Train Loss: 480.651\n",
      "Val   Loss: 453.096\n",
      "cer: 81.256\n",
      "Epoch: 06 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 477.847\n",
      "Val   Loss: 451.727\n",
      "cer: 81.596\n",
      "Epoch: 07 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 475.752\n",
      "Val   Loss: 448.982\n",
      "cer: 81.069\n",
      "Epoch: 08 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 473.396\n",
      "Val   Loss: 447.594\n",
      "cer: 80.821\n",
      "Epoch: 09 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 471.359\n",
      "Val   Loss: 445.622\n",
      "cer: 80.342\n",
      "Epoch: 10 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 469.653\n",
      "Val   Loss: 443.725\n",
      "cer: 80.328\n",
      "Epoch: 11 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 467.835\n",
      "Val   Loss: 442.148\n",
      "cer: 80.300\n",
      "Epoch: 12 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 466.114\n",
      "Val   Loss: 440.693\n",
      "cer: 79.996\n",
      "Epoch: 13 learning rate[5e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 464.619\n",
      "Val   Loss: 438.545\n",
      "cer: 79.505\n",
      "Epoch: 14 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 462.645\n",
      "Val   Loss: 436.944\n",
      "cer: 79.451\n",
      "Epoch: 15 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 461.159\n",
      "Val   Loss: 434.960\n",
      "cer: 79.409\n",
      "Epoch: 16 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 459.623\n",
      "Val   Loss: 432.537\n",
      "cer: 78.711\n",
      "Epoch: 17 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 458.064\n",
      "Val   Loss: 432.122\n",
      "cer: 78.744\n",
      "Epoch: 18 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 456.592\n",
      "Val   Loss: 430.470\n",
      "cer: 78.668\n",
      "Epoch: 19 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 455.080\n",
      "Val   Loss: 428.395\n",
      "cer: 78.379\n",
      "Epoch: 20 learning rate[5e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 453.608\n",
      "Val   Loss: 427.335\n",
      "cer: 78.338\n",
      "Epoch: 21 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 452.370\n",
      "Val   Loss: 425.147\n",
      "cer: 77.951\n",
      "Epoch: 22 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 450.910\n",
      "Val   Loss: 424.516\n",
      "cer: 77.741\n",
      "Epoch: 23 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 449.437\n",
      "Val   Loss: 423.406\n",
      "cer: 77.354\n",
      "Epoch: 24 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 447.849\n",
      "Val   Loss: 421.240\n",
      "cer: 77.333\n",
      "Epoch: 25 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 446.558\n",
      "Val   Loss: 419.986\n",
      "cer: 77.512\n",
      "Epoch: 26 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 444.897\n",
      "Val   Loss: 418.123\n",
      "cer: 77.018\n",
      "Epoch: 27 learning rate[5e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 443.435\n",
      "Val   Loss: 416.644\n",
      "cer: 76.395\n",
      "Epoch: 28 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 442.125\n",
      "Val   Loss: 416.158\n",
      "cer: 76.953\n",
      "Epoch: 29 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 440.743\n",
      "Val   Loss: 415.104\n",
      "cer: 76.854\n",
      "Epoch: 30 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 439.608\n",
      "Val   Loss: 412.865\n",
      "cer: 76.034\n",
      "Epoch: 31 learning rate[5e-06]\n",
      "Time: 2m 56s\n",
      "Train Loss: 438.130\n",
      "Val   Loss: 411.778\n",
      "cer: 76.048\n",
      "Epoch: 32 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 436.706\n",
      "Val   Loss: 411.865\n",
      "cer: 75.963\n",
      "Epoch: 33 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 435.234\n",
      "Val   Loss: 409.888\n",
      "cer: 75.703\n",
      "Epoch: 34 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 434.051\n",
      "Val   Loss: 410.451\n",
      "cer: 75.900\n",
      "Epoch: 35 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 432.548\n",
      "Val   Loss: 409.036\n",
      "cer: 75.885\n",
      "Epoch: 36 learning rate[5e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 431.328\n",
      "Val   Loss: 404.934\n",
      "cer: 75.119\n",
      "Epoch: 37 learning rate[5e-06]\n",
      "Time: 2m 56s\n",
      "Train Loss: 430.075\n",
      "Val   Loss: 403.656\n",
      "cer: 74.988\n",
      "Epoch: 38 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 429.057\n",
      "Val   Loss: 402.860\n",
      "cer: 74.965\n",
      "Epoch: 39 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 427.824\n",
      "Val   Loss: 402.639\n",
      "cer: 75.006\n",
      "Epoch: 40 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 426.495\n",
      "Val   Loss: 401.414\n",
      "cer: 74.917\n",
      "Epoch: 41 learning rate[5e-06]\n",
      "Time: 2m 56s\n",
      "Train Loss: 425.172\n",
      "Val   Loss: 399.967\n",
      "cer: 74.536\n",
      "Epoch: 42 learning rate[5e-06]\n",
      "Time: 2m 56s\n",
      "Train Loss: 424.274\n",
      "Val   Loss: 399.100\n",
      "cer: 74.299\n",
      "Epoch: 43 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 423.323\n",
      "Val   Loss: 397.202\n",
      "cer: 74.031\n",
      "Epoch: 44 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 422.061\n",
      "Val   Loss: 397.342\n",
      "cer: 73.821\n",
      "Epoch: 45 learning rate[5e-06]\n",
      "Time: 2m 56s\n",
      "Train Loss: 421.159\n",
      "Val   Loss: 395.723\n",
      "cer: 73.667\n",
      "Epoch: 46 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 419.594\n",
      "Val   Loss: 394.331\n",
      "cer: 73.877\n",
      "Epoch: 47 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 418.862\n",
      "Val   Loss: 393.399\n",
      "cer: 73.973\n",
      "Epoch: 48 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 417.830\n",
      "Val   Loss: 393.787\n",
      "cer: 73.270\n",
      "Epoch: 49 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 417.055\n",
      "Val   Loss: 391.706\n",
      "cer: 73.596\n",
      "Epoch: 50 learning rate[5e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 415.783\n",
      "Val   Loss: 391.150\n",
      "cer: 73.372\n",
      "Epoch: 51 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 415.122\n",
      "Val   Loss: 390.184\n",
      "cer: 73.248\n",
      "Epoch: 52 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 413.905\n",
      "Val   Loss: 389.815\n",
      "cer: 73.009\n",
      "Epoch: 53 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 412.889\n",
      "Val   Loss: 388.190\n",
      "cer: 72.965\n",
      "Epoch: 54 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 411.973\n",
      "Val   Loss: 388.105\n",
      "cer: 72.524\n",
      "Epoch: 55 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 410.884\n",
      "Val   Loss: 387.136\n",
      "cer: 72.937\n",
      "Epoch: 56 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 410.251\n",
      "Val   Loss: 385.569\n",
      "cer: 72.707\n",
      "Epoch: 57 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 409.144\n",
      "Val   Loss: 385.658\n",
      "cer: 72.608\n",
      "Epoch: 58 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 408.550\n",
      "Val   Loss: 384.322\n",
      "cer: 72.638\n",
      "Epoch: 59 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 407.664\n",
      "Val   Loss: 384.737\n",
      "cer: 72.480\n",
      "Epoch: 60 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 406.595\n",
      "Val   Loss: 383.968\n",
      "cer: 72.693\n",
      "Epoch: 61 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 406.126\n",
      "Val   Loss: 382.556\n",
      "cer: 72.047\n",
      "Epoch: 62 learning rate[5e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 405.666\n",
      "Val   Loss: 382.098\n",
      "cer: 72.239\n",
      "Epoch: 63 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 404.469\n",
      "Val   Loss: 381.903\n",
      "cer: 73.562\n",
      "Epoch: 64 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 403.874\n",
      "Val   Loss: 381.463\n",
      "cer: 72.274\n",
      "Epoch: 65 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 402.809\n",
      "Val   Loss: 380.310\n",
      "cer: 73.128\n",
      "Epoch: 66 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 402.259\n",
      "Val   Loss: 378.916\n",
      "cer: 71.673\n",
      "Epoch: 67 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 401.753\n",
      "Val   Loss: 378.238\n",
      "cer: 71.633\n",
      "Epoch: 68 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 400.838\n",
      "Val   Loss: 378.976\n",
      "cer: 71.017\n",
      "Epoch: 69 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 399.852\n",
      "Val   Loss: 377.161\n",
      "cer: 72.632\n",
      "Epoch: 70 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 398.982\n",
      "Val   Loss: 378.044\n",
      "cer: 74.428\n",
      "Epoch: 71 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 398.716\n",
      "Val   Loss: 376.425\n",
      "cer: 71.862\n",
      "Epoch: 72 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 397.990\n",
      "Val   Loss: 376.297\n",
      "cer: 71.459\n",
      "Epoch: 73 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 397.277\n",
      "Val   Loss: 375.504\n",
      "cer: 71.788\n",
      "Epoch: 74 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 396.803\n",
      "Val   Loss: 375.259\n",
      "cer: 72.645\n",
      "Epoch: 75 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 396.149\n",
      "Val   Loss: 374.407\n",
      "cer: 71.554\n",
      "Epoch: 76 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 395.313\n",
      "Val   Loss: 373.022\n",
      "cer: 70.382\n",
      "Epoch: 77 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 394.512\n",
      "Val   Loss: 374.315\n",
      "cer: 71.760\n",
      "Epoch: 78 learning rate[5e-06]\n",
      "Time: 2m 56s\n",
      "Train Loss: 394.328\n",
      "Val   Loss: 372.974\n",
      "cer: 70.963\n",
      "Epoch: 79 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 393.572\n",
      "Val   Loss: 371.811\n",
      "cer: 70.252\n",
      "Epoch: 80 learning rate[5e-06]\n",
      "Time: 2m 56s\n",
      "Train Loss: 392.526\n",
      "Val   Loss: 371.927\n",
      "cer: 70.145\n",
      "Epoch: 81 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 392.220\n",
      "Val   Loss: 371.303\n",
      "cer: 71.389\n",
      "Epoch: 82 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 391.792\n",
      "Val   Loss: 370.910\n",
      "cer: 71.752\n",
      "Epoch: 83 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 390.871\n",
      "Val   Loss: 371.088\n",
      "cer: 70.324\n",
      "Epoch: 84 learning rate[5e-06]\n",
      "Time: 2m 56s\n",
      "Train Loss: 390.645\n",
      "Val   Loss: 370.291\n",
      "cer: 69.871\n",
      "Epoch: 85 learning rate[5e-06]\n",
      "Time: 2m 56s\n",
      "Train Loss: 390.208\n",
      "Val   Loss: 370.224\n",
      "cer: 70.029\n",
      "Epoch: 86 learning rate[5e-06]\n",
      "Time: 2m 56s\n",
      "Train Loss: 389.416\n",
      "Val   Loss: 369.622\n",
      "cer: 72.399\n",
      "Epoch: 87 learning rate[5e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2m 56s\n",
      "Train Loss: 388.882\n",
      "Val   Loss: 368.940\n",
      "cer: 69.948\n",
      "Epoch: 88 learning rate[5e-06]\n",
      "Time: 2m 56s\n",
      "Train Loss: 388.547\n",
      "Val   Loss: 369.292\n",
      "cer: 70.410\n",
      "Epoch: 89 learning rate[5e-06]\n",
      "Time: 2m 56s\n",
      "Train Loss: 387.981\n",
      "Val   Loss: 369.115\n",
      "cer: 69.910\n",
      "Epoch: 90 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 387.112\n",
      "Val   Loss: 367.685\n",
      "cer: 71.637\n",
      "Epoch: 91 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 386.891\n",
      "Val   Loss: 367.810\n",
      "cer: 69.469\n",
      "Epoch: 92 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 386.100\n",
      "Val   Loss: 367.102\n",
      "cer: 70.092\n",
      "Epoch: 93 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 385.429\n",
      "Val   Loss: 366.393\n",
      "cer: 70.892\n",
      "Epoch: 94 learning rate[5e-06]\n",
      "Time: 2m 56s\n",
      "Train Loss: 385.265\n",
      "Val   Loss: 366.414\n",
      "cer: 69.155\n",
      "Epoch: 95 learning rate[5e-06]\n",
      "Time: 2m 56s\n",
      "Train Loss: 384.551\n",
      "Val   Loss: 365.401\n",
      "cer: 69.700\n",
      "Epoch: 96 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 384.374\n",
      "Val   Loss: 366.170\n",
      "cer: 69.719\n",
      "Epoch: 97 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 383.754\n",
      "Val   Loss: 365.627\n",
      "cer: 69.379\n",
      "Epoch: 98 learning rate[5e-06]\n",
      "Time: 2m 58s\n",
      "Train Loss: 382.913\n",
      "Val   Loss: 364.920\n",
      "cer: 69.653\n",
      "Epoch: 99 learning rate[5e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 382.491\n",
      "Val   Loss: 364.552\n",
      "cer: 68.903\n",
      "Epoch: 100 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 382.154\n",
      "Val   Loss: 365.621\n",
      "cer: 69.694\n",
      "Epoch: 101 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 381.743\n",
      "Val   Loss: 364.215\n",
      "cer: 69.371\n",
      "Epoch: 102 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 381.095\n",
      "Val   Loss: 363.485\n",
      "cer: 69.181\n",
      "Epoch: 103 learning rate[5e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 380.717\n",
      "Val   Loss: 362.979\n",
      "cer: 68.760\n",
      "Epoch: 104 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 380.259\n",
      "Val   Loss: 362.515\n",
      "cer: 69.169\n",
      "Epoch: 105 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 379.739\n",
      "Val   Loss: 364.348\n",
      "cer: 69.092\n",
      "Epoch: 106 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 379.364\n",
      "Val   Loss: 361.770\n",
      "cer: 68.934\n",
      "Epoch: 107 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 379.035\n",
      "Val   Loss: 361.882\n",
      "cer: 68.646\n",
      "Epoch: 108 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 378.688\n",
      "Val   Loss: 361.518\n",
      "cer: 69.577\n",
      "Epoch: 109 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 377.935\n",
      "Val   Loss: 361.051\n",
      "cer: 69.093\n",
      "Epoch: 110 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 377.657\n",
      "Val   Loss: 360.790\n",
      "cer: 69.492\n",
      "Epoch: 111 learning rate[5e-06]\n",
      "Time: 2m 55s\n",
      "Train Loss: 377.158\n",
      "Val   Loss: 361.394\n",
      "cer: 68.795\n",
      "Epoch: 112 learning rate[5e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 376.679\n",
      "Val   Loss: 362.708\n",
      "cer: 68.984\n",
      "Epoch: 113 learning rate[5e-06]\n",
      "Time: 2m 46s\n",
      "Train Loss: 376.229\n",
      "Val   Loss: 360.988\n",
      "cer: 69.301\n",
      "Epoch: 114 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 375.683\n",
      "Val   Loss: 360.667\n",
      "cer: 69.391\n",
      "Epoch: 115 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 375.251\n",
      "Val   Loss: 359.985\n",
      "cer: 68.381\n",
      "Epoch: 116 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 374.874\n",
      "Val   Loss: 359.999\n",
      "cer: 68.237\n",
      "Epoch: 117 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 374.783\n",
      "Val   Loss: 359.581\n",
      "cer: 68.654\n",
      "Epoch: 118 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 373.974\n",
      "Val   Loss: 359.244\n",
      "cer: 68.121\n",
      "Epoch: 119 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 373.580\n",
      "Val   Loss: 358.689\n",
      "cer: 69.077\n",
      "Epoch: 120 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 373.458\n",
      "Val   Loss: 358.440\n",
      "cer: 67.977\n",
      "Epoch: 121 learning rate[5e-06]\n",
      "Time: 2m 46s\n",
      "Train Loss: 372.975\n",
      "Val   Loss: 359.306\n",
      "cer: 68.413\n",
      "Epoch: 122 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 372.574\n",
      "Val   Loss: 360.026\n",
      "cer: 67.974\n",
      "Epoch: 123 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 371.932\n",
      "Val   Loss: 359.024\n",
      "cer: 67.630\n",
      "Epoch: 124 learning rate[5e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 371.897\n",
      "Val   Loss: 358.225\n",
      "cer: 68.115\n",
      "Epoch: 125 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 371.489\n",
      "Val   Loss: 357.749\n",
      "cer: 67.843\n",
      "Epoch: 126 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 370.957\n",
      "Val   Loss: 356.872\n",
      "cer: 68.226\n",
      "Epoch: 127 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 370.627\n",
      "Val   Loss: 357.802\n",
      "cer: 67.731\n",
      "Epoch: 128 learning rate[5e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 370.249\n",
      "Val   Loss: 356.263\n",
      "cer: 67.896\n",
      "Epoch: 129 learning rate[5e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 369.751\n",
      "Val   Loss: 356.563\n",
      "cer: 67.984\n",
      "Epoch: 130 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 369.283\n",
      "Val   Loss: 357.927\n",
      "cer: 68.300\n",
      "Epoch: 131 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 369.162\n",
      "Val   Loss: 355.537\n",
      "cer: 67.518\n",
      "Epoch: 132 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 368.542\n",
      "Val   Loss: 355.667\n",
      "cer: 67.782\n",
      "Epoch: 133 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 368.444\n",
      "Val   Loss: 356.665\n",
      "cer: 70.568\n",
      "Epoch: 134 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 367.956\n",
      "Val   Loss: 355.126\n",
      "cer: 67.893\n",
      "Epoch: 135 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 367.676\n",
      "Val   Loss: 355.199\n",
      "cer: 68.023\n",
      "Epoch: 136 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 367.079\n",
      "Val   Loss: 354.400\n",
      "cer: 67.564\n",
      "Epoch: 137 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 366.701\n",
      "Val   Loss: 355.386\n",
      "cer: 67.379\n",
      "Epoch: 138 learning rate[5e-06]\n",
      "Time: 2m 46s\n",
      "Train Loss: 366.484\n",
      "Val   Loss: 354.071\n",
      "cer: 67.836\n",
      "Epoch: 139 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 366.133\n",
      "Val   Loss: 353.874\n",
      "cer: 67.326\n",
      "Epoch: 140 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 365.573\n",
      "Val   Loss: 353.502\n",
      "cer: 67.451\n",
      "Epoch: 141 learning rate[5e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 365.375\n",
      "Val   Loss: 354.728\n",
      "cer: 69.412\n",
      "Epoch: 142 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 365.189\n",
      "Val   Loss: 353.443\n",
      "cer: 68.137\n",
      "Epoch: 143 learning rate[5e-06]\n",
      "Time: 2m 46s\n",
      "Train Loss: 364.569\n",
      "Val   Loss: 354.076\n",
      "cer: 68.987\n",
      "Epoch: 144 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 364.530\n",
      "Val   Loss: 353.673\n",
      "cer: 67.722\n",
      "Epoch: 145 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 363.838\n",
      "Val   Loss: 353.764\n",
      "cer: 67.506\n",
      "Epoch: 146 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 363.565\n",
      "Val   Loss: 353.637\n",
      "cer: 67.335\n",
      "Epoch: 147 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 363.309\n",
      "Val   Loss: 352.654\n",
      "cer: 67.776\n",
      "Epoch: 148 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 363.045\n",
      "Val   Loss: 352.824\n",
      "cer: 67.671\n",
      "Epoch: 149 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 362.680\n",
      "Val   Loss: 352.405\n",
      "cer: 67.164\n",
      "Epoch: 150 learning rate[5e-06]\n",
      "Time: 2m 46s\n",
      "Train Loss: 362.464\n",
      "Val   Loss: 353.393\n",
      "cer: 67.672\n",
      "Epoch: 151 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 362.255\n",
      "Val   Loss: 351.716\n",
      "cer: 67.094\n",
      "Epoch: 152 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 361.580\n",
      "Val   Loss: 352.954\n",
      "cer: 67.921\n",
      "Epoch: 153 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 361.085\n",
      "Val   Loss: 351.431\n",
      "cer: 67.488\n",
      "Epoch: 154 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 361.009\n",
      "Val   Loss: 351.641\n",
      "cer: 67.986\n",
      "Epoch: 155 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 360.602\n",
      "Val   Loss: 351.968\n",
      "cer: 67.111\n",
      "Epoch: 156 learning rate[5e-06]\n",
      "Time: 2m 46s\n",
      "Train Loss: 360.151\n",
      "Val   Loss: 351.519\n",
      "cer: 66.941\n",
      "Epoch: 157 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 359.986\n",
      "Val   Loss: 351.236\n",
      "cer: 67.684\n",
      "Epoch: 158 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 359.546\n",
      "Val   Loss: 350.858\n",
      "cer: 67.283\n",
      "Epoch: 159 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 359.486\n",
      "Val   Loss: 352.154\n",
      "cer: 67.938\n",
      "Epoch: 160 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 359.223\n",
      "Val   Loss: 350.461\n",
      "cer: 66.893\n",
      "Epoch: 161 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 358.850\n",
      "Val   Loss: 351.748\n",
      "cer: 67.417\n",
      "Epoch: 162 learning rate[5e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 358.429\n",
      "Val   Loss: 350.715\n",
      "cer: 67.556\n",
      "Epoch: 163 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 358.138\n",
      "Val   Loss: 350.007\n",
      "cer: 66.902\n",
      "Epoch: 164 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 357.813\n",
      "Val   Loss: 350.660\n",
      "cer: 66.989\n",
      "Epoch: 165 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 357.474\n",
      "Val   Loss: 349.849\n",
      "cer: 66.567\n",
      "Epoch: 166 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 357.380\n",
      "Val   Loss: 350.087\n",
      "cer: 67.655\n",
      "Epoch: 167 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 356.940\n",
      "Val   Loss: 349.748\n",
      "cer: 67.115\n",
      "Epoch: 168 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 356.681\n",
      "Val   Loss: 349.811\n",
      "cer: 66.670\n",
      "Epoch: 169 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 356.345\n",
      "Val   Loss: 349.189\n",
      "cer: 66.661\n",
      "Epoch: 170 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 356.062\n",
      "Val   Loss: 349.969\n",
      "cer: 66.799\n",
      "Epoch: 171 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 355.715\n",
      "Val   Loss: 349.382\n",
      "cer: 67.214\n",
      "Epoch: 172 learning rate[5e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2m 47s\n",
      "Train Loss: 355.416\n",
      "Val   Loss: 349.170\n",
      "cer: 66.802\n",
      "Epoch: 173 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 354.926\n",
      "Val   Loss: 348.716\n",
      "cer: 66.756\n",
      "Epoch: 174 learning rate[5e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 354.744\n",
      "Val   Loss: 348.435\n",
      "cer: 66.818\n",
      "Epoch: 175 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 354.537\n",
      "Val   Loss: 348.440\n",
      "cer: 66.731\n",
      "Epoch: 176 learning rate[5e-06]\n",
      "Time: 2m 46s\n",
      "Train Loss: 354.385\n",
      "Val   Loss: 349.058\n",
      "cer: 66.774\n",
      "Epoch: 177 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 353.587\n",
      "Val   Loss: 348.281\n",
      "cer: 67.195\n",
      "Epoch: 178 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 353.771\n",
      "Val   Loss: 348.836\n",
      "cer: 66.615\n",
      "Epoch: 179 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 353.175\n",
      "Val   Loss: 348.244\n",
      "cer: 67.084\n",
      "Epoch: 180 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 353.098\n",
      "Val   Loss: 347.965\n",
      "cer: 66.716\n",
      "Epoch: 181 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 353.228\n",
      "Val   Loss: 348.080\n",
      "cer: 66.284\n",
      "Epoch: 182 learning rate[5e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 352.238\n",
      "Val   Loss: 347.204\n",
      "cer: 66.336\n",
      "Epoch: 183 learning rate[5e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 351.995\n",
      "Val   Loss: 348.013\n",
      "cer: 68.319\n",
      "Epoch: 184 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 351.738\n",
      "Val   Loss: 347.813\n",
      "cer: 66.761\n",
      "Epoch: 185 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 351.803\n",
      "Val   Loss: 347.436\n",
      "cer: 67.068\n",
      "Epoch: 186 learning rate[5e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 351.138\n",
      "Val   Loss: 347.736\n",
      "cer: 66.684\n",
      "Epoch: 187 learning rate[5e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 351.084\n",
      "Val   Loss: 347.423\n",
      "cer: 69.436\n",
      "Epoch: 188 learning rate[4.000000000000001e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 350.751\n",
      "Val   Loss: 347.065\n",
      "cer: 66.643\n",
      "Epoch: 189 learning rate[4.000000000000001e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 350.261\n",
      "Val   Loss: 346.912\n",
      "cer: 67.550\n",
      "Epoch: 190 learning rate[4.000000000000001e-06]\n",
      "Time: 2m 50s\n",
      "Train Loss: 350.006\n",
      "Val   Loss: 347.341\n",
      "cer: 68.202\n",
      "Epoch: 191 learning rate[4.000000000000001e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 349.904\n",
      "Val   Loss: 347.402\n",
      "cer: 66.792\n",
      "Epoch: 192 learning rate[4.000000000000001e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 349.536\n",
      "Val   Loss: 346.199\n",
      "cer: 66.146\n",
      "Epoch: 193 learning rate[4.000000000000001e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 349.148\n",
      "Val   Loss: 346.770\n",
      "cer: 66.414\n",
      "Epoch: 194 learning rate[4.000000000000001e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 349.303\n",
      "Val   Loss: 346.142\n",
      "cer: 66.419\n",
      "Epoch: 195 learning rate[4.000000000000001e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 349.237\n",
      "Val   Loss: 347.525\n",
      "cer: 66.349\n",
      "Epoch: 196 learning rate[4.000000000000001e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 349.025\n",
      "Val   Loss: 346.286\n",
      "cer: 67.093\n",
      "Epoch: 197 learning rate[4.000000000000001e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 348.244\n",
      "Val   Loss: 346.409\n",
      "cer: 66.990\n",
      "Epoch: 198 learning rate[4.000000000000001e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 348.472\n",
      "Val   Loss: 346.200\n",
      "cer: 67.535\n",
      "Epoch: 199 learning rate[4.000000000000001e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 347.889\n",
      "Val   Loss: 346.394\n",
      "cer: 67.841\n",
      "Epoch: 200 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 347.744\n",
      "Val   Loss: 346.197\n",
      "cer: 66.589\n",
      "Epoch: 201 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 347.438\n",
      "Val   Loss: 346.341\n",
      "cer: 65.926\n",
      "Epoch: 202 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 347.031\n",
      "Val   Loss: 346.448\n",
      "cer: 66.764\n",
      "Epoch: 203 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 347.066\n",
      "Val   Loss: 345.587\n",
      "cer: 67.036\n",
      "Epoch: 204 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 346.610\n",
      "Val   Loss: 345.892\n",
      "cer: 67.183\n",
      "Epoch: 205 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 52s\n",
      "Train Loss: 346.834\n",
      "Val   Loss: 345.882\n",
      "cer: 66.705\n",
      "Epoch: 206 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 346.664\n",
      "Val   Loss: 345.713\n",
      "cer: 68.300\n",
      "Epoch: 207 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 51s\n",
      "Train Loss: 346.394\n",
      "Val   Loss: 345.525\n",
      "cer: 67.164\n",
      "Epoch: 208 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 52s\n",
      "Train Loss: 346.630\n",
      "Val   Loss: 345.369\n",
      "cer: 67.208\n",
      "Epoch: 209 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 52s\n",
      "Train Loss: 346.060\n",
      "Val   Loss: 346.137\n",
      "cer: 67.327\n",
      "Epoch: 210 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 51s\n",
      "Train Loss: 345.976\n",
      "Val   Loss: 345.814\n",
      "cer: 66.838\n",
      "Epoch: 211 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 52s\n",
      "Train Loss: 345.784\n",
      "Val   Loss: 346.039\n",
      "cer: 67.100\n",
      "Epoch: 212 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 51s\n",
      "Train Loss: 345.667\n",
      "Val   Loss: 345.484\n",
      "cer: 66.856\n",
      "Epoch: 213 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 52s\n",
      "Train Loss: 345.319\n",
      "Val   Loss: 345.107\n",
      "cer: 66.864\n",
      "Epoch: 214 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 52s\n",
      "Train Loss: 345.086\n",
      "Val   Loss: 345.625\n",
      "cer: 66.151\n",
      "Epoch: 215 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 344.982\n",
      "Val   Loss: 345.090\n",
      "cer: 66.212\n",
      "Epoch: 216 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 50s\n",
      "Train Loss: 344.969\n",
      "Val   Loss: 345.068\n",
      "cer: 67.468\n",
      "Epoch: 217 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 344.663\n",
      "Val   Loss: 344.744\n",
      "cer: 66.947\n",
      "Epoch: 218 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 52s\n",
      "Train Loss: 344.790\n",
      "Val   Loss: 345.533\n",
      "cer: 65.952\n",
      "Epoch: 219 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 344.112\n",
      "Val   Loss: 345.147\n",
      "cer: 66.431\n",
      "Epoch: 220 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 52s\n",
      "Train Loss: 344.626\n",
      "Val   Loss: 344.815\n",
      "cer: 66.872\n",
      "Epoch: 221 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 344.235\n",
      "Val   Loss: 344.488\n",
      "cer: 66.573\n",
      "Epoch: 222 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 52s\n",
      "Train Loss: 344.055\n",
      "Val   Loss: 345.366\n",
      "cer: 66.192\n",
      "Epoch: 223 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 54s\n",
      "Train Loss: 343.816\n",
      "Val   Loss: 344.893\n",
      "cer: 66.348\n",
      "Epoch: 224 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 52s\n",
      "Train Loss: 343.649\n",
      "Val   Loss: 344.604\n",
      "cer: 65.949\n",
      "Epoch: 225 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 51s\n",
      "Train Loss: 343.257\n",
      "Val   Loss: 344.348\n",
      "cer: 67.512\n",
      "Epoch: 226 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 52s\n",
      "Train Loss: 343.082\n",
      "Val   Loss: 344.762\n",
      "cer: 67.200\n",
      "Epoch: 227 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 52s\n",
      "Train Loss: 343.008\n",
      "Val   Loss: 344.730\n",
      "cer: 65.677\n",
      "Epoch: 228 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 51s\n",
      "Train Loss: 343.385\n",
      "Val   Loss: 344.175\n",
      "cer: 66.105\n",
      "Epoch: 229 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 52s\n",
      "Train Loss: 343.109\n",
      "Val   Loss: 344.588\n",
      "cer: 66.349\n",
      "Epoch: 230 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 52s\n",
      "Train Loss: 342.254\n",
      "Val   Loss: 344.568\n",
      "cer: 66.794\n",
      "Epoch: 231 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 52s\n",
      "Train Loss: 342.243\n",
      "Val   Loss: 343.968\n",
      "cer: 66.232\n",
      "Epoch: 232 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 52s\n",
      "Train Loss: 342.361\n",
      "Val   Loss: 345.371\n",
      "cer: 66.045\n",
      "Epoch: 233 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 342.415\n",
      "Val   Loss: 344.205\n",
      "cer: 66.688\n",
      "Epoch: 234 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 341.682\n",
      "Val   Loss: 343.536\n",
      "cer: 66.132\n",
      "Epoch: 235 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 53s\n",
      "Train Loss: 341.868\n",
      "Val   Loss: 344.132\n",
      "cer: 67.341\n",
      "Epoch: 236 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 52s\n",
      "Train Loss: 341.523\n",
      "Val   Loss: 343.805\n",
      "cer: 65.764\n",
      "Epoch: 237 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 341.455\n",
      "Val   Loss: 345.922\n",
      "cer: 66.656\n",
      "Epoch: 238 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 43s\n",
      "Train Loss: 341.651\n",
      "Val   Loss: 344.123\n",
      "cer: 66.536\n",
      "Epoch: 239 learning rate[3.2000000000000007e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 340.935\n",
      "Val   Loss: 343.737\n",
      "cer: 66.682\n",
      "Epoch: 240 learning rate[2.560000000000001e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 340.982\n",
      "Val   Loss: 343.787\n",
      "cer: 66.926\n",
      "Epoch: 241 learning rate[2.560000000000001e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 340.683\n",
      "Val   Loss: 344.144\n",
      "cer: 65.406\n",
      "Epoch: 242 learning rate[2.560000000000001e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 340.430\n",
      "Val   Loss: 344.039\n",
      "cer: 66.011\n",
      "Epoch: 243 learning rate[2.560000000000001e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 340.737\n",
      "Val   Loss: 343.540\n",
      "cer: 65.487\n",
      "Epoch: 244 learning rate[2.560000000000001e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 340.326\n",
      "Val   Loss: 343.238\n",
      "cer: 66.425\n",
      "Epoch: 245 learning rate[2.560000000000001e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 340.346\n",
      "Val   Loss: 343.893\n",
      "cer: 66.303\n",
      "Epoch: 246 learning rate[2.560000000000001e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 340.286\n",
      "Val   Loss: 343.568\n",
      "cer: 67.205\n",
      "Epoch: 247 learning rate[2.560000000000001e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2m 50s\n",
      "Train Loss: 340.131\n",
      "Val   Loss: 344.426\n",
      "cer: 66.059\n",
      "Epoch: 248 learning rate[2.560000000000001e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 340.072\n",
      "Val   Loss: 343.586\n",
      "cer: 66.654\n",
      "Epoch: 249 learning rate[2.560000000000001e-06]\n",
      "Time: 2m 50s\n",
      "Train Loss: 339.751\n",
      "Val   Loss: 344.856\n",
      "cer: 65.938\n",
      "Epoch: 250 learning rate[2.048000000000001e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 339.309\n",
      "Val   Loss: 344.371\n",
      "cer: 65.773\n",
      "Epoch: 251 learning rate[2.048000000000001e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 339.114\n",
      "Val   Loss: 343.721\n",
      "cer: 65.816\n",
      "Epoch: 252 learning rate[2.048000000000001e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 339.302\n",
      "Val   Loss: 344.447\n",
      "cer: 65.701\n",
      "Epoch: 253 learning rate[2.048000000000001e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 339.093\n",
      "Val   Loss: 344.606\n",
      "cer: 65.678\n",
      "Epoch: 254 learning rate[2.048000000000001e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 338.826\n",
      "Val   Loss: 344.233\n",
      "cer: 66.037\n",
      "Epoch: 255 learning rate[1.638400000000001e-06]\n",
      "Time: 2m 50s\n",
      "Train Loss: 338.893\n",
      "Val   Loss: 343.145\n",
      "cer: 65.793\n",
      "Epoch: 256 learning rate[1.638400000000001e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 339.075\n",
      "Val   Loss: 343.796\n",
      "cer: 65.871\n",
      "Epoch: 257 learning rate[1.638400000000001e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 338.791\n",
      "Val   Loss: 343.713\n",
      "cer: 66.446\n",
      "Epoch: 258 learning rate[1.638400000000001e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 338.606\n",
      "Val   Loss: 343.769\n",
      "cer: 65.626\n",
      "Epoch: 259 learning rate[1.638400000000001e-06]\n",
      "Time: 2m 50s\n",
      "Train Loss: 338.393\n",
      "Val   Loss: 342.945\n",
      "cer: 66.230\n",
      "Epoch: 260 learning rate[1.638400000000001e-06]\n",
      "Time: 2m 50s\n",
      "Train Loss: 338.711\n",
      "Val   Loss: 343.621\n",
      "cer: 66.189\n",
      "Epoch: 261 learning rate[1.638400000000001e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 338.224\n",
      "Val   Loss: 343.422\n",
      "cer: 66.235\n",
      "Epoch: 262 learning rate[1.638400000000001e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 338.382\n",
      "Val   Loss: 344.338\n",
      "cer: 65.632\n",
      "Epoch: 263 learning rate[1.638400000000001e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 338.345\n",
      "Val   Loss: 343.660\n",
      "cer: 66.006\n",
      "Epoch: 264 learning rate[1.638400000000001e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 337.767\n",
      "Val   Loss: 343.833\n",
      "cer: 66.055\n",
      "Epoch: 265 learning rate[1.3107200000000008e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 337.755\n",
      "Val   Loss: 343.913\n",
      "cer: 65.858\n",
      "Epoch: 266 learning rate[1.3107200000000008e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 337.937\n",
      "Val   Loss: 343.072\n",
      "cer: 65.844\n",
      "Epoch: 267 learning rate[1.3107200000000008e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 337.781\n",
      "Val   Loss: 343.109\n",
      "cer: 65.390\n",
      "Epoch: 268 learning rate[1.3107200000000008e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 337.832\n",
      "Val   Loss: 342.878\n",
      "cer: 65.625\n",
      "Epoch: 269 learning rate[1.3107200000000008e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 337.640\n",
      "Val   Loss: 343.039\n",
      "cer: 65.920\n",
      "Epoch: 270 learning rate[1.3107200000000008e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 337.821\n",
      "Val   Loss: 343.579\n",
      "cer: 66.279\n",
      "Epoch: 271 learning rate[1.3107200000000008e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 337.708\n",
      "Val   Loss: 343.139\n",
      "cer: 66.535\n",
      "Epoch: 272 learning rate[1.3107200000000008e-06]\n",
      "Time: 2m 47s\n",
      "Train Loss: 338.026\n",
      "Val   Loss: 343.800\n",
      "cer: 66.256\n",
      "Epoch: 273 learning rate[1.3107200000000008e-06]\n",
      "Time: 2m 48s\n",
      "Train Loss: 337.527\n",
      "Val   Loss: 343.248\n",
      "cer: 66.008\n",
      "Epoch: 274 learning rate[1.0485760000000006e-06]\n",
      "Time: 2m 51s\n",
      "Train Loss: 337.496\n",
      "Val   Loss: 343.418\n",
      "cer: 65.880\n",
      "Epoch: 275 learning rate[1.0485760000000006e-06]\n",
      "Time: 2m 51s\n",
      "Train Loss: 337.355\n",
      "Val   Loss: 343.524\n",
      "cer: 66.264\n",
      "Epoch: 276 learning rate[1.0485760000000006e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 337.389\n",
      "Val   Loss: 343.011\n",
      "cer: 65.785\n",
      "Epoch: 277 learning rate[1.0485760000000006e-06]\n",
      "Time: 2m 49s\n",
      "Train Loss: 336.729\n",
      "Val   Loss: 343.638\n",
      "cer: 65.572\n",
      "Epoch: 278 learning rate[1.0485760000000006e-06]\n",
      "Time: 2m 51s\n",
      "Train Loss: 337.027\n",
      "Val   Loss: 343.362\n",
      "cer: 65.791\n",
      "Epoch: 279 learning rate[8.388608000000006e-07]\n",
      "Time: 2m 49s\n",
      "Train Loss: 337.161\n",
      "Val   Loss: 343.122\n",
      "cer: 65.661\n",
      "Epoch: 280 learning rate[8.388608000000006e-07]\n",
      "Time: 2m 51s\n",
      "Train Loss: 337.117\n",
      "Val   Loss: 343.070\n",
      "cer: 66.241\n",
      "Epoch: 281 learning rate[8.388608000000006e-07]\n",
      "Time: 2m 50s\n",
      "Train Loss: 337.035\n",
      "Val   Loss: 343.187\n",
      "cer: 65.715\n",
      "Epoch: 282 learning rate[8.388608000000006e-07]\n",
      "Time: 2m 49s\n",
      "Train Loss: 336.916\n",
      "Val   Loss: 343.534\n",
      "cer: 66.042\n",
      "Epoch: 283 learning rate[8.388608000000006e-07]\n",
      "Time: 2m 50s\n",
      "Train Loss: 336.886\n",
      "Val   Loss: 342.971\n",
      "cer: 65.910\n",
      "Epoch: 284 learning rate[6.710886400000005e-07]\n",
      "Time: 2m 49s\n",
      "Train Loss: 336.907\n",
      "Val   Loss: 343.105\n",
      "cer: 65.706\n",
      "Epoch: 285 learning rate[6.710886400000005e-07]\n",
      "Time: 2m 49s\n",
      "Train Loss: 336.831\n",
      "Val   Loss: 343.513\n",
      "cer: 65.600\n",
      "Epoch: 286 learning rate[6.710886400000005e-07]\n",
      "Time: 2m 48s\n",
      "Train Loss: 336.846\n",
      "Val   Loss: 343.099\n",
      "cer: 65.664\n",
      "Epoch: 287 learning rate[6.710886400000005e-07]\n",
      "Time: 2m 49s\n",
      "Train Loss: 336.769\n",
      "Val   Loss: 343.281\n",
      "cer: 66.191\n",
      "Epoch: 288 learning rate[6.710886400000005e-07]\n",
      "Time: 2m 51s\n",
      "Train Loss: 336.501\n",
      "Val   Loss: 343.737\n",
      "cer: 65.467\n",
      "Epoch: 289 learning rate[5.368709120000005e-07]\n",
      "Time: 2m 50s\n",
      "Train Loss: 336.664\n",
      "Val   Loss: 343.893\n",
      "cer: 65.415\n",
      "Epoch: 290 learning rate[5.368709120000005e-07]\n",
      "Time: 2m 50s\n",
      "Train Loss: 336.423\n",
      "Val   Loss: 343.486\n",
      "cer: 65.714\n",
      "Epoch: 291 learning rate[5.368709120000005e-07]\n",
      "Time: 2m 51s\n",
      "Train Loss: 336.427\n",
      "Val   Loss: 343.803\n",
      "cer: 65.862\n",
      "Epoch: 292 learning rate[5.368709120000005e-07]\n",
      "Time: 2m 43s\n",
      "Train Loss: 336.527\n",
      "Val   Loss: 343.523\n",
      "cer: 66.159\n",
      "Epoch: 293 learning rate[5.368709120000005e-07]\n",
      "Time: 2m 52s\n",
      "Train Loss: 336.521\n",
      "Val   Loss: 343.409\n",
      "cer: 66.399\n",
      "Epoch: 294 learning rate[4.294967296000004e-07]\n",
      "Time: 2m 54s\n",
      "Train Loss: 336.363\n",
      "Val   Loss: 343.252\n",
      "cer: 66.332\n",
      "Epoch: 295 learning rate[4.294967296000004e-07]\n",
      "Time: 2m 53s\n",
      "Train Loss: 336.064\n",
      "Val   Loss: 343.506\n",
      "cer: 65.644\n",
      "Epoch: 296 learning rate[4.294967296000004e-07]\n",
      "Time: 2m 52s\n",
      "Train Loss: 336.415\n",
      "Val   Loss: 343.112\n",
      "cer: 66.514\n",
      "Epoch: 297 learning rate[4.294967296000004e-07]\n",
      "Time: 2m 53s\n",
      "Train Loss: 336.466\n",
      "Val   Loss: 343.586\n",
      "cer: 65.619\n",
      "Epoch: 298 learning rate[4.294967296000004e-07]\n",
      "Time: 2m 53s\n",
      "Train Loss: 336.396\n",
      "Val   Loss: 343.772\n",
      "cer: 65.651\n",
      "Epoch: 299 learning rate[3.4359738368000034e-07]\n",
      "Time: 2m 52s\n",
      "Train Loss: 336.293\n",
      "Val   Loss: 343.495\n",
      "cer: 65.725\n",
      "Epoch: 300 learning rate[3.4359738368000034e-07]\n",
      "Time: 2m 53s\n",
      "Train Loss: 336.555\n",
      "Val   Loss: 343.592\n",
      "cer: 65.871\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for epoch in range(300):\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02}\", \"learning rate{}\".format(lr_scheduler.get_last_lr()))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, criterion, optimizer, train_loader)\n",
    "    valid_loss, cer = evaluate(model, criterion, val_loader)\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, time.time())\n",
    "\n",
    "    c += 1\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), target_path.format(\"loss\"))\n",
    "        c=0\n",
    "\n",
    "    if c > 4:\n",
    "        # decrease lr if loss does not deacrease after 5 steps\n",
    "        lr_scheduler.step()\n",
    "        c = 0\n",
    "                   \n",
    "    if epoch%10==0:\n",
    "        torch.save(model.state_dict(), target_path.format(\"epoch\"))\n",
    "\n",
    "\n",
    "    print(f\"Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"Train Loss: {train_loss:.3f}\")\n",
    "    print(f\"Val   Loss: {valid_loss:.3f}\")\n",
    "    print(f\"cer: {cer:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88ae1c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = torch.load(\"../output_cnnt/only_cursive/loss_best.hdf5\")\n",
    "model.load_state_dict(d1)\n",
    "\n",
    "def get_memory(model,imgs):\n",
    "    x = model.conv(model.get_feature(imgs))\n",
    "    bs,_,H, W = x.shape\n",
    "    pos = torch.cat([\n",
    "            model.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            model.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "    return model.transformer.encoder(pos +  0.1 * x.flatten(2).permute(2, 0, 1))\n",
    "    \n",
    "\n",
    "def test(model, test_loader, max_text_length):\n",
    "    model.eval()\n",
    "    predicts = []\n",
    "    gt = []\n",
    "    imgs = []\n",
    "    c=0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            src, trg = batch\n",
    "            imgs.append(src.flatten(0,1))\n",
    "            src, trg = src.to(device), trg.to(device)            \n",
    "            memory = get_memory(model,src.float())\n",
    "            out_indexes = [tokenizer.chars.index('SOS'), ]\n",
    "            for i in range(max_text_length):\n",
    "                mask = model.generate_square_subsequent_mask(i+1).to(device)\n",
    "                trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(device)\n",
    "                output = model.vocab(model.transformer.decoder(model.query_pos(model.decoder(trg_tensor)), memory,tgt_mask=mask))\n",
    "                out_token = output.argmax(2)[-1].item()\n",
    "                out_indexes.append(out_token)\n",
    "                if out_token == tokenizer.chars.index('EOS'):\n",
    "                    break\n",
    "            predicts.append(tokenizer.decode(out_indexes))\n",
    "            gt.append(tokenizer.decode(trg.flatten(0,1)))\n",
    "#             if c==5:\n",
    "#                 break\n",
    "            c+=1\n",
    "    return predicts, gt, imgs\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(DataGenerator(source_path,'test',transform_valid, tokenizer), batch_size=1, shuffle=False)\n",
    "\n",
    "predicts, gt, imgs = test(model,test_loader , max_text_length)\n",
    "\n",
    "predicts = list(map(lambda x : x.replace('SOS','').replace('EOS',''),predicts))\n",
    "gt = list(map(lambda x : x.replace('SOS','').replace('EOS',''),gt))\n",
    "\n",
    "evaluate = evaluation.ocr_metrics(predicts=predicts,\n",
    "                                  ground_truth=gt,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7fc4bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56216768, 0.76640838, 1.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed0c67a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52918469, 0.74089705, 1.        ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd4a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
